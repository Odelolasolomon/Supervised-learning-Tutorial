{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb61864b",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "Welcome to the first lesson in Optimizing Machine Learning Models in Python. Machine learning is the process of training mathematical models on data and using these models to make predictions on new, unseen data.\n",
    "According to the State of Data Science 2020 from Anaconda, feature engineering is a crucial step in the machine learning workflow, representing about a quarter of programing time. Feature engineering is the process of taking features or predictors in the data and transforming them into a format that improves predictions. Feature engineering is also known as feature extraction. A feature or predictor is a numeric representation of our raw data.\n",
    "\n",
    "In this course, we'll learn how, why, and when to apply feature engineering by learning the reasoning and mathematics behind it. Feature engineering is a big topic, so we will limit the scope of this lesson to numerical data. To help you learn, we have provided additional resources in the Takeaways at the end of this lesson.\n",
    "\n",
    "Features can be continuous or categorical, but feature engineering requires us to look at other aspects of these data. This includes the following:\n",
    "\n",
    "- **Sign: \n",
    "refers to a feature's positivity or negativity. This can be important when we use aggregated values or count values, like daily visits to a website or to a restaurant.\n",
    "- **Scale: \n",
    "refers to a feature's size might be. We might need to check if different features span different orders of magnitude since this might affect prediction accuracy.\n",
    "- **Distribution: \n",
    "refers to how common specific values of a feature are relative to others. We may find that a feature takes values in some typical range, and that there might be outliers that are far away from this typical set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99434905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "3    -122.25     37.85                52.0       1274.0           235.0   \n",
       "4    -122.25     37.85                52.0       1627.0           280.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
       "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
       "2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n",
       "3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n",
       "4       565.0       259.0         3.8462            342200.0        NEAR BAY  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df= pd.read_csv('housing.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "396b9bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of        longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0        -122.23     37.88                41.0        880.0           129.0   \n",
       "1        -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2        -122.24     37.85                52.0       1467.0           190.0   \n",
       "3        -122.25     37.85                52.0       1274.0           235.0   \n",
       "4        -122.25     37.85                52.0       1627.0           280.0   \n",
       "...          ...       ...                 ...          ...             ...   \n",
       "20635    -121.09     39.48                25.0       1665.0           374.0   \n",
       "20636    -121.21     39.49                18.0        697.0           150.0   \n",
       "20637    -121.22     39.43                17.0       2254.0           485.0   \n",
       "20638    -121.32     39.43                18.0       1860.0           409.0   \n",
       "20639    -121.24     39.37                16.0       2785.0           616.0   \n",
       "\n",
       "       population  households  median_income  median_house_value  \\\n",
       "0           322.0       126.0         8.3252            452600.0   \n",
       "1          2401.0      1138.0         8.3014            358500.0   \n",
       "2           496.0       177.0         7.2574            352100.0   \n",
       "3           558.0       219.0         5.6431            341300.0   \n",
       "4           565.0       259.0         3.8462            342200.0   \n",
       "...           ...         ...            ...                 ...   \n",
       "20635       845.0       330.0         1.5603             78100.0   \n",
       "20636       356.0       114.0         2.5568             77100.0   \n",
       "20637      1007.0       433.0         1.7000             92300.0   \n",
       "20638       741.0       349.0         1.8672             84700.0   \n",
       "20639      1387.0       530.0         2.3886             89400.0   \n",
       "\n",
       "      ocean_proximity  \n",
       "0            NEAR BAY  \n",
       "1            NEAR BAY  \n",
       "2            NEAR BAY  \n",
       "3            NEAR BAY  \n",
       "4            NEAR BAY  \n",
       "...               ...  \n",
       "20635          INLAND  \n",
       "20636          INLAND  \n",
       "20637          INLAND  \n",
       "20638          INLAND  \n",
       "20639          INLAND  \n",
       "\n",
       "[20640 rows x 10 columns]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13869a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "longitude               0\n",
       "latitude                0\n",
       "housing_median_age      0\n",
       "total_rooms             0\n",
       "total_bedrooms        207\n",
       "population              0\n",
       "households              0\n",
       "median_income           0\n",
       "median_house_value      0\n",
       "ocean_proximity         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edf80147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "longitude             float64\n",
       "latitude              float64\n",
       "housing_median_age    float64\n",
       "total_rooms           float64\n",
       "total_bedrooms        float64\n",
       "population            float64\n",
       "households            float64\n",
       "median_income         float64\n",
       "median_house_value    float64\n",
       "ocean_proximity        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83aa2838",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_missing=['total_bedrooms']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007c1ec2",
   "metadata": {},
   "source": [
    "## Univariate Imputation\n",
    " Traning a model on only observations where all the data is present is called a complete case analysis. **Complete case analysis** isn't necessarily a bad thing, but it can result in bias and reduced efficiency in the model if we have to remove large portions of the data.\n",
    "The first type of imputation that we'll discuss is **univariate imputation**. As the name suggests, univariate imputation only uses information from a single column to perform the imputation, and this column is usually the one with the missing values. In univariate imputation, we can choose to fill in missing values based on some statistic such as the mean, median, or mode. These statistics represent the average or most frequent value of a feature, so these are not bad predictions to make.\n",
    "\n",
    "The scikit-learn library provides objects for us to implement imputation using the impute module. Specifically, we'll use the SimpleImputer class. In order to instantiate a SimpleImputer() object, we need to specify two arguments:\n",
    "\n",
    "missing_values: what values indicate missing values?\n",
    "strategy: how should we fill in these missing values?\n",
    "\n",
    "`from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy=\"mean\")`\n",
    "\n",
    "Above, we initialize a SimpleImputer() object and specify that NaN values represent missing values. To fill in these missing values, we use the mean. These arguments can take other values, so you can read further into the documentation for the SimpleImputer class to learn more.\n",
    "\n",
    "After we instantiate a SimpleImputer() object, we can use it to perform imputation on any dataset we feed it. To do so, we need to use the fit_transform() method in the class. This method takes in a dataset with missing values and outputs the same dataset with imputed values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7427b7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.drop(columns=['ocean_proximity'], axis=1)\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer= SimpleImputer(missing_values= np.nan, strategy='median')\n",
    "\n",
    "data= imputer.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fda0b493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "longitude             0\n",
       "latitude              0\n",
       "housing_median_age    0\n",
       "total_rooms           0\n",
       "total_bedrooms        0\n",
       "population            0\n",
       "households            0\n",
       "median_income         0\n",
       "median_house_value    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing= pd.DataFrame(data, columns=df.columns)\n",
    "housing.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3180a22f",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors Imputation\n",
    "This is the idea behind K-Nearest Neighbors imputation. If we are trying to impute a feature for an observation, we should look at other observations that are similar or \"close\" to it (i.e., its neighbors). If these neighbors have values for the feature of interest, we can make a more educated prediction of what the missing value should be. Specifically, we take the average of the neighbor's features and use this to impute for the one with the missing value. (The main concepts behind K-nearest neighbors has been explored elsewhere, so we won't delve into it here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83aaef9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "longitude             0\n",
       "latitude              0\n",
       "housing_median_age    0\n",
       "total_rooms           0\n",
       "total_bedrooms        0\n",
       "population            0\n",
       "households            0\n",
       "median_income         0\n",
       "median_house_value    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer= KNNImputer(missing_values= np.nan, n_neighbors=3)\n",
    "data2= imputer.fit_transform(df)\n",
    "\n",
    "housing2= pd.DataFrame(data2, columns=df.columns)\n",
    "housing2.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511e489d",
   "metadata": {},
   "source": [
    "## Outlier Detection\n",
    "The next aspect of feature engineering that we'll discuss is handling outliers. Outliers are observations that stand out from the concentration of other observations. As such, outliers can be extremely small or extremely large. Outliers can be troublesome because they can influence the estimation of model parameters, which harms model predictions. Therefore, we might consider removing outliers from our data — or at least being aware of their presence.\n",
    "\n",
    "In either case, we have to first identify outliers in our data. This is the process of outlier detection. Since outliers represent either extremely low or high values, one way we can detect them is by generating box plots, also known as box and whisker plots. Box plots tell us about the distribution of a feature in terms of its quartiles. The quartiles represent the 25th, 50th, and 75th percentiles of the feature. We show an example of a box plot below:\n",
    "\n",
    "<img src=boxplot.png width=500 height=500>\n",
    "\n",
    "The numbers denote the following:\n",
    "\n",
    "This line is a lower bound, which we'll discuss below.\n",
    "The lower end of the box is the 25th quantile of median_house_value, also known as \n",
    "Q\n",
    "1\n",
    ".\n",
    "The middle of the box is the 50th quantile, also known as the median or \n",
    "Q\n",
    "2\n",
    ".\n",
    "The upper end of the box is the 75th quantile, also known as \n",
    "Q\n",
    "3\n",
    ".\n",
    "This line is an upper bound, which we'll also discuss below.\n",
    "These dots represent observations that go past the upper bound, which we classify as outliers.\n",
    "The top and bottom of the \"box\" in the box plot are formed by \n",
    "Q\n",
    "3\n",
    " and \n",
    "Q\n",
    "1\n",
    ". The upper and lower bounds are calculated as a function of these two values. First, we must understand the interquartile range (\n",
    "IQR\n",
    "). The \n",
    "IQR\n",
    " is calculated as \n",
    "Q\n",
    "3\n",
    " - \n",
    "Q\n",
    "1\n",
    ", and we may think of it as the range of values that contain the central 50% of the observations.\n",
    "\n",
    "In order to calculate the lower bound in the box plot, we subtract \n",
    "\\1.5 × IQR from Q1. Similarly, to calculate the upper bound, we add 1.5 × IQR to Q3\n",
    "\n",
    ". Any values that are outside of these bounds are considered outliers. Therefore, we can use the quartiles to help detect outliers. Keen readers might ask, \"Why is 1.5 used in that calculation?\" We'll discuss this more on the next screen, but know that it's not an arbitrary number. There is a mathematical reason behind it!\n",
    "\n",
    "As a note, the above plot was created using the boxplot() method. This function flags outliers using the rule we described, but it draws the lower and upper bounds in terms of data points that are close to the upper and lower bound calculations. This might explain any discrepancies you may see in the plot versus hand calculation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2071277e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  18.,  275., 1359.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentiles= [ 25, 50, 75]\n",
    "\n",
    "data_quartiles= np.percentile(housing, percentiles)\n",
    "data_quartiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f8ea890",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1= housing.quantile(.25, axis=0)\n",
    "q2= housing.quantile(.50, axis=0)\n",
    "q3= housing.quantile(.75, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4610c92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longitude               -121.8000\n",
      "latitude                  33.9300\n",
      "housing_median_age        18.0000\n",
      "total_rooms             1447.7500\n",
      "total_bedrooms           297.0000\n",
      "population               787.0000\n",
      "households               280.0000\n",
      "median_income              2.5634\n",
      "median_house_value    119600.0000\n",
      "Name: 0.25, dtype: float64 longitude               -118.4900\n",
      "latitude                  34.2600\n",
      "housing_median_age        29.0000\n",
      "total_rooms             2127.0000\n",
      "total_bedrooms           435.0000\n",
      "population              1166.0000\n",
      "households               409.0000\n",
      "median_income              3.5348\n",
      "median_house_value    179700.0000\n",
      "Name: 0.5, dtype: float64 longitude               -118.01000\n",
      "latitude                  37.71000\n",
      "housing_median_age        37.00000\n",
      "total_rooms             3148.00000\n",
      "total_bedrooms           643.25000\n",
      "population              1725.00000\n",
      "households               605.00000\n",
      "median_income              4.74325\n",
      "median_house_value    264725.00000\n",
      "Name: 0.75, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(q1,q2,q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1721dd38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
       "       'total_bedrooms', 'population', 'households', 'median_income',\n",
       "       'median_house_value'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48dcdfff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119600.0 264725.0 145125.0\n"
     ]
    }
   ],
   "source": [
    "mhv_q1= housing['median_house_value'].quantile(.25)\n",
    "mhv_q3= housing['median_house_value'].quantile(.75)\n",
    "mhv_iqr= mhv_q3-mhv_q1\n",
    "print(mhv_q1, mhv_q3, mhv_iqr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3363d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-98087.5 482412.5\n"
     ]
    }
   ],
   "source": [
    "lower_bound_mhv= mhv_q1 - (1.5*mhv_iqr)\n",
    "upper_bound_mhv= mhv_q3 + (1.5*mhv_iqr)\n",
    "print(lower_bound_mhv, upper_bound_mhv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50a2ac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a= housing['median_house_value']\n",
    "mhv_num_outliers=a[(a < (lower_bound_mhv))|( a > (upper_bound_mhv))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7c3f031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1071"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mhv_num_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "caac4747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20640, 9)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d54585b",
   "metadata": {},
   "source": [
    "##  Z-Scores\n",
    "On this screen, we'll learn another method for detecting outliers based on the normal distribution. A normal, or Gaussian, distribution is a probability distribution with a characteristic \"bell-shaped\" curve. The peak of the bell is centered on the mean of the normal distribution, and the spread of the data around the mean is dictated by its variance.\n",
    "\n",
    "<img src=normal.png width=500 height=500>\n",
    "\n",
    "A normal distribution has several qualities that make it useful for characterizing data. One of these qualities is that normally distributed data are highly concentrated around its mean. Concentration around the mean results in the 68-95-99 rule, which states the following:\n",
    "\n",
    "- 68% of normally distributed data falls within 1 standard deviation (i.e., the square root of variance).\n",
    "- 95% of normally distributed data falls within 2 standard deviations.\n",
    "- 99% of normally distributed data falls within 3 standard deviations.\n",
    "\n",
    "That is, virtually all the data will fall within three standard deviations of the mean. Note that we must make the assumption that the data is normally distributed for this rule to hold. Verifying this assumption is outside of the scope of this lesson, but it is something we should make explicit when we check for outliers.\n",
    "\n",
    "Knowing that 99% of normally distributed data is within three standard deviations of the mean, we can use this fact to detect outliers. By calculating how many standard deviations each observation is away from the mean, we can use this as our detection method. In other words, we're using the mean and standard deviation of a normal distribution, instead of the quartiles used in the box plot method.\n",
    "\n",
    "To calculate the number of standard deviations that an observation is away from the mean, we can calculate something called the **Z-score**. We calculate the Z-score as follows:\n",
    "\n",
    "Assuming we have an array of observations from some feature \n",
    "X\n",
    ", we can calculate its mean \n",
    "μ\n",
    " and its standard deviation \n",
    "σ\n",
    ". Then, for each observation in \n",
    "X\n",
    ", we subtract the mean and divide by the standard deviation to get the Z-score. This value actually represents how many standard deviations the original observation is away from the mean. Doing these two operations is also known as centering and scaling.\n",
    "\n",
    "Z-scores can also have a sign as well, which indicates where the observation is relative to the mean. For example, if an observation had a Z-score of 1, it indicates that the observation is 1 standard deviation above the mean. A Z-score of -2.5 indicates the observation is 2.5 standard deviations below the mean.\n",
    "\n",
    "Using the 68-95-99 rule, any observation that has a Z-score outside of the range -3 to 3 will be marked as an outlier. In fact, the reason a 1.5 factor is used in the box plot method for detecting outliers references this idea. It turns out that 1.5 is approximately the factor needed to cover 99% of the data based on the quartiles instead of the mean and standard deviation. So, both outlier detection methods use this idea that the outliers will stray extremely far from the middle of the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2551bd17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "\n",
       "   population  households  median_income  median_house_value  \n",
       "0       322.0       126.0         8.3252            452600.0  \n",
       "1      2401.0      1138.0         8.3014            358500.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "913533f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2.129580\n",
       "1        1.314124\n",
       "2        1.258663\n",
       "3        1.165072\n",
       "4        1.172871\n",
       "           ...   \n",
       "20635   -1.115777\n",
       "20636   -1.124443\n",
       "20637   -0.992722\n",
       "20638   -1.058583\n",
       "20639   -1.017853\n",
       "Name: median_house_value, Length: 20640, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_mhv= housing['median_house_value'].mean()\n",
    "std_mhv= housing['median_house_value'].std()\n",
    "\n",
    "z_scores= (housing['median_house_value'] - mean_mhv)/std_mhv\n",
    "z_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfcad358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result= sum((z_scores < -3) | (z_scores > 3))\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19061614",
   "metadata": {},
   "source": [
    "We've learned how to detect outliers using imputation, but we haven't really examined what we should do about them. Handling outliers can be difficult because we have to carefully examine the data and make a judgment: does this outlier stem from a data collection error or from the underlying nature of the data?\n",
    "\n",
    "The former case is easier to handle. If the outlier represents some kind of random mistake from data collection, then we can treat it like a missing data problem. The data is observed, but it's incorrect, so we can try to remove this observation or use the imputation methods we learned earlier. Sometimes it's easy to spot this type of outlier. If all of the values of a column range from 0 to 10 except for an outlier that has a value of 10000, then we should either remove it or impute it.\n",
    "\n",
    "The more difficult case is when we believe that the outlier may have happened naturally for that feature; extreme values are normal in some cases. For example, stock prices may vary, historically, within a small range for most of its existence. Then, rare events like a recession may suddenly spike or flatten the price. In this case, some force is causing the extreme value, not some random measurement error. If we were to ignore these extreme stock prices in a machine learning model, we run the risk of heavily biasing the model because we are essentially ignoring an important phenomenon that caused the change.\n",
    "\n",
    "Because of this, we must resolve outliers on a case-by-case basis. Every dataset is created under different circumstances, so the best thing we can do is examine the data and make a judgment. Here are a few heuristics to follow:\n",
    "\n",
    "- As mentioned above, if the outlier seems to be the result of a random mistake, then exclude it or use imputation to make a better guess as to the outlier value.\n",
    "\n",
    "- In the presence of outliers, repeat the analysis under different circumstances: try including them, and then try excluding them. There may be a lucky case where the outlier does not really affect the model predictions. Otherwise, it's important to document this finding.\n",
    "\n",
    "- Be transparent. Use either a box plot or the z-score to identify outliers and make it clear in your report how many there are and which columns contain them. If your model predictions seem off, then they might be a potential cause. In either case, documenting whether or not you include them can help your teammates decide how to proceed.\n",
    "\n",
    "You may have noticed that the box plot and Z-score methods we used to identify outliers on the previous two screens had starkly different results. The box plot method found 1071 outliers, while the Z-score method found none! One of the reasons behind this discrepancy is an implicit assumption we need to make when using Z-scores: we assume the data is normally distributed. Let's check this assumption by looking at the histogram for median_house_value:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fac53e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='median_house_value', ylabel='Count'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.histplot(data= housing, x='median_house_value')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caa7989",
   "metadata": {},
   "source": [
    "We can see from the histogram that median_house_value is actually skewed to the right, meaning that there are more extremely high values than would be expected in a normal distirbution. If the normality assumption is violated, then Z-scores aren't as useful as an outlier detection tool and can even yield false conclusions. As a general rule, we suggest using the box plot method to evaluate if there are outliers in your dataset. Since the important points of a box plot are decided by the quartiles, they will be less influenced by extreme values unlike the mean and standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac59b16",
   "metadata": {},
   "source": [
    "## Imbalanced Datasets\n",
    "To conclude this lesson, we'll learn about one last aspect of feature engineering that might prove useful as we see more datasets. While outliers and missing data are more general problems, this particular feature engineering aspect deals specifically with classification. In classification tasks, we know that we have both cases (coded as 1 or True) and non-cases (coded as 0 or False). When there's an approximately equal number of cases and non-cases, the model can more easily be optimized to distinguish between them.\n",
    "\n",
    "However, sometimes we aren't so lucky, and we might encounter a dataset where either the cases or non-cases form a majority of the dataset. This can happen in contexts when cases are uncommon, such as with rare diseases or events. When this happens, we refer to the dataset as being imbalanced.\n",
    "\n",
    "When this happens, we refer to the dataset as being imbalanced.\n",
    "\n",
    "Imbalanced datasets can hurt predictive ability because the minority class has less data, making it harder to determine how to use the predictors to distinguish them. An imbalanced dataset can also be problematic because it affects how we typically interpret standard performance metrics like accuracy. For example, if a dataset contained 90% non-cases and only 10% cases, a naive classifier might predict that every observation in the dataset is a non-case. The accuracy of this naive classifier would already be 90%, even if we don't use any information in our prediction.\n",
    "\n",
    "Imbalance can vary in terms of severity as well. A good heuristic for diagnosing this imbalance is to check the percentage of the minority class, the class with a smaller amount of observations in the data:\n",
    "\n",
    "- 20–40% of the data is in the minority class = mild imbalance\n",
    "- 1–20% of the data is in the minority class = moderate imbalance\n",
    "- < 1% of the data is in the minority class = extreme imbalance\n",
    "\n",
    "To alleviate imbalance, we'll learn two related feature engineering techniques. The first is **downsampling**. Downsampling is the process of reducing the number of majority class observations so that the proportion of the data we use is closer to the proportion of the minority class. To downsample a dataset, we need to choose what proportion of the data we want to be the majority class. Then, we remove enough observations to achieve this.\n",
    "\n",
    "The second technique is called **upweighting**. Whereas downsampling involves manipulating the majority class, upweighting manipulates the minority class. As its name suggests, upweighting involves giving more \"weight\" to the minority class. To understand weights, we can think of every observation in a dataset as having a single \"vote\" of influence. When we train models, each of these observations are treated equally and are used only once. Now, when we upweigh an observation, say one from the minority class, we are giving it more \"votes\" of influence. Instead of being used only once, an upweighted observation will be used multiple times. As a result, the upweighted minority class will have \"more\" observations present in the data, without needing to collect more data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15cf8260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
       "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
       "2       496.0       177.0         7.2574            352100.0        NEAR BAY  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house= pd.read_csv('house.csv')\n",
    "house.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6304f21a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1H OCEAN     9136\n",
       "INLAND        6551\n",
       "NEAR OCEAN    2658\n",
       "NEAR BAY      2290\n",
       "ISLAND           5\n",
       "Name: ocean_proximity, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house['ocean_proximity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98d06ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
       "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
       "2       496.0       177.0         7.2574            352100.0        NEAR BAY  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb7c77fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20640, 10)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e32cd03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "house['near_ocean']= np.where(house['ocean_proximity']== 'NEAR OCEAN', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "456acb6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "      <th>near_ocean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "3    -122.25     37.85                52.0       1274.0           235.0   \n",
       "4    -122.25     37.85                52.0       1627.0           280.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \\\n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY   \n",
       "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY   \n",
       "2       496.0       177.0         7.2574            352100.0        NEAR BAY   \n",
       "3       558.0       219.0         5.6431            341300.0        NEAR BAY   \n",
       "4       565.0       259.0         3.8462            342200.0        NEAR BAY   \n",
       "\n",
       "   near_ocean  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1674b5df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    17982\n",
       "1     2658\n",
       "Name: near_ocean, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house['near_ocean'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b23b0126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1594.8"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.6 * 2658"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eebee722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3987"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_min= 2658\n",
    "n_maj= int(np.ceil((0.6/0.4) *n_min))\n",
    "n_maj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "27f67c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "minority_housing= house[house['near_ocean']==1]\n",
    "majority_housing= house[house['near_ocean']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a1bfc301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3987"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maj_ds= majority_housing.sample(n_maj, random_state=1)\n",
    "len(maj_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b8429814",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_housing= pd.concat([maj_ds, minority_housing])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ac81a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "      <th>near_ocean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5466</th>\n",
       "      <td>-118.47</td>\n",
       "      <td>33.99</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1568.0</td>\n",
       "      <td>501.0</td>\n",
       "      <td>764.0</td>\n",
       "      <td>478.0</td>\n",
       "      <td>3.0150</td>\n",
       "      <td>414300.0</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11296</th>\n",
       "      <td>-117.92</td>\n",
       "      <td>33.79</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2737.0</td>\n",
       "      <td>614.0</td>\n",
       "      <td>1877.0</td>\n",
       "      <td>606.0</td>\n",
       "      <td>2.8622</td>\n",
       "      <td>184300.0</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18996</th>\n",
       "      <td>-121.92</td>\n",
       "      <td>38.37</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2056.0</td>\n",
       "      <td>413.0</td>\n",
       "      <td>933.0</td>\n",
       "      <td>367.0</td>\n",
       "      <td>2.7051</td>\n",
       "      <td>193800.0</td>\n",
       "      <td>INLAND</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1644</th>\n",
       "      <td>-121.94</td>\n",
       "      <td>37.83</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2836.0</td>\n",
       "      <td>373.0</td>\n",
       "      <td>959.0</td>\n",
       "      <td>335.0</td>\n",
       "      <td>10.5815</td>\n",
       "      <td>500001.0</td>\n",
       "      <td>INLAND</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4921</th>\n",
       "      <td>-118.25</td>\n",
       "      <td>34.00</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1218.0</td>\n",
       "      <td>342.0</td>\n",
       "      <td>1292.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>1.5781</td>\n",
       "      <td>102900.0</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "5466     -118.47     33.99                50.0       1568.0           501.0   \n",
       "11296    -117.92     33.79                26.0       2737.0           614.0   \n",
       "18996    -121.92     38.37                26.0       2056.0           413.0   \n",
       "1644     -121.94     37.83                11.0       2836.0           373.0   \n",
       "4921     -118.25     34.00                32.0       1218.0           342.0   \n",
       "\n",
       "       population  households  median_income  median_house_value  \\\n",
       "5466        764.0       478.0         3.0150            414300.0   \n",
       "11296      1877.0       606.0         2.8622            184300.0   \n",
       "18996       933.0       367.0         2.7051            193800.0   \n",
       "1644        959.0       335.0        10.5815            500001.0   \n",
       "4921       1292.0       304.0         1.5781            102900.0   \n",
       "\n",
       "      ocean_proximity  near_ocean  \n",
       "5466        <1H OCEAN           0  \n",
       "11296       <1H OCEAN           0  \n",
       "18996          INLAND           0  \n",
       "1644           INLAND           0  \n",
       "4921        <1H OCEAN           0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "368ae100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3987\n",
       "1    2658\n",
       "Name: near_ocean, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_housing['near_ocean'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d769f690",
   "metadata": {},
   "source": [
    "## Model Selection/ Feature Selection Techniques\n",
    "Welcome to the second lesson in Optimizing Machine Learning Models in Python. In the previous lesson, we learned about different aspects of feature engineering and how to handle different data issues, such as missing data and outliers. Often, we won't know beforehand which features to use or engineer for our models, especially when there are many features to choose from. It would be nice if we could determine which features to use before we invest any time in feature engineering. Instead of having just one model, we'll often need to craft multiple models with different combinations of features to try to maximize predictive ability.\n",
    "\n",
    "### Sequential Feature Selection\n",
    "When we explore the data, we often use visualizations to see how individual features relate to the outcome of interest. With these visualizations, we can pick out features that seem to be strongly correlated with the outcome or help distinguish between cases and non-cases. A weakness of this approach is that it's difficult to consider how these relationships change as we add features to a model. For example, we might notice that two features each show a strong correlation with the outcome of interest.\n",
    "\n",
    "<img src=sf.png width= 400 height=400>\n",
    "\n",
    "Intuitively, we might want to include both features, \n",
    "X1 and X2, in a regression model. What this approach fails to consider is that X1 and X2 might be correlated with each other. If this is the case, having X1 in a model alone will produce similar results to having X2 in a model, but including them both won't add extra benefit. We might mistakenly believe X2 is a useful feature and attempt to use it in future models, making them more complex than they need to be.\n",
    "\n",
    "This weakness highlights a need for an approach that selects groups of features for a model. This process is called **sequential feature selection, also known as subset selection.** scikit-learn provides some functionality for implementing subset selection.\n",
    "\n",
    "There are two main methods of sequential feature selection that we'll learn: **forward selection and backward selection**. Both methods produce a model with a subset of features that perform well based on some metric, such as MSE, but they differ in their approach to how the features are chosen.\n",
    "\n",
    "We'll start with the forward selection algorithm. In forward selection, we start with an intercept-only model (a regression model without predictors). Then, the algorithm iterates over each feature in the dataset to see which one would produce the best model if it were added. The metric used to define \"best\" is a cross-validation score such as MSE or accuracy. After iterating through each feature, the one that produces the best metric is added to the model. Once this feature is added, this process is repeated until we reach some pre-specified number of features or the metric does not improve substantially with the addition of more features.\n",
    "\n",
    "`from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.linear_model import LinearRegression`\n",
    "\n",
    "`lm = LinearRegression()\n",
    "forward_lm = SequentialFeatureSelector(estimator=lm,\n",
    "                                       n_features_to_select=2, \n",
    "                                       direction=\"forward\")`\n",
    "`forward_lm.fit(X, y)`\n",
    "\n",
    "- estimator: this is an object used to construct the models during feature selection, such as LinearRegression or RandomForestClassifier.\n",
    "- n_features_to_select: this is a positive integer that describes how many features we want to be used in the resulting model.\n",
    "- direction: this is a string that describes the type of sequential feature selection we want to do (\"forward\" or \"backward\") as a string.\n",
    "It's important to note that filling in the parameters above merely describes a sort of \"plan\" for what we want to do. No fitting is done yet, so we must call the fit() method in order to actually perform the feature selection.\n",
    "\n",
    "In order to see what features were actually included in the model, we can use the get_feature_names_out() method after creating a SequentialFeatureSelector() object and calling the fit() method on it. It's critical to note that the SequentialFeatureSelector() object does not create a model with the features it selected. It uses the given estimator to select features, and then these features can be used later to actually fit a model. To store the names of the features in an array called features, use the following syntax:\n",
    "\n",
    "`features= forward_lm.get_feature_names_out()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1de20657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "longitude             0\n",
       "latitude              0\n",
       "housing_median_age    0\n",
       "total_rooms           0\n",
       "total_bedrooms        0\n",
       "population            0\n",
       "households            0\n",
       "median_income         0\n",
       "median_house_value    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2034fc7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "longitude             float64\n",
       "latitude              float64\n",
       "housing_median_age    float64\n",
       "total_rooms           float64\n",
       "total_bedrooms        float64\n",
       "population            float64\n",
       "households            float64\n",
       "median_income         float64\n",
       "median_house_value    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7a35fe0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['housing_median_age', 'total_rooms', 'total_bedrooms',\n",
       "       'population', 'median_income'], dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "x= housing.drop(columns=['median_house_value'], axis=1)\n",
    "y= housing['median_house_value']\n",
    "\n",
    "\n",
    "\n",
    "lm= LinearRegression()\n",
    "\n",
    "forward_lm= SequentialFeatureSelector(estimator=lm,\n",
    "                                     n_features_to_select= 5,\n",
    "                                     direction= 'forward')\n",
    "\n",
    "forward_lm.fit(x,y)\n",
    "f_features= forward_lm.get_feature_names_out()\n",
    "\n",
    "f_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7b53f7",
   "metadata": {},
   "source": [
    "### Backward Selection\n",
    "Now that we know about forward selection, let's see how it compares to backward selection. Forward selection starts with an intercept-only model, while backward selection starts with a full model that uses all the features in the dataset.\n",
    "\n",
    "Backward selection then removes a single feature from the model and reevaluates the metric. The algorithm repeats this process for every feature present in the full model. This results in several metrics, each evaluated on a model where a single predictor is removed. Backward selection chooses the model with the best metric and repeats until we reach the desired number of features specified by the n_features_to_select argument.\n",
    "\n",
    "Thankfully, backward selection is also implemented by the SequentialFeatureSelector class. We only need to change the direction parameter from \"forward\" to \"backward\".\n",
    "\n",
    "`from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.linear_model import LinearRegression`\n",
    "\n",
    "`lm = LinearRegression()`\n",
    "\n",
    "`backward_lm = SequentialFeatureSelector(estimator=lm,\n",
    "                                        n_features_to_select=2, \n",
    "                                        direction=\"backward\")`\n",
    "`backward_lm.fit(X, y)`\n",
    "\n",
    "When should we use forward vs. backward selection? There's no explicit rule that defines which one we should use, so consider them as two different strategies to create candidate models. Forward and backward selection don't necessarily select the same features, so don't be surprised if this happens. We may try both methods on a training set and then see which one performs better on a test set.\n",
    "\n",
    "It's worth mentioning that both forward and backward selection are forms of greedy algorithms. Both of these algorithms make decisions based on the current set of features, and they choose whichever feature best improves the metric immediately. This does not guarantee that the subset of features chosen is actually the optimal subset. To find the optimal subset of features, we would need to test all different combinations of predictors. If there are \n",
    "p predictors, then we would need to evaluate 2p models. This can be time-consuming for even a moderate number of predictors, so greedy approaches like forward or backward selection get around this problem by choosing from current subsets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "885f17b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['longitude', 'latitude', 'total_bedrooms', 'population',\n",
       "       'median_income'], dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lm= LinearRegression()\n",
    "\n",
    "backward_lm= SequentialFeatureSelector(estimator=lm,\n",
    "                                      n_features_to_select=5,\n",
    "                                      direction='backward')\n",
    "backward_lm.fit(x,y)\n",
    "\n",
    "b_features=backward_lm.get_feature_names_out()\n",
    "b_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb566194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
       "       'total_bedrooms', 'population', 'households', 'median_income',\n",
       "       'median_house_value'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ffbbce9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result= sum(f_features==b_features)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad641024",
   "metadata": {},
   "source": [
    "### Criterion-Based Selection\n",
    "Forward and backward selection give us algorithms to quickly select a reasonable subset of features for a candidate model. These algorithms enable us to create many different models, each with a different number of features. We know that we can then compare these models based on their test error, and in many cases, we can solely rely on the results of this comparison to select a model.\n",
    "\n",
    "There are some occasions when we might need another tool. One such occasion is when we have limited data, and by extension, don't have much data to allocate to a test set. When this happens, we may want to use a criterion-based approach for model selection. We'll compute some criterion (other than test error) for every candidate model that we have and then choose the model that produces the best criterion. \n",
    "There are several criteria we can use, and the first one we'll look at is the Akaike Information Criterion (AIC). **The Akaike Information Criterion** is a value that estimates the prediction error of a model. Since we're usually interested in predictive ability, the AIC is a tool we can reasonably use for model selection. We calculate the AIC like this:\n",
    "\n",
    "`AIC= 2p - 2in(L)`\n",
    "where p represents the number of features used in the model, and ^L represents the likelihood of the model. Taking the natural log (i.e.,ln) of L gives us the log-likelihood. Without delving into the specifics, the log-likelihood captures how well the model \"fits\" the data. In most cases, calculating L is complicated and can be done using helpful functions in Python.\n",
    "\n",
    "For teaching purposes though, we'll look at linear regression for this screen. With linear regression, the likelihood $L^$\n",
    " of the model is proportional to sum of squared error : (SSE)= $(sum)_{i=1}^{n}(error)^{2}$ where \n",
    "ϵ\n",
    " represents the error (i.e., the difference between the actual values in the dataset and predicted values from our model). Without delving into the derivation, the AIC can be written in terms of the SSE as follows:\n",
    " \n",
    "AIC= 2p + nIn(SSE)\n",
    " \n",
    "where n represents the number of observations in the dataset. Notice how the sign flips in the second term compared to the first equation. When comparing multiple models, the best model as judged by AIC will be the one with the smallest AIC. When using the AIC, **smaller is better.** Let's try creating a few models and use the AIC to compare them.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "86ce6f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['housing_median_age' 'median_income']\n",
      "['longitude' 'latitude' 'total_bedrooms' 'population' 'median_income']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lm1=LinearRegression()\n",
    "lm2=LinearRegression()\n",
    "\n",
    "forward_lm1= SequentialFeatureSelector(estimator=lm1,\n",
    "                                      n_features_to_select=2,\n",
    "                                      direction='forward')\n",
    "\n",
    "backward_lm2= SequentialFeatureSelector(estimator=lm2,\n",
    "                                       n_features_to_select=5,\n",
    "                                       direction='backward')\n",
    "forward_lm1.fit(x,y)\n",
    "backward_lm2.fit(x,y)\n",
    "\n",
    "features2=forward_lm1.get_feature_names_out()\n",
    "features5=backward_lm2.get_feature_names_out()\n",
    "\n",
    "print(features2)\n",
    "print(features5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f146fa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2= housing[['housing_median_age','median_income']]\n",
    "x5= housing[['longitude', 'latitude', 'total_bedrooms', 'population' ,'median_income']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7b63e420",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm2= LinearRegression()\n",
    "lm5= LinearRegression()\n",
    "\n",
    "lm2.fit(x2, y)\n",
    "lm5.fit(x5, y)\n",
    "\n",
    "y_pred1= lm2.predict(x2)\n",
    "y_pred2= lm5.predict(x5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c50e946b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20640"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n= x.shape[0]\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5209119a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6536319572.710422\n",
      "5075609184.321324\n"
     ]
    }
   ],
   "source": [
    "mse1= mean_squared_error(y_pred1, y)\n",
    "mse2= mean_squared_error(y_pred2, y)\n",
    "\n",
    "print(mse1)\n",
    "print(mse2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "36dfe0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134909635980744.0 104760573564393.0\n"
     ]
    }
   ],
   "source": [
    "sse2= np.ceil(n*mse1)\n",
    "sse5= np.ceil(n*mse2)\n",
    "\n",
    "print(sse2,sse5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de41240",
   "metadata": {},
   "source": [
    "### Other Criterion\n",
    "Learn\n",
    "We've discussed the AIC as if it were another tool in our kit. As we mentioned before, criteria like the AIC are useful when we don't have a lot of data. There are several other criteria available, which we'll mention here but not discuss in detail. However, we'll point out an important similarity between all of these criteria.\n",
    "\n",
    "<img src=bs.png width= 800 height=800>\n",
    "\n",
    "<img src=rs.png width=800 height=800>\n",
    "\n",
    "The adjusted R2 incorporates a term involving the number of features (p) that serves to penalize complex models. If the model gets larger without offering enough benefit to reducing error, it decreases the adjusted R2\n",
    ". So, when we're picking among many models with different numbers of predictors, we can choose the one with the highest adjusted R2 .The adjusted R2 can, in theory, reach an upper threshold of 1, but this would imply that the model produces perfect predictions, and there would be no need for model selection. However, in practice, no model is a perfect predictor.\n",
    "\n",
    "When we use AIC or BIC, we choose models based on the smallest value. When using the adjusted R2\n",
    ", we must choose the model with the largest value. When using these metrics, this is a subtlety that needs to be kept in mind, or you might be led to incorrect conclusions.\n",
    "\n",
    "A common thread that unites the AIC, BIC, and adjusted R2\n",
    " is that they have some sort of penalty for having complex models. This penalty allows us to strike a balance between choosing a good set of features and overfitting the model. This is something that simply looking at the test error does not directly tell us. Comparing training and test error gives us a hint, but these criteria we've learned give us another way to do so.\n",
    "\n",
    "Another characteristic that these criteria share is that they are relative measures. There is no such thing as a model that produces the best absolute AIC, BIC, or adjusted R2\n",
    "; they can only be used to compare candidate models with each other. Furthermore, these measures are specific to linear regression. If we're working with other types of models, then we may need to stick to forward selection or cross-validation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cfae56",
   "metadata": {},
   "source": [
    "### The Curse of Dimensionality\n",
    "As we conclude this lesson, we need to discuss a fundamental issue in machine learning — dimensionality. In this lesson, we've used the housing dataset to practice the concepts we've learned. In housing, the number of observations (n) vastly outnumbers the number of predictors (p) in the dataset. In this context, there's no trouble creating models like regressions, but this won't always be the case. In other situations, the reverse might be true: a large number of features but a smaller number of observations. When this happens, we say that the dataset has **high dimensionality.**\n",
    "\n",
    "In some cases, such as linear regression, high dimensionality prevents us from even creating a model when n < p\n",
    ". The math simply won't allow it. When this happens, we are prevented from using model selection features like backward selection since we can't construct the model in the first place. This motivates introducing techniques for performing model selection in high-dimensional situations. We'll learn some strategies for dealing with high-dimensional datasets later in the course, but it's worth introducing the idea here.\n",
    "\n",
    "A common theme when dealing with high dimensionality is to strategically reduce the number of features we need to consider for the model. This process is called **dimension reduction.** One popular technique for dimension reduction is **principal component analysis (PCA).** PCA seeks to reduce the dimensions of the data by creating a new dataset with features based off of the original data. This new dataset will have fewer features than observations, which allows us to use all of the techniques we've learned so far.\n",
    "\n",
    "A feature created by PCA is called a principal component. A principal component z is a linear combination of the original features (X). So, the first principal component z1 would be constructed as\n",
    "\n",
    "We won't go too deep into the math; rather we'll create a picture of what PCA is doing. The weights a1,…,ap\n",
    " are chosen by PCA such that the first principal component contains the most variance in the original dataset. Having high variance means that the values of z1 will be highly spread out. Being more spread out allows us to see how the outcome Y\n",
    " can vary with z1 while preserving the variance in the original dataset.\n",
    "\n",
    "After the first principal component is made, the second is constructed similarly, using all of the variance remaining after the first one is created. The other principal components are created iteratively in this way.\n",
    "\n",
    "Done this way, the first principal component will explain the most variance, and the second wil explain the second-most. When the number of principal components equals the original number of features p\n",
    ", then the original dataset is essentially recreated. We can reduce dimensionality by only choosing a small number of principal components that explain some high degree of variance in the original dataset, say 90%.\n",
    "\n",
    "scikit-learn implements PCA under the PCA class in the decomposition module. When we instantiate a PCA() object, we choose the number of principal components we want to create. Then, we can fit this PCA() object on a dataset.\n",
    "\n",
    "`from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(X)`\n",
    "\n",
    "To understand how much variance is explained by each principal component, we can use the explained_variance_ratio_ attribute. This attribute returns an array of values, one for each principal component. The first element in this array is a number representing the proportion of remaining variance explained by the first component. The second element corresponds to the second principal component, and so on. To get the total variance explained, we can take the sum() of the explained_variance_ratio_ attribute to get a float value that represents the percentage of the variance being explained.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e930704f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.49964146e-01 4.47150177e-02 4.74927571e-03 5.48403265e-04\n",
      " 2.14651975e-05]\n",
      "0.9999983082728477\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca= PCA(n_components=5)\n",
    "pca.fit(x)\n",
    "\n",
    "var= pca.explained_variance_ratio_\n",
    "toat_expvar= sum(var)\n",
    "print(var)\n",
    "print(sum(var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b382d204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20640"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n= housing.shape[0]\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "74a59c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185760"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_exp= int(np.ceil((0.9/0.1) * n))\n",
    "var_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0db824",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "Welcome to the third lesson in Optimizing Machine Learning Models in Python. We've learned how to optimize models from the perspective of changing and choosing features. We'll now return our attention to a concept we learned early in the machine learning path: the train-test split approach.\n",
    "\n",
    "Up to this point, we have been satisfied with splitting the data into two sets, training and test, and reporting our predictive results on the test set. While this approach works, there are better ways to calculate and understand the test error of a model. We often judge model quality entirely on its ability to make accurate predictions on unseen data, so we should be sure that they represent the model's capabilities.\n",
    "\n",
    "First, we'll learn what problems might come from drawing conclusions from just a single split. Then we'll learn different strategies for how to overcome these problems and how to implement these strategies in scikit-learn. As before, we will use the California Housing Prices to practice.\n",
    "\n",
    "One way this randomness can hurt us is when it results in a training and test set that are markedly different. By virtue of how the data was randomized, we get a single test metric. How do we reconcile these differences in test error? What is the relationship between these test errors, if any?\n",
    "\n",
    "A second problem comes from the presence outliers. As a mental exercise, imagine that we have a dataset with 100 observations, and we are doing an 80-20 split for a linear regression problem. Then, imagine that just one of these observations is an outlier that we cannot reasonably remove from the data. When we perform the split, the outlier will end up either in the training set or in the test set. In either of these cases, our model predictions are potentially harmed. This is because of the following:\n",
    "\n",
    "- If the outlier is in the training set, it can adversely affect the model estimates. Then, we'll get poor test predictions overall.\n",
    "- If the outlier is in the test set, our model estimates won't be affected, but we'll have one prediction for the outlier that might be extremely far from the actual observation because our model wasn't trained on similar data. This, in turn, inflates our test mean squared error.\n",
    "\n",
    "In either case, a single extreme observation influences the test predictions and worsens model performance. We might hastily conclude that the model itself is bad, where instead the metric itself is not truly representative of performance on unseen data. In our exercise, we use an outlier to demonstrate how test predictions can be affected, but there are plenty of other factors that can do this (e.g., missing data).\n",
    "\n",
    "Instead of using just one test error to judge the model, we can create a better picture of model performance by using multiple training and test sets using the original dataset and separate rounds of training. With multiple rounds of training comes multiple test errors, which we can examine to understand not only how well the model performs, but how well this performance varies. This is the core idea behind **K-fold cross-validation**, which we'll learn to incorporate into our machine learning workflow.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "19963666",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mses=[]\n",
    "\n",
    "x= housing[['median_house_value','housing_median_age','total_rooms','population']]\n",
    "y= housing['median_income']\n",
    "\n",
    "for i in range(5):\n",
    "    x_train,x_test,y_train,y_test= train_test_split(x,y, random_state=i, test_size=40, shuffle=True)\n",
    "    lm= LinearRegression()\n",
    "    lm.fit(x_train, y_train)\n",
    "    y_pred= lm.predict(x_test)\n",
    "    mse= mean_squared_error(y_pred, y_test)\n",
    "    mses.append(mse)\n",
    "    avg_mses= np.mean(mses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3b6395dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.918644929377133, 1.1398779302510404, 1.2621620920224388, 1.0962093617652688, 1.1504573653243335]\n"
     ]
    }
   ],
   "source": [
    "print(mses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aa4d1442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1134703357480429"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_mses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1805388e",
   "metadata": {},
   "source": [
    "#### Two-Fold Cross-Validation\n",
    "On the previous screen, we saw that the test error can vary, depending on what subset of the data was used for training and testing. While these differences may or may not be significant, they can provide us with valuable insights about our model. Instead of relying on a single test score to understand model performance, we should actually look at the distribution of these scores over different subsets of the data.\n",
    "\n",
    "We used random subsets on the previous screen to divide our data. A more principled way to approach this would be to divide the entire dataset into evenly sized groups of observations called **folds**. Folds are equal, mutually exclusive divisions of the data. Let's visualize how a dataset might be divided into two folds:\n",
    "\n",
    "<img src='2fold.svg' width=500 height=500>\n",
    "\n",
    "By dividing the data this way, each fold will be used for both training and testing purposes. This approach enables us to have two estimates for the test error — one for each iteration of training. We can take the average of these estimates to give us a better estimate of the predictive ability of the model. Even if an outlier causes one MSE to be higher than expected, taking the average will help balance this out.\n",
    "\n",
    "This approach is called two-fold cross-validation. Cross-validation is an iterative process. We iterate over each fold and use it as the test set, using the other as the training set. By doing this, each observation in the dataset has the chance to be used for both training and testing. This process is similar to the exercise on the previous screen, but it's more methodical because the data isn't being randomly chosen. We'll implement cross-validation by hand on this screen, but we'll see how we can use scikit-learn to implement a more generalized version of it automatically.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3f808b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20640\n",
      "10320.0\n"
     ]
    }
   ],
   "source": [
    "dim=housing.shape[0]\n",
    "mid=housing.shape[0]/2\n",
    "print(dim)\n",
    "print(mid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fa13e98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1= housing.iloc[0:10320,]\n",
    "df2= housing.iloc[10321:20640,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "745916cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "3    -122.25     37.85                52.0       1274.0           235.0   \n",
       "4    -122.25     37.85                52.0       1627.0           280.0   \n",
       "\n",
       "   population  households  median_income  median_house_value  \n",
       "0       322.0       126.0         8.3252            452600.0  \n",
       "1      2401.0      1138.0         8.3014            358500.0  \n",
       "2       496.0       177.0         7.2574            352100.0  \n",
       "3       558.0       219.0         5.6431            341300.0  \n",
       "4       565.0       259.0         3.8462            342200.0  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e103aeb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10321</th>\n",
       "      <td>-117.78</td>\n",
       "      <td>33.86</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4390.0</td>\n",
       "      <td>660.0</td>\n",
       "      <td>2146.0</td>\n",
       "      <td>633.0</td>\n",
       "      <td>6.1504</td>\n",
       "      <td>266000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10322</th>\n",
       "      <td>-117.76</td>\n",
       "      <td>33.87</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3182.0</td>\n",
       "      <td>429.0</td>\n",
       "      <td>1663.0</td>\n",
       "      <td>428.0</td>\n",
       "      <td>7.0592</td>\n",
       "      <td>288200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10323</th>\n",
       "      <td>-117.78</td>\n",
       "      <td>33.81</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1986.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>826.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>7.7752</td>\n",
       "      <td>380000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10324</th>\n",
       "      <td>-117.77</td>\n",
       "      <td>33.80</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3973.0</td>\n",
       "      <td>483.0</td>\n",
       "      <td>1373.0</td>\n",
       "      <td>452.0</td>\n",
       "      <td>9.8074</td>\n",
       "      <td>417000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10325</th>\n",
       "      <td>-117.80</td>\n",
       "      <td>33.79</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>362.0</td>\n",
       "      <td>1081.0</td>\n",
       "      <td>341.0</td>\n",
       "      <td>4.3269</td>\n",
       "      <td>231400.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "10321    -117.78     33.86                16.0       4390.0           660.0   \n",
       "10322    -117.76     33.87                16.0       3182.0           429.0   \n",
       "10323    -117.78     33.81                23.0       1986.0           278.0   \n",
       "10324    -117.77     33.80                16.0       3973.0           483.0   \n",
       "10325    -117.80     33.79                13.0       2021.0           362.0   \n",
       "\n",
       "       population  households  median_income  median_house_value  \n",
       "10321      2146.0       633.0         6.1504            266000.0  \n",
       "10322      1663.0       428.0         7.0592            288200.0  \n",
       "10323       826.0       260.0         7.7752            380000.0  \n",
       "10324      1373.0       452.0         9.8074            417000.0  \n",
       "10325      1081.0       341.0         4.3269            231400.0  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b80224fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2678640002394417\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lm=LinearRegression()\n",
    "\n",
    "x_train=df1.drop(columns=['median_income'],axis=1)\n",
    "y_train=df1['median_income']\n",
    "\n",
    "x_test=df2.drop(columns=['median_income'],axis=1)\n",
    "y_test=df2['median_income']\n",
    "\n",
    "lm.fit(x_train, y_train)\n",
    "pred=lm.predict(x_test)\n",
    "\n",
    "mse_first_fold= mean_squared_error(pred, y_test)\n",
    "print(mse_first_fold)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "031f1c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3011088816309084\n"
     ]
    }
   ],
   "source": [
    "x_train= df2.drop(columns=['median_income'],axis=1)\n",
    "y_train= df2['median_income']\n",
    "\n",
    "x_test= df1.drop(columns=['median_income'],axis=1)\n",
    "y_test= df1['median_income']\n",
    "\n",
    "lm.fit(x_train, y_train)\n",
    "pred= lm.predict(x_test)\n",
    "\n",
    "mse_second_fold= mean_squared_error(pred, y_test)\n",
    "print(mse_second_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "698bc291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.284486440935175"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_mses= (mse_first_fold+mse_second_fold)/2\n",
    "avg_mses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35929f37",
   "metadata": {},
   "source": [
    "#### K-Fold Cross-Validation\n",
    "Now that we've manually implemented a quick two-fold cross-validation, we can move on to the more general form. Instead of using only two folds, we can choose an arbitrary number of folds, which leads us to k-fold cross-validation. Let's extend the previous visualization to reflect this.\n",
    "<img src='kfold.svg' width=500 height=500>\n",
    "\n",
    "As before, the dataset will be divided into \n",
    "k\n",
    " folds of equal or approximately equal size. Then one fold will be used as the test set, while the remaining \n",
    "k\n",
    "−\n",
    "1\n",
    " folds will be used for the training set. With \n",
    "k\n",
    " folds, we'll also get \n",
    "k\n",
    " test MSEs, which means we can calculate not only their average but also their standard deviation. While \n",
    "k\n",
    " is generally considered arbitrary, a general rule is to use k=5 or k=10. We'll explain later why this is the case.\n",
    "\n",
    "Instead of having to implement k-fold cross-validation by hand, we can turn to scikit-learn. We implement K-fold cross-validation with the cross_val_score() function, which is contained in the model_selection module. Although the cross_val_score() function has many arguments, we'll cover the five main ones you should know:\n",
    "\n",
    "- estimator: this should be a model that we want to perform cross-validation on, like LinearRegression or LogisticRegression. The model doesn't need to be fit to data, just instantiated from the class.\n",
    "- X: this is the predictor variable(s) that we'd like to use in the model.\n",
    "- y: this is the outcome of interest\n",
    "- cv: this corresponds to how many folds we want to use in the cross-validation. While this already defaults to 5, it's good to know just in case you'd like to change it.\n",
    "- scoring: this corresponds to the metric that we should use to judge the model. This is important because we might be interested in other types of metrics other than the default for the model\n",
    "\n",
    "`from sklearn.model_selection import cross_val_score \n",
    "from linear_model import LinearRegression`\n",
    "\n",
    "`model = LinearRegression()\n",
    "fold_mses = cross_val_score(model, X, y, cv=5, scoring=\"neg_mean_squared_error\")`\n",
    "\n",
    "The output of cross_val_score() is an array containing all of the test scores. The default score function for LinearRegression is actually R2\n",
    ", not the mean squared error. To use MSE, we needed to indicate this with the scoring parameter. The cross_val_score() function actually checks what type of model is given to the estimator (first) argument, and it uses the default score for that model if scoring isn't specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7b7affe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "\n",
       "   population  households  median_income  median_house_value  \n",
       "0       322.0       126.0         8.3252            452600.0  \n",
       "1      2401.0      1138.0         8.3014            358500.0  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "61885900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x= housing.drop(columns=['median_income'], axis=1)\n",
    "y= housing['median_income']\n",
    "\n",
    "model= LinearRegression()\n",
    "\n",
    "housing_cv= cross_val_score(model, x,y, cv=5, scoring='neg_mean_squared_error')\n",
    "avg_mses= np.mean(housing_cv)\n",
    "std_mses= np.std(housing_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "584058d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.96815306 -1.45865495 -1.53997063 -1.07167341 -1.37704749]\n",
      "-1.2830999097125555\n",
      "0.2233924670086638\n"
     ]
    }
   ],
   "source": [
    "print(housing_cv)\n",
    "print(avg_mses)\n",
    "print(std_mses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9a076319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x= housing.drop(columns=['median_income'], axis=1)\n",
    "y= housing['median_income']\n",
    "\n",
    "model= LinearRegression()\n",
    "\n",
    "housing_cv= cross_val_score(model, x,y, cv=10, scoring='neg_mean_squared_error')\n",
    "avg_mses= np.mean(housing_cv)\n",
    "std_mses= np.std(housing_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6c473e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.03456358 -0.89840374 -1.95453636 -0.974117   -1.6192793  -1.44084322\n",
      " -0.79234085 -1.35760474 -1.93898022 -0.83755882]\n",
      "-1.2848227824695093\n",
      "0.4205959949629612\n"
     ]
    }
   ],
   "source": [
    "print(housing_cv)\n",
    "print(avg_mses)\n",
    "print(std_mses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e44b75",
   "metadata": {},
   "source": [
    "#### The Bias-Variance Tradeoff\n",
    "Between the extremes of LOOCV and two-fold cross-validation, we saw on the previous screen that as the number of folds used in K-fold cross-validation increases, the variance of the resulting test errors increases. At the same time, taking the average of this large number of test errors gives us a more accurate estimate of what it might be when used on unseen data. In other words, increasing the number of folds reduces the bias of the average test error, but it increases their variance. With a small number of folds, these trends are reversed.\n",
    "\n",
    "\n",
    "K-fold cross-validation comes with one form of the **bias-variance tradeoff.** There are several forms of bias-variance tradeoff in machine learning, but this particular one involves the test error. Ideally, we'd like to strike a balance between minimizing the bias (because we want to know in general how well the model will perform) and minimizing the variance of the test errors (because we want to know how our error might vary with different datasets). Researchers have performed numerical simulations and found that k=5 or k=10 produce this desired balance.\n",
    "\n",
    "Ultimately, whether we choose k=5 or k=10 isn't really important. It's a best practice to choose a value for \n",
    " and make it explicit in your report so that others may replicate your methodology. Cross-validation is an essential skill for data scientists, so we recommend performing k-fold cross-validation whenever possible. Thanks to scikit-learn, implementing it is as easy as just a few lines of code. It may be good to go back to previous personal projects and implement cross-validation wherever possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b7f074",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "Welcome to the fourth lesson in Optimizing Machine Learning Models in Python. In this lesson, we'll turn our attention to high-dimensional contexts. High dimensional datasets have many features, possibly greater than the number of observations. In these cases, many features might be helpful in predicting the outcome, but it might be difficult to pick all of them out. We might try to use an automated procedure like sequential feature selection to choose features, but these algorithms are computationally expensive in high-dimensional contexts and might not be a good use of our time.\n",
    "\n",
    "To discover one of the potential solutions to this problem, we'll learn about some techniques on **regularization.** Regularization is the process of simplifying a model. Simplifying a model helps us overcome high-dimesional issues because it allows us to discard features that don't contribute to the model's predictive ability. In this lesson, we'll learn about regularization in the context of linear models.\n",
    "\n",
    "Regularization is the process of simplifying a model so it generalizes better to unseen data. What forms do these simplifications take? Let's use linear regression as an example.\n",
    "\n",
    "The more predictors we include in the model, the more complex it gets. With more complex models, we can get better predictions but not necessarily on the test data. We must always be mindful of overfitting, so we should consider eliminating features that don't provide additional predictive power to the model. We don't know beforehand which features are valuable or not, so we need an approach to eliminate them via regularization.\n",
    "\n",
    "One way to simplify the model above is to force some of the coefficients to be close to zero or equal to zero. Forcing coefficients to be close to zero (or equal to zero) essentially removes the associated feature from the model, which ultimately simplifies it. Regularization is also called shrinkage, owing to the fact that coefficients are shrunk to zero or close to it.\n",
    "\n",
    "We will cover two extensions of the linear regression model that allow us to regularize the coefficients: **ridge regression and LASSO.** These models are both regularized versions of linear regression, but they approach shrinkage in different ways. We'll explore both of these models in this lesson and how they perform regularization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "023d2ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "\n",
       "   population  households  median_income  median_house_value  \n",
       "0       322.0       126.0         8.3252            452600.0  \n",
       "1      2401.0      1138.0         8.3014            358500.0  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "02e24a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= housing.drop(columns=['median_income'], axis=1)\n",
    "y= housing['median_income']\n",
    "\n",
    "x_train,x_test,y_train,y_test= train_test_split(x,y, test_size= 0.20,random_state=762, shuffle=True)\n",
    "\n",
    "lm= LinearRegression()\n",
    "\n",
    "lm.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c192b5",
   "metadata": {},
   "source": [
    "The first extension of linear regression we'll learn is ridge regression. A ridge regression model has the same form as a linear regression model, but its coefficient values are regularized closer to 0. With the same form as linear regression, what differentiates ridge regression from its unregularized cousin? The difference comes in the loss function.\n",
    "\n",
    "In standard linear regression, the coefficients are estimated by minimizing the mean squared error (MSE). Recall that the MSE is the **loss function** for linear regression and is defined as follows:\n",
    "\n",
    "<img src='regularize.png' width=600 height=600>\n",
    "\n",
    "This second penalty term is how ridge regression regularizes its coefficients. When the coefficients get large, this penalty term also gets larger as a result. Unless a larger coefficient significantly reduces the MSE, the coefficients will be shrunk towards zero. The penalty also incorporates a **tuning parameter** into the penalty term, denoted by α\n",
    ". High values of α will give the penalty term more weight, which encourages smaller regression coefficients, while smaller values of α do the opposite. α is a hyperparameter for ridge regression, so it should be chosen through cross-validation as opposed to being hand-picked. Notice that when a = 0 , we get back to the MSE, returning us to standard linear regression.\n",
    "\n",
    "scikit-learn has a class dedicated for ridge regression: the RidgeCV class. The RidgeCV class is also contained in the linear_model module, the same one as LinearRegression. There's another Ridge class that implements ridge regression, but RidgeCV has cross-validation built into it, so this is preferred.\n",
    "\n",
    "We show a pseudocode implementation below:\n",
    "\n",
    "`from sklearn.linear_model import RidgeCV\n",
    "model = RidgeCV()\n",
    "model.fit(X, y)`\n",
    "\n",
    "With RidgeCV, there's a coef_ attribute that allows us to examine the estimated values of the coefficients of the model. Each feature we use in the model will have a coefficient, and the coefficient values in coef_ will appear in the same order in which the features are used in the model. Our example code doesn't show any arguments when creating the RidgeCV() object, but there are two that are worth introducing:\n",
    "\n",
    "- alphas: this is an array of positive values to test for cross-validation. We can also specify a single value for the argument as well. RidgeCV has default values here, but we may want to use a different range or magnitude for the α values we want to validate with.\n",
    "- cv: this value indicates how many folds to use in cross-validation. By default, RidgeCV implements efficient LOOCV to determine the best α value to use among the alphas values we provide.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "58356d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "lm_1= RidgeCV(alphas=1)\n",
    "lm_100= RidgeCV(alphas=100)\n",
    "\n",
    "lm_1.fit(x_train, y_train)\n",
    "lm_100.fit(x_train, y_train)\n",
    "\n",
    "ridge_1_coef= lm_1.coef_\n",
    "ridge_100_coef= lm_100.coef_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0f202911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.93547653e-02  1.11094933e-03 -2.17602100e-02  8.93842720e-04\n",
      " -4.26786994e-03  8.27726326e-05 -7.23395409e-04  1.06981024e-05]\n"
     ]
    }
   ],
   "source": [
    "print(ridge_1_coef)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c644d637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.90570633e-02  8.41233637e-04 -2.17623565e-02  8.93896018e-04\n",
      " -4.26768954e-03  8.27290492e-05 -7.23776334e-04  1.02936756e-05]\n"
     ]
    }
   ],
   "source": [
    "print(ridge_100_coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dcafb6",
   "metadata": {},
   "source": [
    "#### Optimizing the Tuning Parameter\n",
    "Ridge regression constrains its coefficients through an extra penalty term in its loss function. We can control how much regularization occurs through the tuning parameter α, but it's a best practice to use cross-validation to select one that reduces validation error. Higher values of α reduce the coefficients more because it increases the value of the penalty term. The penalty term forces the model to compromise between higher coefficient values to reduce the MSE and raising the value of the penalty term.\n",
    "\n",
    "The RidgeCV class implements efficient LOOCV to find the best value for α\n",
    ", but unfortunately, the default values used by the class aren't ideal. By default, the value for the alphas argument is [0.1, 1, 10]. It's highly likely that the best value for α lies somewhere outside of these values, so we want to outline an approach for finding one.\n",
    "\n",
    "The process for finding an optimal tuning parameter is an iterative one. It requires us to assign different lists for the alphas parameter with increasing precision until we find an adequate answer. \n",
    "\n",
    "`ridge = RidgeCV(alphas = np.linspace(0.1, 10, num=100))`\n",
    "\n",
    "The linspace() function above from NumPy gives us an array of numbers from 0.1 to 10 in evenly spaced increments. The num parameter controls how many increments there should be. This will give us more \n",
    "α\n",
    " values to cross-validate with, and more values to optimize on. Increasing this would be useful if we found that the optimal α\n",
    " lies between the minimum and maximum we provided, and we wanted to explore more granular values to home in on the best one.\n",
    " \n",
    " There's also an edge case we need to consider. In the above code, we assume the correct value for \n",
    "α\n",
    " is contained within the range of 0.1 and 10, but what if it isn't? One way we'll be able to notice this is if one of these two extremes ends up being the value of \n",
    "α\n",
    " chosen. This result signifies that we should change the bounds of linspace().\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c5fb81be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n"
     ]
    }
   ],
   "source": [
    "ridge_initial= RidgeCV(alphas= np.linspace(10,100, num=100))\n",
    "\n",
    "ridge_initial.fit(x_train, y_train)\n",
    "\n",
    "alpha_initial= ridge_initial.alpha_\n",
    "print(alpha_initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9dc0f998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000.0\n"
     ]
    }
   ],
   "source": [
    "#try another one since one of the boundaries was chosen\n",
    "\n",
    "ridge_better= RidgeCV(alphas= np.linspace(100, 1000, num=100))\n",
    "ridge_better.fit(x_train, y_train)\n",
    "\n",
    "print(ridge_better.alpha_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbfa24b",
   "metadata": {},
   "source": [
    "#### Standardization\n",
    "Although we're fully equipped to use ridge regression, before we move on, there's an important issue that we've ignored until now. Recall that the coefficients of linear regression are interpreted as average changes to the outcome for unit changes in the features. Therefore, the magnitude of these coefficients depends heavily on the magnitude of the feature.\n",
    "\n",
    "This can become a problem if we have features of different magnitudes. We can see this in the housing dataset. The housing_median_age column is on a scale of 10s, but the population column ranges into the  1000s. This difference in magnitudes can have adverse effects on regularized models like ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b6520ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "\n",
       "   population  households  median_income  median_house_value  \n",
       "0       322.0       126.0         8.3252            452600.0  \n",
       "1      2401.0      1138.0         8.3014            358500.0  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "df1dd666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([41., 21., 52., 42., 50., 40., 49., 48., 51., 43.,  2., 46., 26.,\n",
       "       20., 17., 36., 19., 23., 38., 35., 10., 16., 27., 39., 31., 29.,\n",
       "       22., 37., 28., 34., 32., 47., 44., 30., 18., 45., 33., 24., 15.,\n",
       "       14., 13., 25.,  5., 12.,  6.,  8.,  9.,  7.,  3.,  4., 11.,  1.])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing['housing_median_age'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "55598d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 322., 2401.,  496., ..., 3060., 2707., 6912.])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing['population'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "30511206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-119.569704</td>\n",
       "      <td>35.631861</td>\n",
       "      <td>28.639486</td>\n",
       "      <td>2635.763081</td>\n",
       "      <td>536.838857</td>\n",
       "      <td>1425.476744</td>\n",
       "      <td>499.539680</td>\n",
       "      <td>3.870671</td>\n",
       "      <td>206855.816909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.003532</td>\n",
       "      <td>2.135952</td>\n",
       "      <td>12.585558</td>\n",
       "      <td>2181.615252</td>\n",
       "      <td>419.391878</td>\n",
       "      <td>1132.462122</td>\n",
       "      <td>382.329753</td>\n",
       "      <td>1.899822</td>\n",
       "      <td>115395.615874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-124.350000</td>\n",
       "      <td>32.540000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.499900</td>\n",
       "      <td>14999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-121.800000</td>\n",
       "      <td>33.930000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1447.750000</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>787.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>2.563400</td>\n",
       "      <td>119600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-118.490000</td>\n",
       "      <td>34.260000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>2127.000000</td>\n",
       "      <td>435.000000</td>\n",
       "      <td>1166.000000</td>\n",
       "      <td>409.000000</td>\n",
       "      <td>3.534800</td>\n",
       "      <td>179700.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-118.010000</td>\n",
       "      <td>37.710000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>3148.000000</td>\n",
       "      <td>643.250000</td>\n",
       "      <td>1725.000000</td>\n",
       "      <td>605.000000</td>\n",
       "      <td>4.743250</td>\n",
       "      <td>264725.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>-114.310000</td>\n",
       "      <td>41.950000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>39320.000000</td>\n",
       "      <td>6445.000000</td>\n",
       "      <td>35682.000000</td>\n",
       "      <td>6082.000000</td>\n",
       "      <td>15.000100</td>\n",
       "      <td>500001.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          longitude      latitude  housing_median_age   total_rooms  \\\n",
       "count  20640.000000  20640.000000        20640.000000  20640.000000   \n",
       "mean    -119.569704     35.631861           28.639486   2635.763081   \n",
       "std        2.003532      2.135952           12.585558   2181.615252   \n",
       "min     -124.350000     32.540000            1.000000      2.000000   \n",
       "25%     -121.800000     33.930000           18.000000   1447.750000   \n",
       "50%     -118.490000     34.260000           29.000000   2127.000000   \n",
       "75%     -118.010000     37.710000           37.000000   3148.000000   \n",
       "max     -114.310000     41.950000           52.000000  39320.000000   \n",
       "\n",
       "       total_bedrooms    population    households  median_income  \\\n",
       "count    20640.000000  20640.000000  20640.000000   20640.000000   \n",
       "mean       536.838857   1425.476744    499.539680       3.870671   \n",
       "std        419.391878   1132.462122    382.329753       1.899822   \n",
       "min          1.000000      3.000000      1.000000       0.499900   \n",
       "25%        297.000000    787.000000    280.000000       2.563400   \n",
       "50%        435.000000   1166.000000    409.000000       3.534800   \n",
       "75%        643.250000   1725.000000    605.000000       4.743250   \n",
       "max       6445.000000  35682.000000   6082.000000      15.000100   \n",
       "\n",
       "       median_house_value  \n",
       "count        20640.000000  \n",
       "mean        206855.816909  \n",
       "std         115395.615874  \n",
       "min          14999.000000  \n",
       "25%         119600.000000  \n",
       "50%         179700.000000  \n",
       "75%         264725.000000  \n",
       "max         500001.000000  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1dae0983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51.0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing['housing_median_age'].max()-housing['housing_median_age'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9f09d432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35679.0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing['population'].max()-housing['population'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a448c17",
   "metadata": {},
   "source": [
    "How do we do this? We can do this by **standardizing** all of the features so they have an average of 0 and a standard deviation of 1. Standardization is the process of transforming all of the features so that they are all similar in some way. The act of transforming a variable's average to be 0 is also called **centering**, and transforming the standard deviation to 1 is also called **scaling.** In this case, we are standardizing all of the features' means and standard deviations. Let's say that x is the value of a feature that isn't standardized, then the standardized value z is calculated as follows:\n",
    "\n",
    "where μ represents the mean of that feature and σ is its standard deviation.\n",
    "\n",
    "Instead of doing this by hand, we can use a helpful class offered by the scikit-learn library: the StandardScaler class in the preprocessing library.\n",
    "\n",
    "In order to standardize the features, we just need to instantiate a StandardScaler() object and create a new standardized dataset using the fit_transform() method. We pass an unstandardized dataset X to the method and reassign the returned standardized dataset to a new variable, standardized_X.\n",
    "\n",
    "`from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "standardized_X = scaler.fit_transform(X)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7dcd23db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler= StandardScaler()\n",
    "\n",
    "x_train_scaled= scaler.fit_transform(x_train)\n",
    "\n",
    "rigmodel= RidgeCV(alphas= np.linspace(1,10, num=100))\n",
    "\n",
    "\n",
    "rigmodel.fit(x_train_scaled, y_train)\n",
    "\n",
    "rigcoef= rigmodel.coef_\n",
    "rigalpha= rigmodel.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0a45219d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.03822906  0.0027479  -0.27399059  1.91717504 -1.74238296  0.09599716\n",
      " -0.28805399  1.18999287]\n"
     ]
    }
   ],
   "source": [
    "print(rigcoef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7d343dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n"
     ]
    }
   ],
   "source": [
    "print(rigalpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728c5842",
   "metadata": {},
   "source": [
    "#### The LASSO Model\n",
    "Now that we understand how to implement ridge regression, we can look at a closely related model: the LASSO model. The acronym LASSO stands for \"Least Absolute Shrinkage and Selection Operator.\" The model's name states precisely what it does: shrinkage and selection. We've learned both terms in this course, so we'll see how LASSO accomplishes them here.\n",
    "\n",
    "Like ridge regression, LASSO is a regularized model. This regularization comes from an additional penalty term in the loss function. But unlike ridge regression, LASSO punishes high coefficient values with a different function, as shown below:\n",
    "\n",
    "<img src='lasso.png' width=600 height=600>\n",
    "\n",
    "LASSO penalizes the loss function using the absolute values of the coefficients instead of their squared values. While this change seems small, it gives LASSO a valuable characteristic that ridge regression lacks: feature selection. In ridge regression, if we feed p features into the model, then we'll still retain all of them after the regularization. In LASSO, some of the coefficients might be forced to zero, effectively removing the associated feature from the model.\n",
    "\n",
    "`from sklearn.linear_model import LassoCV\n",
    "lasso = LassoCV(alphas = linspace(0.1, 10, num=100))\n",
    "lasso.fit(X, y)`\n",
    "\n",
    "LASSO's feature selection makes it especially useful in high-dimensional situations. Feature selection becomes especially important as datasets get more complex, so we should be open to these regularized approaches.\n",
    "\n",
    "First, we'll check how ridge and LASSO compare in terms of their coefficients. Then, we'll examine how they each fare on predicting the test set. Both RidgeCV and LassoCV have a predict() method that we can use to predict the test data, so we can use this to calculate the test MSE. If you believe that some of the predictors truly don't contribute to explaining the outcome, then you should use LASSO.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a5c144b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "\n",
       "   population  households  median_income  median_house_value  \n",
       "0       322.0       126.0         8.3252            452600.0  \n",
       "1      2401.0      1138.0         8.3014            358500.0  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "743bcaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "x= housing.drop(columns=['median_income'],axis=1)\n",
    "y= housing['median_income']\n",
    "\n",
    "x= scaler.fit_transform(x)\n",
    "\n",
    "x_train,x_test,y_train,y_test= train_test_split(x,y, test_size=0.20, random_state= 762, shuffle=True)\n",
    "\n",
    "lassomodel= LassoCV(alphas= np.linspace(1,10, num=100))\n",
    "ridgemodel= RidgeCV(alphas= np.linspace(1,10, num=100))\n",
    "\n",
    "lassomodel.fit(x_train, y_train)\n",
    "ridgemodel.fit(x_train, y_train)\n",
    "\n",
    "lassocoef= lassomodel.coef_\n",
    "ridgecoef= ridgemodel.coef_\n",
    "\n",
    "lpredict= lassomodel.predict(x_test)\n",
    "rpredict= ridgemodel.predict(x_test)\n",
    "\n",
    "lasso_mses= mean_squared_error(lpredict, y_test)\n",
    "ridge_mses= mean_squared_error(rpredict, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "044f1e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.         -0.         -0.          0.         -0.          0.\n",
      "  0.          0.31026586]\n"
     ]
    }
   ],
   "source": [
    "print(lassocoef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "05ed11f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0384154   0.00279322 -0.27445801  1.93368078 -1.76144002  0.0969363\n",
      " -0.29220931  1.1889206 ]\n"
     ]
    }
   ],
   "source": [
    "print(ridgecoef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "03200ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.820070898359236\n"
     ]
    }
   ],
   "source": [
    "print(lasso_mses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "65d3054b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2690211718202609\n"
     ]
    }
   ],
   "source": [
    "print(ridge_mses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ac462865",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.array(x)[np.abs(lassocoef) !=0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d75c3b",
   "metadata": {},
   "source": [
    "## Going Beyond Linear Models\n",
    "Welcome to the fifth and final lesson in Optimizing Machine Learning Models in Python. In this lesson, we'll explore complex models like polynomials and splines. These models can accommodate more complicated feature-outcome relationships that linear models can't capture. These models can also be as complex as we want them to be, but this can come at the cost of high overfitting.\n",
    "\n",
    "Rather than learn the underlying relationship between the feature and the outcome, the model only learns how to predict the training outcomes and doesn't generalize well to unseen data. Regularization can also serve a purpose in this context, and we'll see how to implement it here.\n",
    "\n",
    "We'll use the California Housing Prices to practice. Let's get started!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "425b0959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "      <th>near_ocean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \\\n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY   \n",
       "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY   \n",
       "\n",
       "   near_ocean  \n",
       "0           0  \n",
       "1           0  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "12eb8f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "longitude               0\n",
       "latitude                0\n",
       "housing_median_age      0\n",
       "total_rooms             0\n",
       "total_bedrooms        207\n",
       "population              0\n",
       "households              0\n",
       "median_income           0\n",
       "median_house_value      0\n",
       "ocean_proximity         0\n",
       "near_ocean              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f4b0b374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "longitude             float64\n",
       "latitude              float64\n",
       "housing_median_age    float64\n",
       "total_rooms           float64\n",
       "total_bedrooms        float64\n",
       "population            float64\n",
       "households            float64\n",
       "median_income         float64\n",
       "median_house_value    float64\n",
       "ocean_proximity        object\n",
       "near_ocean              int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b731aa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#med_bed= house['total_bedrooms'].mean()\n",
    "#house['total_bedrooms']= house['total_bedrooms'].fillna(med_bed, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "131c9e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= house.drop(columns=['ocean_proximity','near_ocean'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "40c407af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
       "       'total_bedrooms', 'population', 'households', 'median_income',\n",
       "       'median_house_value'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "db7a7777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "longitude               0\n",
       "latitude                0\n",
       "housing_median_age      0\n",
       "total_rooms             0\n",
       "total_bedrooms        207\n",
       "population              0\n",
       "households              0\n",
       "median_income           0\n",
       "median_house_value      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e146159c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy= 'median' )\n",
    "\n",
    "data= imputer.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4ed138ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.DataFrame(data, columns= df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ca8ceb51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "longitude             0\n",
       "latitude              0\n",
       "housing_median_age    0\n",
       "total_rooms           0\n",
       "total_bedrooms        0\n",
       "population            0\n",
       "households            0\n",
       "median_income         0\n",
       "median_house_value    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f74ba513",
   "metadata": {},
   "source": [
    "To begin our discussion of non-linear models, let's look at polynomial models. A polynomial model of degree d has the following form:\n",
    "\n",
    "\n",
    "\n",
    "where X represents a feature, and its polynomials are also included as features in the model. For simplicity, we'll assume that we're only using a single feature in the model. The parameters, β^0,...,β^d also represent changes in the outcome due to the feature. In a polynomial model of degree d\n",
    ", we use increasing powers on the feature in the model, and each of these powers gets a coefficient. This model is still linear in the sense that it's a linear combination of the features, but it is genuinely a non-linear model because of the increasing powers on the feature.\n",
    "\n",
    "We can choose the degree (d) of the model to capture particular types of trends. For example, if we observed some type of exponential growth in the model, we might choose a quadratic model (degree 2). If we were to look at a scatter plot of the households to total_rooms columns in the housing dataset, we'll find a non-linear trend:\n",
    "<img src='pm.png' width=600 height=600>\n",
    "<img src='pm1.png' width=500 height=500>\n",
    "\n",
    "Based on the plot, if the number of houses in a block increases, then the number of rooms increases faster than a linear model would predict. This might be a possible use case for a polynomial model.\n",
    "\n",
    "In scikit-learn, there isn't an implementation for a polynomial model, but there is a helper class for generating polynomial features. This class is the PolynomialFeatures class in the preprocessing module. To introduce the class, let's look at a use case:\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatur\n",
    "\n",
    "`from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "poly_X = poly.fit_transform(X)`\n",
    "\n",
    "First, the PolynomialFeatures class needs to be imported and then instantiated. When we create the object, we need to define how many degrees we would like to generate via the degree parameter. Above, we're creating an object that will generate the linear, quadratic, and cubic transformations of the features in X. The Boolean include_bias parameter controls whether or not to include a column for the intercept. Since we don't need this when fitting a model, we've set include_bias to False here to exclude it. Finally, we can generate the features themselves by calling the fit_transform() method on the object and passing the dataset for which we want to create new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "032fc21e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
       "       'total_bedrooms', 'population', 'households', 'median_income',\n",
       "       'median_house_value'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "926bbc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "x1= df['households']\n",
    "\n",
    "x= df['households'].values.reshape(-1,1)\n",
    "y= df['total_rooms']\n",
    "\n",
    "\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y, test_size=0.20, random_state=763, shuffle=True)\n",
    "\n",
    "poly= PolynomialFeatures(degree=3, include_bias= False)\n",
    "\n",
    "quad_x_train= poly.fit_transform(x_train)\n",
    "quad_x_test= poly.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "873fd92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[811.]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f0b2f24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.11000000e+02 6.57721000e+05 5.33411731e+08]\n"
     ]
    }
   ],
   "source": [
    "print(quad_x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbd5e85",
   "metadata": {},
   "source": [
    "#### Fitting the Polynomial Model\n",
    "We can see that this array contains a single list, which corresponds to the first row of the transformed households column. The first element corresponds to the intercept column, and the second element corresponds to the original X_train data we called fit_transform() on. Therefore, the last column contains the quadratic transformation of the households column. We can use these features to fit our polynomial model using the LinearRegression class. Remember that a polynomial model still has a linear nature, but it's the features that have been converted into a non-linear form.\n",
    "\n",
    "Using the quad_X data, our model would look like the following:\n",
    "<img src='pmodel.png' width=600 height=600>\n",
    "\n",
    "What we'd like to highlight here is that the regression coefficients gain new interpretations in the context of a polynomial model. Each of the non-intercept coefficients captures the strength of a different trend. β1 corresponds to the strength of the linear trend, while β2 corresponds to the strength of the quadratic trends. The intercept β0 represents the average value of the outcome when the feature is 0. ϵ still captures the error of the model not captured by the features. If there's truly a quadratic trend in the data, then ideally the addition of the quadratic term will further reduce the MSE. If not, then we will see that this quadratic term won't contribute to or could even adversely affect predictive ability. To check this, it's best to compare polynomial models to their linear forms to check if these more complex predictors are justified.\n",
    "\n",
    "Let's compare a purely linear model with a quadratic model in terms of predictive performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a46b85ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model= LinearRegression()\n",
    "quadratic_model= LinearRegression()\n",
    "\n",
    "linear_model.fit(x_train,y_train)\n",
    "quadratic_model.fit(quad_x_train, y_train)\n",
    "\n",
    "lpredict= linear_model.predict(x_test)\n",
    "qpredict= quadratic_model.predict(quad_x_test)\n",
    "\n",
    "linear_mse= mean_squared_error(lpredict, y_test)\n",
    "quadratic_mse= mean_squared_error(qpredict, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "dd96f4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808610.2650644777\n",
      "794080.6717970215\n"
     ]
    }
   ],
   "source": [
    "print(linear_mse)\n",
    "print(quadratic_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bfe457fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "better_model= 'quadratic model'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4dd64f",
   "metadata": {},
   "source": [
    "#### Piecewise Functions\n",
    "Polynomials are the perfect launching point for discussing flexible, non-linear models. We can introduce non-linearity in the model just by including quadratic or cubic versions of the feature. However, the inclusion of these polynomial terms comes at the cost of terrible **extrapolations** — predictions made using feature values outside the range of the training data. As we venture farther from the range of the data, polynomial model predictions grow extremely fast, going outside of any realistic value we might observe. Let's take a look at our plot of households against total_rooms from earlier:\n",
    "<img src='pm1.png' width=500 height=500>\n",
    "\n",
    "If we were to fit a quadratic model on this data, the resulting model would predict higher values than expected of total_rooms for households values past 7000. One way around this is to use different models to describe different domains (input ranges) of the feature column. For example, instead of having a single quadratic model for the entire range of households values, we could use two linear models with a knot that separates the domains of the two models. We show this visually below:\n",
    "\n",
    "<img src='piecewise.png' width=600 height=600>\n",
    "There are two linear models above, one for households < 2000 and one for households > 2000. There is a single knot, denoted by the red dot, that describes the point where the model changes, going from one slope and intercept to another. This is an example of a **piecewise function.** In this case, one model is fit on all the data where households < 2000, and a second one is fit where households > 2000. We choose one of the models to cover the prediction at 2000 to prevent it from having two predictions at the same point. The full piecewise model is described by both of these models. The value 2000 was chosen because there's visual evidence that the slope changes around this point, but it can also be in other places where the trend seems to change significantly. We have just indicated it here for demonstration purposes. \n",
    "\n",
    "Piecewise functions allow us another degree of flexibility that single models don't. Our example here uses piecewise linear functions, but we could just as easily fit different polynomials for different regions of households. On the next screen, we'll learn how to develop these piecewise functions in scikit-learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0f5e9aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "housing = pd.read_csv(\"housing.csv\").dropna()\n",
    "X = housing[\"households\"].values.reshape(-1, 1)\n",
    "y = housing[\"total_rooms\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=763)\n",
    "\n",
    "# Subdividing the data into different domains\n",
    "train_lt_2000 = X_train < 2000\n",
    "train_gte_2000 = X_train >= 2000\n",
    "test_lt_2000 = X_test < 2000\n",
    "test_gte_2000 = X_test >= 2000\n",
    "\n",
    "X_train_lt_2000 = X_train[train_lt_2000].reshape(-1, 1) \n",
    "X_train_gte_2000 = X_train[train_gte_2000].reshape(-1, 1) \n",
    "X_test_lt_2000 = X_test[test_lt_2000].reshape(-1, 1) \n",
    "X_test_gte_2000 = X_test[test_gte_2000].reshape(-1, 1) \n",
    "\n",
    "y_train_lt_2000 = y_train.loc[train_lt_2000] \n",
    "y_train_gte_2000 = y_train.loc[train_gte_2000]\n",
    "y_test_lt_2000 = y_test.loc[test_lt_2000] \n",
    "y_test_gte_2000 = y_test.loc[test_gte_2000]\n",
    "\n",
    "# Training the models\n",
    "lm = LinearRegression().fit(X_train, y_train)\n",
    "pre2000 = LinearRegression().fit(X_train_lt_2000, y_train_lt_2000) \n",
    "post2000 = LinearRegression().fit(X_train_gte_2000, y_train_gte_2000)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "standard_lm_test_mse = mean_squared_error(y_test, lm.predict(X_test))\n",
    "\n",
    "test_predictions_lt_2000 = pre2000.predict(X_test_lt_2000)\n",
    "test_predictions_gte_2000 = post2000.predict(X_test_gte_2000)\n",
    "\n",
    "piecewise_test_predictions = np.append(test_predictions_lt_2000, test_predictions_gte_2000)\n",
    "y_test_recombined = np.append(y_test_lt_2000, y_test_gte_2000)\n",
    "piecewise_test_mse = mean_squared_error(y_test_recombined, piecewise_test_predictions)\n",
    "\n",
    "better_model = \"piecewise\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295582ab",
   "metadata": {},
   "source": [
    "####  Creating Splines\n",
    "One of the weaknesses of piecewise functions is that they produce discontinuities, places where the function will suddenly change in value. In our example, the discontinuity happens at the knot:\n",
    "Typically, we'd want our predictive model to be continuous instead. We won't explore the mathematical sense of continuity, but we can understand a continuous function as one that forms an uninterrupted line without any gaps or breaks. We can add additional constraints, or restrictions, on piecewise functions so that they will be continuous. Placing a continuity constraint on our example function would force it to look like this:\n",
    "<img src='continuity.png' width=600 height=600>\n",
    "We can see that the function is now continuous at the knot. However, you may also notice that there is now a sharp \"elbow\" in the function. We also might want our predictive model to be smooth, without this sharp point. We can do this by placing two more constraints on the function: that both the first and second derivative are continuous at the knot. Imposing these constrains results in a model that roughly looks like this:\n",
    "<img src='smooth.png' width=600 height=600>\n",
    "\n",
    "The final product above is still a piecewise polynomial, but it has three additional constraints to force it to be continuous and smooth. This model has a special name: a **spline**. Splines are extremely flexible models that we can use to model complex feature-outcome relationships. They require more mathematical detail, but thankfully, we can abstract this away using some functionality in scikit-learn. We'll introduce it here and explore it in more detail on the next screen.\n",
    "\n",
    "We can create splines using the SplineTransformer class. The SplineTransformer class has a similar interface as PolynomialFeatures: \n",
    "`from sklearn.preprocessing import SplineTransformer\n",
    "spline = SplineTransformer(degree=1, n_knots=2, knots=\"uniform\")\n",
    "spline.fit_transform(X)`\n",
    "\n",
    "Like PolynomialFeatures, we instantiate a SplineTransformer() object with some arguments and then call fit_transform() on some data. The three parameters used above are as follows:\n",
    "\n",
    "- degree: defines the degree of the polynomials that we would like to use in the spline. This value must be a non-negative integer.\n",
    "- n_knots: defines how many knots to place in the model. This value must be greater than or equal to 2.\n",
    "- knots: describes where the knots should be placed. Although we can specify the locations of the knots ourselves, it's often better to have them dispersed uniformly over the values of the feature. By default, SplineTransfomer uses knots=\"uniform\".\n",
    "Let's apply the SplineTransformer class to our ongoing example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "abb022ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import SplineTransformer\n",
    "\n",
    "spline= SplineTransformer(degree= 1, n_knots=2, knots=\"uniform\")\n",
    "\n",
    "spline_x_train= spline.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b6eb5417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.83957219 0.16042781]\n",
      " [0.96870667 0.03129333]\n",
      " [0.82927312 0.17072688]\n",
      " ...\n",
      " [0.9681125  0.0318875 ]\n",
      " [0.81837988 0.18162012]\n",
      " [0.94196871 0.05803129]]\n"
     ]
    }
   ],
   "source": [
    "print(spline_x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a4fe41",
   "metadata": {},
   "source": [
    "#### The Basis Expansion\n",
    "The SplineTransformer class helps us create features that enable us to construct spline models, but it doesn't actually create the model for us. On this screen, we'll learn how to convert these features into an actual spline model. The process is somewhat surprising, so it's worth taking some time to explain it carefully.\n",
    "\n",
    "If we were to examine the spline_X_train that we created on the previous screen, we would see the following output:\n",
    "\n",
    "`spline = SplineTransformer(degree=1, n_knots=2)\n",
    "spline_X_train = spline.fit_transform(X_train)\n",
    "print(spline_X_train[:5])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6e0fc03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.83957219 0.16042781]\n",
      " [0.96870667 0.03129333]\n",
      " [0.82927312 0.17072688]\n",
      " [0.7660923  0.2339077 ]\n",
      " [0.93681917 0.06318083]]\n"
     ]
    }
   ],
   "source": [
    "print(spline_x_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836c8192",
   "metadata": {},
   "source": [
    "The result is an array of lists, similar to how PolynomialFeatures transforms the data. We know that PolynomialFeatures produces features based off the different powers of the feature, but what transformation does SplineTransformer use? To understand this, we need to understand the spline basis representation. The spline basis representation represents the spline as a linear combination of functions of features. For example, if we were using two functions, b1(X) and b2(X), to construct the spline, the model would be written like this:\n",
    "<img src='spline.png' width=600 height=600>\n",
    "\n",
    "Let's say that we want to use polynomials of degree d in the model, and we want to use k\n",
    " knots. It turns out that just by specifying these two pieces of information, we can completely describe the resulting spline with d+k−1 basis functions. In our example above for spline, we specified d=1 and k=2\n",
    ", so we would need two basis functions to describe the spline. This is exactly what we see in the output above! The SplineTransformer class calculates the values of each of the basis functions for us.\n",
    "\n",
    "One weakness of this approach is that we won't know the explicit form of the basis functions. However, the only thing we need to create the spline is the output of these basis functions, and this is what SplineTransformer provides. If we want to use a spline as our predictive model, it's important we keep track of the polynomial degree and number of knots so that we can keep track of how the model was constructed. Knowing exactly how each of the features in a spline is calculated is less important to know.\n",
    "\n",
    "In order to estimate the coefficients, we just need to pass these transformed features into a LinearRegression() object. This highlights the power of the basis representation: we can make incredibly complex models while still using familiar model classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "03fe7f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import SplineTransformer\n",
    "\n",
    "\n",
    "x1= df['households']\n",
    "\n",
    "x= df['households'].values.reshape(-1,1)\n",
    "y= df['total_rooms']\n",
    "\n",
    "\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y, test_size=0.20, random_state=763, shuffle=True)\n",
    "\n",
    "\n",
    "spline= SplineTransformer(degree=1, n_knots=2)\n",
    "\n",
    "spline_x_train= spline.fit_transform(x_train)\n",
    "spline_x_test= spline.fit_transform(x_test)\n",
    "\n",
    "lm= LinearRegression()\n",
    "sm= LinearRegression()\n",
    "\n",
    "lmodel=lm.fit(x_train, y_train)\n",
    "smodel=sm.fit(spline_x_train, y_train)\n",
    "\n",
    "lpredict= lm.predict(x_test)\n",
    "spredict= sm.predict(spline_x_test)\n",
    "\n",
    "l_mse= mean_squared_error(lpredict, y_test)\n",
    "s_mse= mean_squared_error(spredict, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7900290e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808610.2650644777\n",
      "1210582.3566850913\n"
     ]
    }
   ],
   "source": [
    "print(l_mse)\n",
    "print(s_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a23030",
   "metadata": {},
   "source": [
    "#### Cubic Splines\n",
    "To conclude our discussion of splines, we'll turn to a general rule. Splines give us two parameters to construct a model: the degree of the polynomial and the number of knots. Practically speaking, higher-degree polynomials are too flexible and are prone to overfitting. Instead of increasing the degree of the spline, it would be good to stick to a simpler polynomial and then experiment with the number of knots via cross-validation.\n",
    "\n",
    "Specifically, cubic splines (d=3) are popular because they hide the knots in the function well. We recommend sticking to cubic splines unless some expert knowledge or data exploration suggests strongly otherwise. In fact, the default argument for the SplineTransformer class is to use a cubic spline (degree=3).\n",
    "\n",
    "With this in mind, let's practice trying to optimize for the best number of knots to use for our spline mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3c80760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mses= []\n",
    "\n",
    "x1= df['households']\n",
    "\n",
    "x= df['households'].values.reshape(-1,1)\n",
    "y= df['total_rooms']\n",
    "\n",
    "\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y, test_size=0.20, random_state=763, shuffle=True)\n",
    "\n",
    "\n",
    "for i in range(2,11):\n",
    "    spline= SplineTransformer(degree=3, n_knots=i )\n",
    "    spline_x_train= spline.fit_transform(x_train)\n",
    "    spline_x_test=  spline.fit_transform(x_test)\n",
    "    \n",
    "    model=LinearRegression()\n",
    "    model.fit(spline_x_train, y_train)\n",
    "    pred= model.predict(spline_x_test)\n",
    "    \n",
    "    mses= round(mean_squared_error(pred,y_test),0)\n",
    "    test_mses.append(mses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "163ccf21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1185299.0, 1189984.0, 1192893.0, 1201203.0, 1210479.0, 1203316.0, 1196386.0, 1199196.0, 1192297.0]\n"
     ]
    }
   ],
   "source": [
    "print(test_mses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a555b915",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_knot= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5287df5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
