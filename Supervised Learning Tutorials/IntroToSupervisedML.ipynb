{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1e2c022",
   "metadata": {},
   "source": [
    "## Introduction to the Machine Learning Workflow\n",
    "Dataquest wants to figure out whether or not a new learner will complete a specific course. We are confident that the course has a good balance that allows a learner to understand any concept by applying it. Unfortunately, that doesn't always imply that everyone will complete a course.\n",
    "\n",
    "So, how could we predict that a new learner will complete the course?\n",
    "\n",
    "Let's say we've collected data for every current learner on the website. We know:\n",
    "\n",
    "- How many lessons they've completed previously on the website.\n",
    "- How many of the other courses they've completed.\n",
    "- How many exercises they've successfully passed.\n",
    "- How many hours they've spent on each individual lesson.\n",
    "- Whether they've already completed the course\n",
    "\n",
    "Based on the data, we already know whether an existing learner has completed the course. Are there relevant details in the rest of the data that could help us find a pattern that relates a learner's actions on the website to the likelihood of their completing the course?\n",
    "\n",
    "Maybe, upon exploring the data, we find that 90% of learners who completed the course:\n",
    "\n",
    "- Completed more than L lessons previously.\n",
    "- Spent more than H hours, on average, per lesson.\n",
    "- Passed more than E exercises.\n",
    "- Given these patterns, can we say that a new learner who satisfies the above criteria will complete that course also?\n",
    "\n",
    "Well, we can't guarantee it. But, the likelihood of that happening might be high if the new learner satisfies the above rules. What if we learn something new from the data that changes that likelihood? It could get lower or higher.\n",
    "\n",
    "We can't always identify these patterns one at a time, especially when we have lots of data. This is exactly where machine learning can help.\n",
    "\n",
    "Instead of making decisions based on patterns identified from information we already have, we can use machine learning to do it for us.\n",
    "\n",
    "The small set of rules or criteria we identified above is essentially what a machine learning model does without us explicitly programming those rules. We train the model so it learns to identify those patterns from the data on its own. We then use the model to predict something given a new, unseen,\n",
    "\n",
    "input. That \"something\" might seem a bit ambiguous, but what we want to predict depends on the intended outcome or task. In this and the next few courses, we will cover such tasks and learn machine learning models that can help us with those tasks.\n",
    "\n",
    "What we learned above gives us a quick introduction to the machine learning workflow:\n",
    "\n",
    "- Data Collection.\n",
    "- Data Exploration and Wrangling.\n",
    "- Data Preparation (Feature Engineering).\n",
    "- Building and training a model.\n",
    "- Evaluating the model performance\n",
    "- Fine-tuning the model.\n",
    "- Evaluating the model performance.\n",
    "\n",
    "<img src='ML_workflow.svg' width=500 height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06af424e",
   "metadata": {},
   "source": [
    "That might seem like a lot of steps and terminology! Some things might not be explicitly clear right away, but don't worry. Throughout this course, we will go through each step of the above workflow in more detail.\n",
    "For the rest of this lesson, we will use a popular, python-based machine learning library called scikit-learn and learn more about this workflow. We'll also train a simple machine learning model to predict if a patient has breast cancer.\n",
    "\n",
    "Please note that this lesson will focus on helping us familiarize with the workflow. Any concepts or terminology that might come across as confusing or challenging will be covered in varying depths across the remaining lessons and courses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9ac16d",
   "metadata": {},
   "source": [
    "## Data Collection and Exploration\n",
    "We'll build a model that predicts whether a patient has breast cancer. We'll start with the data.\n",
    "While it's possible, it's not always feasible to collect our own data. Fortunately, there are a lot of publicly available datasets that we can freely access and work with, depending on what kind of problem we want to solve.\n",
    "\n",
    "We'll work with the Breast Cancer Wisconsin (Diagnostic) Dataset https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic). The scikit-learn library stores several datasets, including this breast cancer dataset, and makes it convenient for us to load and work with them.\n",
    "\n",
    "On the first screen, we looked at what kind of data could be collected by a company like Dataquest:\n",
    "\n",
    "On the first screen, we looked at what kind of data could be collected by a company like Dataquest:\n",
    "\n",
    "- How many lessons students have completed on the website.\n",
    "- How many courses they've completed so far.\n",
    "- How many exercises they've successfully passed.\n",
    "- How many hours they've spent on each lesson.\n",
    "Each of the above is called a **feature**. They all describe or are a property of our data. When we work with tabular data, each column name corresponds to a feature -- except one:of our data. When we work with tabular data, each column name corresponds to a feature -- except one:\n",
    "\n",
    "Whether or not a learner has already completed the course.\n",
    "The column corresponding to the above is called the **target variable** because that's what we want our model to predict. That's our target.\n",
    "\n",
    "Every row for the above would contain information related to an individual learner. Each of these rows is called an **observation or a feature vector**. It's an n-dimensional vector of features\n",
    "\n",
    "Let's look at the dataset we'll be working with. The link to our dataset provides us with the following information:\n",
    "\n",
    "- There are a total of 30 attributes, or 30 features.\n",
    "- There are a total of 569 instances or observations.\n",
    "- There are two classes in the target variable:\n",
    "- WDBC-Malignant\n",
    "- WDBC-Benign\n",
    "The above two classes, or labels, tell us whether a patient has a benign or a malignant tumor. For every observation, there's a corresponding class or label in our data.\n",
    "\n",
    "Let's load in our dataset using scikit-learn's load_breast_cancer function https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer. The function will return an object with several attributes, a couple of which we will utilize:\n",
    "\n",
    "- data stores the data points.\n",
    "- target stores the value 0 if the tumor is benign or 1 if the tumor is malignant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f1f8624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n",
       "         1.189e-01],\n",
       "        [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n",
       "         8.902e-02],\n",
       "        [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n",
       "         8.758e-02],\n",
       "        ...,\n",
       "        [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n",
       "         7.820e-02],\n",
       "        [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n",
       "         1.240e-01],\n",
       "        [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n",
       "         7.039e-02]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['malignant', 'benign'], dtype='<U9'),\n",
       " 'DESCR': '.. _breast_cancer_dataset:\\n\\nBreast cancer wisconsin (diagnostic) dataset\\n--------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 569\\n\\n    :Number of Attributes: 30 numeric, predictive attributes and the class\\n\\n    :Attribute Information:\\n        - radius (mean of distances from center to points on the perimeter)\\n        - texture (standard deviation of gray-scale values)\\n        - perimeter\\n        - area\\n        - smoothness (local variation in radius lengths)\\n        - compactness (perimeter^2 / area - 1.0)\\n        - concavity (severity of concave portions of the contour)\\n        - concave points (number of concave portions of the contour)\\n        - symmetry\\n        - fractal dimension (\"coastline approximation\" - 1)\\n\\n        The mean, standard error, and \"worst\" or largest (mean of the three\\n        worst/largest values) of these features were computed for each image,\\n        resulting in 30 features.  For instance, field 0 is Mean Radius, field\\n        10 is Radius SE, field 20 is Worst Radius.\\n\\n        - class:\\n                - WDBC-Malignant\\n                - WDBC-Benign\\n\\n    :Summary Statistics:\\n\\n    ===================================== ====== ======\\n                                           Min    Max\\n    ===================================== ====== ======\\n    radius (mean):                        6.981  28.11\\n    texture (mean):                       9.71   39.28\\n    perimeter (mean):                     43.79  188.5\\n    area (mean):                          143.5  2501.0\\n    smoothness (mean):                    0.053  0.163\\n    compactness (mean):                   0.019  0.345\\n    concavity (mean):                     0.0    0.427\\n    concave points (mean):                0.0    0.201\\n    symmetry (mean):                      0.106  0.304\\n    fractal dimension (mean):             0.05   0.097\\n    radius (standard error):              0.112  2.873\\n    texture (standard error):             0.36   4.885\\n    perimeter (standard error):           0.757  21.98\\n    area (standard error):                6.802  542.2\\n    smoothness (standard error):          0.002  0.031\\n    compactness (standard error):         0.002  0.135\\n    concavity (standard error):           0.0    0.396\\n    concave points (standard error):      0.0    0.053\\n    symmetry (standard error):            0.008  0.079\\n    fractal dimension (standard error):   0.001  0.03\\n    radius (worst):                       7.93   36.04\\n    texture (worst):                      12.02  49.54\\n    perimeter (worst):                    50.41  251.2\\n    area (worst):                         185.2  4254.0\\n    smoothness (worst):                   0.071  0.223\\n    compactness (worst):                  0.027  1.058\\n    concavity (worst):                    0.0    1.252\\n    concave points (worst):               0.0    0.291\\n    symmetry (worst):                     0.156  0.664\\n    fractal dimension (worst):            0.055  0.208\\n    ===================================== ====== ======\\n\\n    :Missing Attribute Values: None\\n\\n    :Class Distribution: 212 - Malignant, 357 - Benign\\n\\n    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\\n\\n    :Donor: Nick Street\\n\\n    :Date: November, 1995\\n\\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\\nhttps://goo.gl/U2Uwz2\\n\\nFeatures are computed from a digitized image of a fine needle\\naspirate (FNA) of a breast mass.  They describe\\ncharacteristics of the cell nuclei present in the image.\\n\\nSeparating plane described above was obtained using\\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\\nConstruction Via Linear Programming.\" Proceedings of the 4th\\nMidwest Artificial Intelligence and Cognitive Science Society,\\npp. 97-101, 1992], a classification method which uses linear\\nprogramming to construct a decision tree.  Relevant features\\nwere selected using an exhaustive search in the space of 1-4\\nfeatures and 1-3 separating planes.\\n\\nThe actual linear program used to obtain the separating plane\\nin the 3-dimensional space is that described in:\\n[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\\nProgramming Discrimination of Two Linearly Inseparable Sets\",\\nOptimization Methods and Software 1, 1992, 23-34].\\n\\nThis database is also available through the UW CS ftp server:\\n\\nftp ftp.cs.wisc.edu\\ncd math-prog/cpo-dataset/machine-learn/WDBC/\\n\\n.. topic:: References\\n\\n   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \\n     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \\n     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\\n     San Jose, CA, 1993.\\n   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \\n     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \\n     July-August 1995.\\n   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\\n     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \\n     163-171.',\n",
       " 'feature_names': array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "        'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "        'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "        'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "        'smoothness error', 'compactness error', 'concavity error',\n",
       "        'concave points error', 'symmetry error',\n",
       "        'fractal dimension error', 'worst radius', 'worst texture',\n",
       "        'worst perimeter', 'worst area', 'worst smoothness',\n",
       "        'worst compactness', 'worst concavity', 'worst concave points',\n",
       "        'worst symmetry', 'worst fractal dimension'], dtype='<U23'),\n",
       " 'filename': 'breast_cancer.csv',\n",
       " 'data_module': 'sklearn.datasets.data'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import necessary libraries\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "#call the imported function\n",
    "load_breast_cancer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3afe837",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pass the as_frame=True parameter to the function\n",
    "cancer_data=load_breast_cancer(as_frame=True)\n",
    "\n",
    "#assign the data attribute to cancer_df\n",
    "cancer_df=cancer_data.data\n",
    "cancer_df['target']=cancer_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07c61744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  target  \n",
       "0          0.4601                  0.11890       0  \n",
       "1          0.2750                  0.08902       0  \n",
       "2          0.3613                  0.08758       0  \n",
       "3          0.6638                  0.17300       0  \n",
       "4          0.2364                  0.07678       0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the first five rows of the dataset\n",
    "cancer_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c45ca12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer_df['target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1a9069f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 31)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the shape of the data\n",
    "cancer_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ea7e8c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mean radius                0\n",
       "mean texture               0\n",
       "mean perimeter             0\n",
       "mean area                  0\n",
       "mean smoothness            0\n",
       "mean compactness           0\n",
       "mean concavity             0\n",
       "mean concave points        0\n",
       "mean symmetry              0\n",
       "mean fractal dimension     0\n",
       "radius error               0\n",
       "texture error              0\n",
       "perimeter error            0\n",
       "area error                 0\n",
       "smoothness error           0\n",
       "compactness error          0\n",
       "concavity error            0\n",
       "concave points error       0\n",
       "symmetry error             0\n",
       "fractal dimension error    0\n",
       "worst radius               0\n",
       "worst texture              0\n",
       "worst perimeter            0\n",
       "worst area                 0\n",
       "worst smoothness           0\n",
       "worst compactness          0\n",
       "worst concavity            0\n",
       "worst concave points       0\n",
       "worst symmetry             0\n",
       "worst fractal dimension    0\n",
       "target                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mssing values in each column\n",
    "cancer_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90b30b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    357\n",
       "0    212\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0af31715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14.127292</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>0.062798</td>\n",
       "      <td>...</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.083946</td>\n",
       "      <td>0.627417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.524049</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>...</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.018061</td>\n",
       "      <td>0.483918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.981000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.049960</td>\n",
       "      <td>...</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.055040</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.700000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>...</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.071460</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.370000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>...</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.080040</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.780000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.066120</td>\n",
       "      <td>...</td>\n",
       "      <td>29.720000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.092080</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>28.110000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.097440</td>\n",
       "      <td>...</td>\n",
       "      <td>49.540000</td>\n",
       "      <td>251.200000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>1.058000</td>\n",
       "      <td>1.252000</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.207500</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean radius  mean texture  mean perimeter    mean area  \\\n",
       "count   569.000000    569.000000      569.000000   569.000000   \n",
       "mean     14.127292     19.289649       91.969033   654.889104   \n",
       "std       3.524049      4.301036       24.298981   351.914129   \n",
       "min       6.981000      9.710000       43.790000   143.500000   \n",
       "25%      11.700000     16.170000       75.170000   420.300000   \n",
       "50%      13.370000     18.840000       86.240000   551.100000   \n",
       "75%      15.780000     21.800000      104.100000   782.700000   \n",
       "max      28.110000     39.280000      188.500000  2501.000000   \n",
       "\n",
       "       mean smoothness  mean compactness  mean concavity  mean concave points  \\\n",
       "count       569.000000        569.000000      569.000000           569.000000   \n",
       "mean          0.096360          0.104341        0.088799             0.048919   \n",
       "std           0.014064          0.052813        0.079720             0.038803   \n",
       "min           0.052630          0.019380        0.000000             0.000000   \n",
       "25%           0.086370          0.064920        0.029560             0.020310   \n",
       "50%           0.095870          0.092630        0.061540             0.033500   \n",
       "75%           0.105300          0.130400        0.130700             0.074000   \n",
       "max           0.163400          0.345400        0.426800             0.201200   \n",
       "\n",
       "       mean symmetry  mean fractal dimension  ...  worst texture  \\\n",
       "count     569.000000              569.000000  ...     569.000000   \n",
       "mean        0.181162                0.062798  ...      25.677223   \n",
       "std         0.027414                0.007060  ...       6.146258   \n",
       "min         0.106000                0.049960  ...      12.020000   \n",
       "25%         0.161900                0.057700  ...      21.080000   \n",
       "50%         0.179200                0.061540  ...      25.410000   \n",
       "75%         0.195700                0.066120  ...      29.720000   \n",
       "max         0.304000                0.097440  ...      49.540000   \n",
       "\n",
       "       worst perimeter   worst area  worst smoothness  worst compactness  \\\n",
       "count       569.000000   569.000000        569.000000         569.000000   \n",
       "mean        107.261213   880.583128          0.132369           0.254265   \n",
       "std          33.602542   569.356993          0.022832           0.157336   \n",
       "min          50.410000   185.200000          0.071170           0.027290   \n",
       "25%          84.110000   515.300000          0.116600           0.147200   \n",
       "50%          97.660000   686.500000          0.131300           0.211900   \n",
       "75%         125.400000  1084.000000          0.146000           0.339100   \n",
       "max         251.200000  4254.000000          0.222600           1.058000   \n",
       "\n",
       "       worst concavity  worst concave points  worst symmetry  \\\n",
       "count       569.000000            569.000000      569.000000   \n",
       "mean          0.272188              0.114606        0.290076   \n",
       "std           0.208624              0.065732        0.061867   \n",
       "min           0.000000              0.000000        0.156500   \n",
       "25%           0.114500              0.064930        0.250400   \n",
       "50%           0.226700              0.099930        0.282200   \n",
       "75%           0.382900              0.161400        0.317900   \n",
       "max           1.252000              0.291000        0.663800   \n",
       "\n",
       "       worst fractal dimension      target  \n",
       "count               569.000000  569.000000  \n",
       "mean                  0.083946    0.627417  \n",
       "std                   0.018061    0.483918  \n",
       "min                   0.055040    0.000000  \n",
       "25%                   0.071460    0.000000  \n",
       "50%                   0.080040    1.000000  \n",
       "75%                   0.092080    1.000000  \n",
       "max                   0.207500    1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display the summary statistics of the data\n",
    "cancer_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d0c706",
   "metadata": {},
   "source": [
    "## Supervised Machine Learning: Classification\n",
    "On the previous screen, we identified that our data has no missing values. We can, for now, work with the assumption that our data is clean and doesn't require further wrangling. That's always a dangerous assumption to make, but, for this lesson, our focus is on exploring the machine learning workflow from a broader perspective.\n",
    "\n",
    "Our next step is to prepare our data before we input it to our model. However, we haven't really discussed what this model is.\n",
    "\n",
    "Let's say we don't know what a giraffe looks like. Someone shows us a photo of a giraffe and we are able to identify some features that we think are unique to a giraffe. The next time we see a photo of a giraffe, even if it's a completely different one than before, we are highly likely to know that it's a giraffe. Our innate pattern-matching abilities could fail us, and we might confuse it for an ostrich just because of the long neck. But that's unlikely.\n",
    "\n",
    "<img src='Giraffe.png' width=600 height=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92ffe6d",
   "metadata": {},
   "source": [
    "We were supplied with a label, along with a set of features, and our brains learned to associate that label with those features. The next time we saw a similar set of features, we were able to predict the correct label. That's the intuition behind the subset of machine learning we'll learn about in this course--**supervised machine learning**\n",
    "\n",
    "It's supervised because our model learns from existing data and the corresponding labels. There are usually two types of labels that we encounter:\n",
    "\n",
    "- Continuous labels.\n",
    "For example, we could be working with a dataset that contains features that describe different types of cars, and the labels could be the price of those cars. Our model would then learn to predict the price of a car, given those features as input.\n",
    "- Categorical labels.\n",
    "Our breast cancer dataset has only two labels--benign and malignant, or 0 and 1. Each observation is categorized by its own label or class.\n",
    "\n",
    "If the label or target we want to predict is a categorical value, we call it a **classification task**. The model, a classifier, will try to classify a given set of inputs into a category. There are other types of machine learning models that can be used for different kinds of tasks. We'll learn about those later.\n",
    "\n",
    "-So, what exactly does the model learn from the data?\n",
    "\n",
    "This is dependent on the machine learning algorithm we end up using for our model. Throughout this course, we will cover several algorithms that will answer that question for us. But, intuitively, think of our dataset as a bunch of points on a chart:\n",
    "\n",
    "<img src='chart.svg' width=500 height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeefc97",
   "metadata": {},
   "source": [
    "The points on the charts above are our features in a two-dimensional space. This space is called a **feature space** and the colors represent the labels. The line that we see is a **decision boundary** and it divides the feature space into two. Points on one side of the boundary belong to one class, and points on the other side of the boundary belong to the other class.\n",
    "\n",
    "That decision boundary is our **classifier**. Since we only have two labels, it's called a **binary classifier**. If we had more labels, it would be a multi-class classifier.\n",
    "\n",
    "<img src='multi_classifier.svg' width=500 height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3025cf",
   "metadata": {},
   "source": [
    "If we had more features, we would have an n-dimensional space and the decision boundary would be a hyperplane instead of a line.\n",
    "\n",
    "But where does that line come from?\n",
    "\n",
    "We know that a line or a curve can be defined using a parametric equation. That's what the model tries to learn - the parameters of that equation. The model estimates those parameters from the data, and those estimated parameters define the decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4461da1b",
   "metadata": {},
   "source": [
    "If we had more features, we would have an n-dimensional space and the decision boundary would be a hyperplane instead of a line.\n",
    "\n",
    "- But where does that line come from?\n",
    "\n",
    "We know that a line or a curve can be defined using a parametric equation. That's what the model tries to learn - **the parameters** of that equation. The model estimates those parameters from the data, and those estimated parameters define the decision boundary.\n",
    "\n",
    "Now that we have a better idea of what kind of machine learning task we're carrying out, let's prepare our data so that we can then train a model on it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afea81ae",
   "metadata": {},
   "source": [
    "## · Data Preparation\n",
    "In the previous screens, we came across a couple of concepts:\n",
    "\n",
    "- Training a model on our data.\n",
    "- Using the model to make a prediction on unseen data.\n",
    "The data that we use to train our model is called **training data**, a **training set** or a **training dataset**\n",
    "The unseen data, as the name suggests, is data that our model hasn't seen before. The model has not trained on that data and will only use that data and predict a label for it.\n",
    "\n",
    "This unseen data is called test data, a test set or a test dataset. It's used to evaluate the model's performance. Since we don't have any additional data to use as a test set, we can randomly select a small set of observations from our original dataset and set it aside. That way, we can train our model on the rest of the dataset and test it later using the test set.\n",
    "Next we'll prepare our training and test datasets using scikit-learn's train_test_split function https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html. There will be two stages to this process:\n",
    "\n",
    "- 1. The function expects the feature columns and the target column as inputs. If we look at the examples in the documentation, those two are passed to the function as X and y. As we work with the library more, we'll notice this convention often--the input data is usually stored in X and the target variable is stored in y. We'll follow the same convention here.\n",
    "\n",
    "- 2. The function will split the dataset into a training set and a test set based on a proportion that we decide. Usually, the test set's size is about 15 to 20 percent of the dataset's. Different factors, such as the original dataset's size, can also play a part in deciding that percentage\n",
    "\n",
    "The function splits the data randomly into each set. We'll learn why that's relevant in a future lesson.\n",
    "The output of train_test_split() is a list containing 4 elements:\n",
    "- The training set features.\n",
    "- The test set features.\n",
    "- The training set labels.\n",
    "- The test set labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6ec4423",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x=cancer_df.drop(columns=['target'], axis=1)\n",
    "y=cancer_df['target']\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y, test_size=0.15, random_state=417)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4addf8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((483, 30), (483,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the dimension of the train data sets\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2281f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>11.360</td>\n",
       "      <td>17.57</td>\n",
       "      <td>72.49</td>\n",
       "      <td>399.8</td>\n",
       "      <td>0.08858</td>\n",
       "      <td>0.05313</td>\n",
       "      <td>0.02783</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.05913</td>\n",
       "      <td>...</td>\n",
       "      <td>13.05</td>\n",
       "      <td>36.32</td>\n",
       "      <td>85.07</td>\n",
       "      <td>521.3</td>\n",
       "      <td>0.1453</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.18110</td>\n",
       "      <td>0.08698</td>\n",
       "      <td>0.2973</td>\n",
       "      <td>0.07745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>14.420</td>\n",
       "      <td>19.77</td>\n",
       "      <td>94.48</td>\n",
       "      <td>642.5</td>\n",
       "      <td>0.09752</td>\n",
       "      <td>0.11410</td>\n",
       "      <td>0.09388</td>\n",
       "      <td>0.058390</td>\n",
       "      <td>0.1879</td>\n",
       "      <td>0.06390</td>\n",
       "      <td>...</td>\n",
       "      <td>16.33</td>\n",
       "      <td>30.86</td>\n",
       "      <td>109.50</td>\n",
       "      <td>826.4</td>\n",
       "      <td>0.1431</td>\n",
       "      <td>0.3026</td>\n",
       "      <td>0.31940</td>\n",
       "      <td>0.15650</td>\n",
       "      <td>0.2718</td>\n",
       "      <td>0.09353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>9.606</td>\n",
       "      <td>16.84</td>\n",
       "      <td>61.64</td>\n",
       "      <td>280.5</td>\n",
       "      <td>0.08481</td>\n",
       "      <td>0.09228</td>\n",
       "      <td>0.08422</td>\n",
       "      <td>0.022920</td>\n",
       "      <td>0.2036</td>\n",
       "      <td>0.07125</td>\n",
       "      <td>...</td>\n",
       "      <td>10.75</td>\n",
       "      <td>23.07</td>\n",
       "      <td>71.25</td>\n",
       "      <td>353.6</td>\n",
       "      <td>0.1233</td>\n",
       "      <td>0.3416</td>\n",
       "      <td>0.43410</td>\n",
       "      <td>0.08120</td>\n",
       "      <td>0.2982</td>\n",
       "      <td>0.09825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>14.970</td>\n",
       "      <td>16.95</td>\n",
       "      <td>96.22</td>\n",
       "      <td>685.9</td>\n",
       "      <td>0.09855</td>\n",
       "      <td>0.07885</td>\n",
       "      <td>0.02602</td>\n",
       "      <td>0.037810</td>\n",
       "      <td>0.1780</td>\n",
       "      <td>0.05650</td>\n",
       "      <td>...</td>\n",
       "      <td>16.11</td>\n",
       "      <td>23.00</td>\n",
       "      <td>104.60</td>\n",
       "      <td>793.7</td>\n",
       "      <td>0.1216</td>\n",
       "      <td>0.1637</td>\n",
       "      <td>0.06648</td>\n",
       "      <td>0.08485</td>\n",
       "      <td>0.2404</td>\n",
       "      <td>0.06428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>13.870</td>\n",
       "      <td>20.70</td>\n",
       "      <td>89.77</td>\n",
       "      <td>584.8</td>\n",
       "      <td>0.09578</td>\n",
       "      <td>0.10180</td>\n",
       "      <td>0.03688</td>\n",
       "      <td>0.023690</td>\n",
       "      <td>0.1620</td>\n",
       "      <td>0.06688</td>\n",
       "      <td>...</td>\n",
       "      <td>15.05</td>\n",
       "      <td>24.75</td>\n",
       "      <td>99.17</td>\n",
       "      <td>688.6</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>0.2037</td>\n",
       "      <td>0.13770</td>\n",
       "      <td>0.06845</td>\n",
       "      <td>0.2249</td>\n",
       "      <td>0.08492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>20.180</td>\n",
       "      <td>23.97</td>\n",
       "      <td>143.70</td>\n",
       "      <td>1245.0</td>\n",
       "      <td>0.12860</td>\n",
       "      <td>0.34540</td>\n",
       "      <td>0.37540</td>\n",
       "      <td>0.160400</td>\n",
       "      <td>0.2906</td>\n",
       "      <td>0.08142</td>\n",
       "      <td>...</td>\n",
       "      <td>23.37</td>\n",
       "      <td>31.72</td>\n",
       "      <td>170.30</td>\n",
       "      <td>1623.0</td>\n",
       "      <td>0.1639</td>\n",
       "      <td>0.6164</td>\n",
       "      <td>0.76810</td>\n",
       "      <td>0.25080</td>\n",
       "      <td>0.5440</td>\n",
       "      <td>0.09964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>16.780</td>\n",
       "      <td>18.80</td>\n",
       "      <td>109.30</td>\n",
       "      <td>886.3</td>\n",
       "      <td>0.08865</td>\n",
       "      <td>0.09182</td>\n",
       "      <td>0.08422</td>\n",
       "      <td>0.065760</td>\n",
       "      <td>0.1893</td>\n",
       "      <td>0.05534</td>\n",
       "      <td>...</td>\n",
       "      <td>20.05</td>\n",
       "      <td>26.30</td>\n",
       "      <td>130.70</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>0.1168</td>\n",
       "      <td>0.2119</td>\n",
       "      <td>0.23180</td>\n",
       "      <td>0.14740</td>\n",
       "      <td>0.2810</td>\n",
       "      <td>0.07228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>11.300</td>\n",
       "      <td>18.19</td>\n",
       "      <td>73.93</td>\n",
       "      <td>389.4</td>\n",
       "      <td>0.09592</td>\n",
       "      <td>0.13250</td>\n",
       "      <td>0.15480</td>\n",
       "      <td>0.028540</td>\n",
       "      <td>0.2054</td>\n",
       "      <td>0.07669</td>\n",
       "      <td>...</td>\n",
       "      <td>12.58</td>\n",
       "      <td>27.96</td>\n",
       "      <td>87.16</td>\n",
       "      <td>472.9</td>\n",
       "      <td>0.1347</td>\n",
       "      <td>0.4848</td>\n",
       "      <td>0.74360</td>\n",
       "      <td>0.12180</td>\n",
       "      <td>0.3308</td>\n",
       "      <td>0.12970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>9.668</td>\n",
       "      <td>18.10</td>\n",
       "      <td>61.06</td>\n",
       "      <td>286.3</td>\n",
       "      <td>0.08311</td>\n",
       "      <td>0.05428</td>\n",
       "      <td>0.01479</td>\n",
       "      <td>0.005769</td>\n",
       "      <td>0.1680</td>\n",
       "      <td>0.06412</td>\n",
       "      <td>...</td>\n",
       "      <td>11.15</td>\n",
       "      <td>24.62</td>\n",
       "      <td>71.11</td>\n",
       "      <td>380.2</td>\n",
       "      <td>0.1388</td>\n",
       "      <td>0.1255</td>\n",
       "      <td>0.06409</td>\n",
       "      <td>0.02500</td>\n",
       "      <td>0.3057</td>\n",
       "      <td>0.07875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>12.980</td>\n",
       "      <td>19.35</td>\n",
       "      <td>84.52</td>\n",
       "      <td>514.0</td>\n",
       "      <td>0.09579</td>\n",
       "      <td>0.11250</td>\n",
       "      <td>0.07107</td>\n",
       "      <td>0.029500</td>\n",
       "      <td>0.1761</td>\n",
       "      <td>0.06540</td>\n",
       "      <td>...</td>\n",
       "      <td>14.42</td>\n",
       "      <td>21.95</td>\n",
       "      <td>99.21</td>\n",
       "      <td>634.3</td>\n",
       "      <td>0.1288</td>\n",
       "      <td>0.3253</td>\n",
       "      <td>0.34390</td>\n",
       "      <td>0.09858</td>\n",
       "      <td>0.3596</td>\n",
       "      <td>0.09166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>483 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "410       11.360         17.57           72.49      399.8          0.08858   \n",
       "99        14.420         19.77           94.48      642.5          0.09752   \n",
       "341        9.606         16.84           61.64      280.5          0.08481   \n",
       "169       14.970         16.95           96.22      685.9          0.09855   \n",
       "544       13.870         20.70           89.77      584.8          0.09578   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "78        20.180         23.97          143.70     1245.0          0.12860   \n",
       "167       16.780         18.80          109.30      886.3          0.08865   \n",
       "242       11.300         18.19           73.93      389.4          0.09592   \n",
       "467        9.668         18.10           61.06      286.3          0.08311   \n",
       "331       12.980         19.35           84.52      514.0          0.09579   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "410           0.05313         0.02783             0.021000         0.1601   \n",
       "99            0.11410         0.09388             0.058390         0.1879   \n",
       "341           0.09228         0.08422             0.022920         0.2036   \n",
       "169           0.07885         0.02602             0.037810         0.1780   \n",
       "544           0.10180         0.03688             0.023690         0.1620   \n",
       "..                ...             ...                  ...            ...   \n",
       "78            0.34540         0.37540             0.160400         0.2906   \n",
       "167           0.09182         0.08422             0.065760         0.1893   \n",
       "242           0.13250         0.15480             0.028540         0.2054   \n",
       "467           0.05428         0.01479             0.005769         0.1680   \n",
       "331           0.11250         0.07107             0.029500         0.1761   \n",
       "\n",
       "     mean fractal dimension  ...  worst radius  worst texture  \\\n",
       "410                 0.05913  ...         13.05          36.32   \n",
       "99                  0.06390  ...         16.33          30.86   \n",
       "341                 0.07125  ...         10.75          23.07   \n",
       "169                 0.05650  ...         16.11          23.00   \n",
       "544                 0.06688  ...         15.05          24.75   \n",
       "..                      ...  ...           ...            ...   \n",
       "78                  0.08142  ...         23.37          31.72   \n",
       "167                 0.05534  ...         20.05          26.30   \n",
       "242                 0.07669  ...         12.58          27.96   \n",
       "467                 0.06412  ...         11.15          24.62   \n",
       "331                 0.06540  ...         14.42          21.95   \n",
       "\n",
       "     worst perimeter  worst area  worst smoothness  worst compactness  \\\n",
       "410            85.07       521.3            0.1453             0.1622   \n",
       "99            109.50       826.4            0.1431             0.3026   \n",
       "341            71.25       353.6            0.1233             0.3416   \n",
       "169           104.60       793.7            0.1216             0.1637   \n",
       "544            99.17       688.6            0.1264             0.2037   \n",
       "..               ...         ...               ...                ...   \n",
       "78            170.30      1623.0            0.1639             0.6164   \n",
       "167           130.70      1260.0            0.1168             0.2119   \n",
       "242            87.16       472.9            0.1347             0.4848   \n",
       "467            71.11       380.2            0.1388             0.1255   \n",
       "331            99.21       634.3            0.1288             0.3253   \n",
       "\n",
       "     worst concavity  worst concave points  worst symmetry  \\\n",
       "410          0.18110               0.08698          0.2973   \n",
       "99           0.31940               0.15650          0.2718   \n",
       "341          0.43410               0.08120          0.2982   \n",
       "169          0.06648               0.08485          0.2404   \n",
       "544          0.13770               0.06845          0.2249   \n",
       "..               ...                   ...             ...   \n",
       "78           0.76810               0.25080          0.5440   \n",
       "167          0.23180               0.14740          0.2810   \n",
       "242          0.74360               0.12180          0.3308   \n",
       "467          0.06409               0.02500          0.3057   \n",
       "331          0.34390               0.09858          0.3596   \n",
       "\n",
       "     worst fractal dimension  \n",
       "410                  0.07745  \n",
       "99                   0.09353  \n",
       "341                  0.09825  \n",
       "169                  0.06428  \n",
       "544                  0.08492  \n",
       "..                       ...  \n",
       "78                   0.09964  \n",
       "167                  0.07228  \n",
       "242                  0.12970  \n",
       "467                  0.07875  \n",
       "331                  0.09166  \n",
       "\n",
       "[483 rows x 30 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c195a868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>18.630</td>\n",
       "      <td>25.11</td>\n",
       "      <td>124.80</td>\n",
       "      <td>1088.0</td>\n",
       "      <td>0.10640</td>\n",
       "      <td>0.18870</td>\n",
       "      <td>0.23190</td>\n",
       "      <td>0.124400</td>\n",
       "      <td>0.2183</td>\n",
       "      <td>0.06197</td>\n",
       "      <td>...</td>\n",
       "      <td>23.15</td>\n",
       "      <td>34.01</td>\n",
       "      <td>160.50</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>0.14910</td>\n",
       "      <td>0.42570</td>\n",
       "      <td>0.6133</td>\n",
       "      <td>0.18480</td>\n",
       "      <td>0.3444</td>\n",
       "      <td>0.09782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>12.850</td>\n",
       "      <td>21.37</td>\n",
       "      <td>82.63</td>\n",
       "      <td>514.5</td>\n",
       "      <td>0.07551</td>\n",
       "      <td>0.08316</td>\n",
       "      <td>0.06126</td>\n",
       "      <td>0.018670</td>\n",
       "      <td>0.1580</td>\n",
       "      <td>0.06114</td>\n",
       "      <td>...</td>\n",
       "      <td>14.40</td>\n",
       "      <td>27.01</td>\n",
       "      <td>91.63</td>\n",
       "      <td>645.8</td>\n",
       "      <td>0.09402</td>\n",
       "      <td>0.19360</td>\n",
       "      <td>0.1838</td>\n",
       "      <td>0.05601</td>\n",
       "      <td>0.2488</td>\n",
       "      <td>0.08151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>12.760</td>\n",
       "      <td>18.84</td>\n",
       "      <td>81.87</td>\n",
       "      <td>496.6</td>\n",
       "      <td>0.09676</td>\n",
       "      <td>0.07952</td>\n",
       "      <td>0.02688</td>\n",
       "      <td>0.017810</td>\n",
       "      <td>0.1759</td>\n",
       "      <td>0.06183</td>\n",
       "      <td>...</td>\n",
       "      <td>13.75</td>\n",
       "      <td>25.99</td>\n",
       "      <td>87.82</td>\n",
       "      <td>579.7</td>\n",
       "      <td>0.12980</td>\n",
       "      <td>0.18390</td>\n",
       "      <td>0.1255</td>\n",
       "      <td>0.08312</td>\n",
       "      <td>0.2744</td>\n",
       "      <td>0.07238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>9.683</td>\n",
       "      <td>19.34</td>\n",
       "      <td>61.05</td>\n",
       "      <td>285.7</td>\n",
       "      <td>0.08491</td>\n",
       "      <td>0.05030</td>\n",
       "      <td>0.02337</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.1580</td>\n",
       "      <td>0.06235</td>\n",
       "      <td>...</td>\n",
       "      <td>10.93</td>\n",
       "      <td>25.59</td>\n",
       "      <td>69.10</td>\n",
       "      <td>364.2</td>\n",
       "      <td>0.11990</td>\n",
       "      <td>0.09546</td>\n",
       "      <td>0.0935</td>\n",
       "      <td>0.03846</td>\n",
       "      <td>0.2552</td>\n",
       "      <td>0.07920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>14.260</td>\n",
       "      <td>19.65</td>\n",
       "      <td>97.83</td>\n",
       "      <td>629.9</td>\n",
       "      <td>0.07837</td>\n",
       "      <td>0.22330</td>\n",
       "      <td>0.30030</td>\n",
       "      <td>0.077980</td>\n",
       "      <td>0.1704</td>\n",
       "      <td>0.07769</td>\n",
       "      <td>...</td>\n",
       "      <td>15.30</td>\n",
       "      <td>23.73</td>\n",
       "      <td>107.00</td>\n",
       "      <td>709.0</td>\n",
       "      <td>0.08949</td>\n",
       "      <td>0.41930</td>\n",
       "      <td>0.6783</td>\n",
       "      <td>0.15050</td>\n",
       "      <td>0.2398</td>\n",
       "      <td>0.10820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>19.270</td>\n",
       "      <td>26.47</td>\n",
       "      <td>127.90</td>\n",
       "      <td>1162.0</td>\n",
       "      <td>0.09401</td>\n",
       "      <td>0.17190</td>\n",
       "      <td>0.16570</td>\n",
       "      <td>0.075930</td>\n",
       "      <td>0.1853</td>\n",
       "      <td>0.06261</td>\n",
       "      <td>...</td>\n",
       "      <td>24.15</td>\n",
       "      <td>30.90</td>\n",
       "      <td>161.40</td>\n",
       "      <td>1813.0</td>\n",
       "      <td>0.15090</td>\n",
       "      <td>0.65900</td>\n",
       "      <td>0.6091</td>\n",
       "      <td>0.17850</td>\n",
       "      <td>0.3672</td>\n",
       "      <td>0.11230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>23.210</td>\n",
       "      <td>26.97</td>\n",
       "      <td>153.50</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>0.09509</td>\n",
       "      <td>0.16820</td>\n",
       "      <td>0.19500</td>\n",
       "      <td>0.123700</td>\n",
       "      <td>0.1909</td>\n",
       "      <td>0.06309</td>\n",
       "      <td>...</td>\n",
       "      <td>31.01</td>\n",
       "      <td>34.51</td>\n",
       "      <td>206.00</td>\n",
       "      <td>2944.0</td>\n",
       "      <td>0.14810</td>\n",
       "      <td>0.41260</td>\n",
       "      <td>0.5820</td>\n",
       "      <td>0.25930</td>\n",
       "      <td>0.3103</td>\n",
       "      <td>0.08677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>15.220</td>\n",
       "      <td>30.62</td>\n",
       "      <td>103.40</td>\n",
       "      <td>716.9</td>\n",
       "      <td>0.10480</td>\n",
       "      <td>0.20870</td>\n",
       "      <td>0.25500</td>\n",
       "      <td>0.094290</td>\n",
       "      <td>0.2128</td>\n",
       "      <td>0.07152</td>\n",
       "      <td>...</td>\n",
       "      <td>17.52</td>\n",
       "      <td>42.79</td>\n",
       "      <td>128.70</td>\n",
       "      <td>915.0</td>\n",
       "      <td>0.14170</td>\n",
       "      <td>0.79170</td>\n",
       "      <td>1.1700</td>\n",
       "      <td>0.23560</td>\n",
       "      <td>0.4089</td>\n",
       "      <td>0.14090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>14.580</td>\n",
       "      <td>13.66</td>\n",
       "      <td>94.29</td>\n",
       "      <td>658.8</td>\n",
       "      <td>0.09832</td>\n",
       "      <td>0.08918</td>\n",
       "      <td>0.08222</td>\n",
       "      <td>0.043490</td>\n",
       "      <td>0.1739</td>\n",
       "      <td>0.05640</td>\n",
       "      <td>...</td>\n",
       "      <td>16.76</td>\n",
       "      <td>17.24</td>\n",
       "      <td>108.50</td>\n",
       "      <td>862.0</td>\n",
       "      <td>0.12230</td>\n",
       "      <td>0.19280</td>\n",
       "      <td>0.2492</td>\n",
       "      <td>0.09186</td>\n",
       "      <td>0.2626</td>\n",
       "      <td>0.07048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>20.310</td>\n",
       "      <td>27.06</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1288.0</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.10880</td>\n",
       "      <td>0.15190</td>\n",
       "      <td>0.093330</td>\n",
       "      <td>0.1814</td>\n",
       "      <td>0.05572</td>\n",
       "      <td>...</td>\n",
       "      <td>24.33</td>\n",
       "      <td>39.16</td>\n",
       "      <td>162.30</td>\n",
       "      <td>1844.0</td>\n",
       "      <td>0.15220</td>\n",
       "      <td>0.29450</td>\n",
       "      <td>0.3788</td>\n",
       "      <td>0.16970</td>\n",
       "      <td>0.3151</td>\n",
       "      <td>0.07999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "30        18.630         25.11          124.80     1088.0          0.10640   \n",
       "407       12.850         21.37           82.63      514.5          0.07551   \n",
       "362       12.760         18.84           81.87      496.6          0.09676   \n",
       "548        9.683         19.34           61.05      285.7          0.08491   \n",
       "112       14.260         19.65           97.83      629.9          0.07837   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "33        19.270         26.47          127.90     1162.0          0.09401   \n",
       "236       23.210         26.97          153.50     1670.0          0.09509   \n",
       "562       15.220         30.62          103.40      716.9          0.10480   \n",
       "513       14.580         13.66           94.29      658.8          0.09832   \n",
       "260       20.310         27.06          132.90     1288.0          0.10000   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "30            0.18870         0.23190             0.124400         0.2183   \n",
       "407           0.08316         0.06126             0.018670         0.1580   \n",
       "362           0.07952         0.02688             0.017810         0.1759   \n",
       "548           0.05030         0.02337             0.009615         0.1580   \n",
       "112           0.22330         0.30030             0.077980         0.1704   \n",
       "..                ...             ...                  ...            ...   \n",
       "33            0.17190         0.16570             0.075930         0.1853   \n",
       "236           0.16820         0.19500             0.123700         0.1909   \n",
       "562           0.20870         0.25500             0.094290         0.2128   \n",
       "513           0.08918         0.08222             0.043490         0.1739   \n",
       "260           0.10880         0.15190             0.093330         0.1814   \n",
       "\n",
       "     mean fractal dimension  ...  worst radius  worst texture  \\\n",
       "30                  0.06197  ...         23.15          34.01   \n",
       "407                 0.06114  ...         14.40          27.01   \n",
       "362                 0.06183  ...         13.75          25.99   \n",
       "548                 0.06235  ...         10.93          25.59   \n",
       "112                 0.07769  ...         15.30          23.73   \n",
       "..                      ...  ...           ...            ...   \n",
       "33                  0.06261  ...         24.15          30.90   \n",
       "236                 0.06309  ...         31.01          34.51   \n",
       "562                 0.07152  ...         17.52          42.79   \n",
       "513                 0.05640  ...         16.76          17.24   \n",
       "260                 0.05572  ...         24.33          39.16   \n",
       "\n",
       "     worst perimeter  worst area  worst smoothness  worst compactness  \\\n",
       "30            160.50      1670.0           0.14910            0.42570   \n",
       "407            91.63       645.8           0.09402            0.19360   \n",
       "362            87.82       579.7           0.12980            0.18390   \n",
       "548            69.10       364.2           0.11990            0.09546   \n",
       "112           107.00       709.0           0.08949            0.41930   \n",
       "..               ...         ...               ...                ...   \n",
       "33            161.40      1813.0           0.15090            0.65900   \n",
       "236           206.00      2944.0           0.14810            0.41260   \n",
       "562           128.70       915.0           0.14170            0.79170   \n",
       "513           108.50       862.0           0.12230            0.19280   \n",
       "260           162.30      1844.0           0.15220            0.29450   \n",
       "\n",
       "     worst concavity  worst concave points  worst symmetry  \\\n",
       "30            0.6133               0.18480          0.3444   \n",
       "407           0.1838               0.05601          0.2488   \n",
       "362           0.1255               0.08312          0.2744   \n",
       "548           0.0935               0.03846          0.2552   \n",
       "112           0.6783               0.15050          0.2398   \n",
       "..               ...                   ...             ...   \n",
       "33            0.6091               0.17850          0.3672   \n",
       "236           0.5820               0.25930          0.3103   \n",
       "562           1.1700               0.23560          0.4089   \n",
       "513           0.2492               0.09186          0.2626   \n",
       "260           0.3788               0.16970          0.3151   \n",
       "\n",
       "     worst fractal dimension  \n",
       "30                   0.09782  \n",
       "407                  0.08151  \n",
       "362                  0.07238  \n",
       "548                  0.07920  \n",
       "112                  0.10820  \n",
       "..                       ...  \n",
       "33                   0.11230  \n",
       "236                  0.08677  \n",
       "562                  0.14090  \n",
       "513                  0.07048  \n",
       "260                  0.07999  \n",
       "\n",
       "[86 rows x 30 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9d00412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "410    1\n",
       "99     0\n",
       "341    1\n",
       "169    1\n",
       "544    1\n",
       "      ..\n",
       "78     0\n",
       "167    0\n",
       "242    1\n",
       "467    1\n",
       "331    1\n",
       "Name: target, Length: 483, dtype: int32"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15f7035f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30     0\n",
       "407    1\n",
       "362    1\n",
       "548    1\n",
       "112    1\n",
       "      ..\n",
       "33     0\n",
       "236    0\n",
       "562    0\n",
       "513    1\n",
       "260    0\n",
       "Name: target, Length: 86, dtype: int32"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61293e8",
   "metadata": {},
   "source": [
    "##  Building and Training a Classifier\n",
    "We have our training data ready! We can now train a model:\n",
    "\n",
    "<img src='train.svg' width=400 height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef62f2a8",
   "metadata": {},
   "source": [
    "scikit-learn offers many machine learning models https://scikit-learn.org/stable/supervised_learning.html. We can pick any one of the classification models and use it out of the box.\n",
    "That's exactly what we'll do. We'll use the Linear Support Vector Classification model https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html. We can check the documentation to see what kind of input parameters it can take. scikit-learn makes it easy for us to use the model without needing to understand the minute details of the algorithm. We'll observe this soon.\n",
    "\n",
    "- There are only two steps for building and training the model:\n",
    "\n",
    "- 1 We first instantiate the model. This is similar to how we would instantiate a Python class.\n",
    "\n",
    "- 2 This step doesn't take any training data as input. We can, however, define and set the values for the parameters we saw in the documentation linked above.\n",
    "\n",
    "- We fit the model onto the training data features (X) and labels (y). Fitting the model is the same as training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f07955b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\512GB\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 1, 0, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "#instantiate the model\n",
    "model=LinearSVC(penalty='l2', loss='squared_hinge', C=10, random_state=417)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "y_pred=model.predict(x_test)\n",
    "y_pred[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a89c7be",
   "metadata": {},
   "source": [
    "## Evaluating the Model on Test Set I\n",
    "We have successfully trained our model! But we don't know how well it performs.\n",
    "\n",
    "How can we measure the performance of a model?\n",
    "\n",
    "<img src='evaluate.svg' width=400 height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6e1e76",
   "metadata": {},
   "source": [
    "One of the most common ways to evaluate a classifier is to look at how accurate its predictions are. For a supervised learning task, we already know which class a particular observation belongs to.\n",
    "\n",
    "We can use our model to predict the labels of our test data. We can then calculate the accuracy of our model by comparing those predictions to the actual labels. But we don't need to do that all by ourselves.\n",
    "scikit-learn provides a method score()https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC.score that can calculate that accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f48639b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy is : 0.8509316770186336\n",
      "Test Accuracy is: 0.8255813953488372\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "train_accuracy=model.score(x_train, y_train)\n",
    "test_accuracy=model.score(x_test, y_test)\n",
    "\n",
    "print('Train Accuracy is :', train_accuracy)\n",
    "print('Test Accuracy is:',  test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bec79b6",
   "metadata": {},
   "source": [
    "##  Fine-tuning the Model\n",
    "Our model got an accuracy score of approximately 82.56. That means roughly 82.56% of our predictions were correct.\n",
    "\n",
    "That's a pretty good result, but building machine learning models is an iterative process. We can try to improve upon our result.\n",
    "<img src='finetune.svg' width=400 height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22e21cb",
   "metadata": {},
   "source": [
    "When we instantiated the LinearSVC model, we defined the following parameters:\n",
    "\n",
    "- penalty=\"l2\"\n",
    "- loss=\"squared_hinge\"\n",
    "- C=10\n",
    "While we don't know what these values imply for our model, we can experiment with them.\n",
    "- In an attempt to improve upon our result, we will:\n",
    "\n",
    "Change the values to one of the above parameters.\n",
    "Add and set another parameter from the documentation https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn-svm-linearsvc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cb47c8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\512GB\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#fine tune the model\n",
    "model=LinearSVC(penalty='l2' ,loss='squared_hinge' ,C=20, max_iter=3500, random_state=417)\n",
    "\n",
    "#instantiate the model\n",
    "model.fit(x_train, y_train)\n",
    "pred=model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655758fa",
   "metadata": {},
   "source": [
    "## Evaluating the Model on Test Set II\n",
    "In the last screen, we fine-tuned our model. We don't know yet if that improved our model's performance.\n",
    "\n",
    "Before we evaluate our model on the test set, let's look at the following plot:\n",
    "\n",
    "<img src='plot.svg' width=400 height=400>\n",
    "\n",
    "We can notice that there are yellow dots on the left side and there are purple on the right side of the decision boundary. The above is meant to be a simplified representation of how the decision boundary can't always capture the complexity of the data.\n",
    "\n",
    "What happens when we add a point to the above plot? Will it be correctly classified?\n",
    "\n",
    "We can't be sure. Our model is not going to be perfect. It's not going to give us a 100% accuracy, even if we keep experimenting with different parameter values.\n",
    "\n",
    "Let's see how our updated model performs on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d95c6e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy is : 0.9006211180124224\n",
      "Test Accuracy is: 0.872093023255814\n"
     ]
    }
   ],
   "source": [
    "train_accuracy=model.score(x_train, y_train)\n",
    "test_accuracy=model.score(x_test, y_test)\n",
    "\n",
    "print('Train Accuracy is :', train_accuracy)\n",
    "print('Test Accuracy is:',  test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ce17a6",
   "metadata": {},
   "source": [
    "## Experimentation vs Fundamentals\n",
    "Our accuracy increased!\n",
    "\n",
    "By just playing around with a couple of parameters, we were able to improve how our model performed. Our model is now better at predicting whether a patient has breast cancer.\n",
    "\n",
    "It's really amazing how we managed to:\n",
    "\n",
    "- Train a randomly selected model from scikit-learn.\n",
    "- Experiment with a few of its parameters, and\n",
    "- Get it to predict, with reasonably high accuracy, whether a patient has breast cancer.\n",
    "We managed to do that without understanding:\n",
    "\n",
    "- What the model actually does.\n",
    "- What the different parameters of that model are for.\n",
    "- What most of the dataset's features are.\n",
    "While there is clearly a case to be made for the value of being able to quickly iterate and experiment with different models, we can't lose sight of the bigger picture either.\n",
    "\n",
    "Remember we set max_iter to 3500? If we'd set it to 3000 instead, our accuracy would have dropped precipitously!\n",
    "\n",
    "What would we have done in that situation? There are 12 parameters in scikit-learn's LinearSVC.\n",
    "\n",
    "- How many of those parameters could we experiment with?\n",
    "- What possible permutations and combinations of values could we have tried?\n",
    "- What happens if we lower C instead of increasing it? \n",
    "- What does C actually do in the model? Would lowering it make our model perform better on the test set?\n",
    "- If we were to get more data with some new features, would our selected values yield similar results or would we have to go through the process all over again?\n",
    "- What if we had millions of observations instead? Could we afford to run any model any number of times on such a large dataset? Which model would even be suitable for such a large dataset?\n",
    "\n",
    "Experimentation and iteration through direct application have their place in the field of machine learning. However, trying to achieve good-to-great results given certain constraints is a challenging task. Blind experimentation won't get us far.\n",
    "\n",
    "Understanding the fundamentals or understanding how a machine learning algorithm works \"under the hood\" opens up additional insights to rely on when trying to answer some of the questions above. It can allow us to experiment and iterate from an informed perspective.\n",
    "\n",
    "In the next few lessons, we'll learn from that other perspective. We'll learn about a different machine learning algorithm, implement it from scratch, and then use scikit-learn again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a4bfcd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
