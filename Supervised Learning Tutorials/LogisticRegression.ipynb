{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aacbf99a",
   "metadata": {},
   "source": [
    "## Introduction to Logistic Regression\n",
    "Welcome to the first lesson in the **Logistic Regression Modeling in Python course!** Logistic regression is similar to linear regression, but the two are for similar-but-slightly-different tasks.\n",
    "In a somewhat confusing crossing of terms, linear regressions are used for regression tasks, referring to continuous outcomes. In linear regression, we try to predict losses in insurance claims. In logistic regression, we're trying to predict categorical outcomes, otherwise known as classification. Even though the word \"regression\" is also used in \"logistic regression,\" it refers to the fact that we also use a linear model.\n",
    "<img src='lilo.svg' width=500 height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686dcc22",
   "metadata": {},
   "source": [
    "The important thing to know is that logistic regression is the classification-based equivalent of linear regression. Classification problems are widespread in industry and academia. For example, we can ask questions like \"Does an MRI image contain an image of a tumor?\" or \"What kind of customer is this new shopper?\" These are examples of classification tasks.Logistic regression is our first foray into classification, so this lesson is dedicated to understanding how these problems differ from regression tasks. We'll learn how going from a continuous outcome to a categorical outcome changes the different aspects of the machine learning workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9204ebd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "#from sklearn.model_selection import Confusion_Matrix,classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "608295c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symboling</th>\n",
       "      <th>normalized_losses</th>\n",
       "      <th>make</th>\n",
       "      <th>fuel_type</th>\n",
       "      <th>aspiration</th>\n",
       "      <th>num_of_doors</th>\n",
       "      <th>body_style</th>\n",
       "      <th>drive_wheels</th>\n",
       "      <th>engine_location</th>\n",
       "      <th>wheel_base</th>\n",
       "      <th>...</th>\n",
       "      <th>engine_size</th>\n",
       "      <th>fuel_system</th>\n",
       "      <th>bore</th>\n",
       "      <th>stroke</th>\n",
       "      <th>compression_ratio</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>peak_rpm</th>\n",
       "      <th>city_mpg</th>\n",
       "      <th>highway_mpg</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>164</td>\n",
       "      <td>audi</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>fwd</td>\n",
       "      <td>front</td>\n",
       "      <td>99.8</td>\n",
       "      <td>...</td>\n",
       "      <td>109</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>102</td>\n",
       "      <td>5500</td>\n",
       "      <td>24</td>\n",
       "      <td>30</td>\n",
       "      <td>13950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>164</td>\n",
       "      <td>audi</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>4wd</td>\n",
       "      <td>front</td>\n",
       "      <td>99.4</td>\n",
       "      <td>...</td>\n",
       "      <td>136</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>115</td>\n",
       "      <td>5500</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>17450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>audi</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>fwd</td>\n",
       "      <td>front</td>\n",
       "      <td>105.8</td>\n",
       "      <td>...</td>\n",
       "      <td>136</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.4</td>\n",
       "      <td>8.5</td>\n",
       "      <td>110</td>\n",
       "      <td>5500</td>\n",
       "      <td>19</td>\n",
       "      <td>25</td>\n",
       "      <td>17710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>audi</td>\n",
       "      <td>gas</td>\n",
       "      <td>turbo</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>fwd</td>\n",
       "      <td>front</td>\n",
       "      <td>105.8</td>\n",
       "      <td>...</td>\n",
       "      <td>131</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.13</td>\n",
       "      <td>3.4</td>\n",
       "      <td>8.3</td>\n",
       "      <td>140</td>\n",
       "      <td>5500</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>23875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>192</td>\n",
       "      <td>bmw</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>two</td>\n",
       "      <td>sedan</td>\n",
       "      <td>rwd</td>\n",
       "      <td>front</td>\n",
       "      <td>101.2</td>\n",
       "      <td>...</td>\n",
       "      <td>108</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.8</td>\n",
       "      <td>8.8</td>\n",
       "      <td>101</td>\n",
       "      <td>5800</td>\n",
       "      <td>23</td>\n",
       "      <td>29</td>\n",
       "      <td>16430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   symboling  normalized_losses  make fuel_type aspiration num_of_doors  \\\n",
       "0          2                164  audi       gas        std         four   \n",
       "1          2                164  audi       gas        std         four   \n",
       "2          1                158  audi       gas        std         four   \n",
       "3          1                158  audi       gas      turbo         four   \n",
       "4          2                192   bmw       gas        std          two   \n",
       "\n",
       "  body_style drive_wheels engine_location  wheel_base  ...  engine_size  \\\n",
       "0      sedan          fwd           front        99.8  ...          109   \n",
       "1      sedan          4wd           front        99.4  ...          136   \n",
       "2      sedan          fwd           front       105.8  ...          136   \n",
       "3      sedan          fwd           front       105.8  ...          131   \n",
       "4      sedan          rwd           front       101.2  ...          108   \n",
       "\n",
       "   fuel_system  bore  stroke compression_ratio horsepower  peak_rpm city_mpg  \\\n",
       "0         mpfi  3.19     3.4              10.0        102      5500       24   \n",
       "1         mpfi  3.19     3.4               8.0        115      5500       18   \n",
       "2         mpfi  3.19     3.4               8.5        110      5500       19   \n",
       "3         mpfi  3.13     3.4               8.3        140      5500       17   \n",
       "4         mpfi  3.50     2.8               8.8        101      5800       23   \n",
       "\n",
       "   highway_mpg  price  \n",
       "0           30  13950  \n",
       "1           22  17450  \n",
       "2           25  17710  \n",
       "3           20  23875  \n",
       "4           29  16430  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in the dataset\n",
    "auto=pd.read_csv('automobiles.csv')\n",
    "auto.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8689d1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto['high_price']=auto['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81d17a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symboling</th>\n",
       "      <th>normalized_losses</th>\n",
       "      <th>make</th>\n",
       "      <th>fuel_type</th>\n",
       "      <th>aspiration</th>\n",
       "      <th>num_of_doors</th>\n",
       "      <th>body_style</th>\n",
       "      <th>drive_wheels</th>\n",
       "      <th>engine_location</th>\n",
       "      <th>wheel_base</th>\n",
       "      <th>...</th>\n",
       "      <th>fuel_system</th>\n",
       "      <th>bore</th>\n",
       "      <th>stroke</th>\n",
       "      <th>compression_ratio</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>peak_rpm</th>\n",
       "      <th>city_mpg</th>\n",
       "      <th>highway_mpg</th>\n",
       "      <th>price</th>\n",
       "      <th>high_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>164</td>\n",
       "      <td>audi</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>fwd</td>\n",
       "      <td>front</td>\n",
       "      <td>99.8</td>\n",
       "      <td>...</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>102</td>\n",
       "      <td>5500</td>\n",
       "      <td>24</td>\n",
       "      <td>30</td>\n",
       "      <td>13950</td>\n",
       "      <td>13950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>164</td>\n",
       "      <td>audi</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>4wd</td>\n",
       "      <td>front</td>\n",
       "      <td>99.4</td>\n",
       "      <td>...</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>115</td>\n",
       "      <td>5500</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>17450</td>\n",
       "      <td>17450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>audi</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>fwd</td>\n",
       "      <td>front</td>\n",
       "      <td>105.8</td>\n",
       "      <td>...</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.4</td>\n",
       "      <td>8.5</td>\n",
       "      <td>110</td>\n",
       "      <td>5500</td>\n",
       "      <td>19</td>\n",
       "      <td>25</td>\n",
       "      <td>17710</td>\n",
       "      <td>17710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>audi</td>\n",
       "      <td>gas</td>\n",
       "      <td>turbo</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>fwd</td>\n",
       "      <td>front</td>\n",
       "      <td>105.8</td>\n",
       "      <td>...</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.13</td>\n",
       "      <td>3.4</td>\n",
       "      <td>8.3</td>\n",
       "      <td>140</td>\n",
       "      <td>5500</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>23875</td>\n",
       "      <td>23875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>192</td>\n",
       "      <td>bmw</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>two</td>\n",
       "      <td>sedan</td>\n",
       "      <td>rwd</td>\n",
       "      <td>front</td>\n",
       "      <td>101.2</td>\n",
       "      <td>...</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.8</td>\n",
       "      <td>8.8</td>\n",
       "      <td>101</td>\n",
       "      <td>5800</td>\n",
       "      <td>23</td>\n",
       "      <td>29</td>\n",
       "      <td>16430</td>\n",
       "      <td>16430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   symboling  normalized_losses  make fuel_type aspiration num_of_doors  \\\n",
       "0          2                164  audi       gas        std         four   \n",
       "1          2                164  audi       gas        std         four   \n",
       "2          1                158  audi       gas        std         four   \n",
       "3          1                158  audi       gas      turbo         four   \n",
       "4          2                192   bmw       gas        std          two   \n",
       "\n",
       "  body_style drive_wheels engine_location  wheel_base  ...  fuel_system  bore  \\\n",
       "0      sedan          fwd           front        99.8  ...         mpfi  3.19   \n",
       "1      sedan          4wd           front        99.4  ...         mpfi  3.19   \n",
       "2      sedan          fwd           front       105.8  ...         mpfi  3.19   \n",
       "3      sedan          fwd           front       105.8  ...         mpfi  3.13   \n",
       "4      sedan          rwd           front       101.2  ...         mpfi  3.50   \n",
       "\n",
       "   stroke  compression_ratio horsepower peak_rpm  city_mpg highway_mpg  price  \\\n",
       "0     3.4               10.0        102     5500        24          30  13950   \n",
       "1     3.4                8.0        115     5500        18          22  17450   \n",
       "2     3.4                8.5        110     5500        19          25  17710   \n",
       "3     3.4                8.3        140     5500        17          20  23875   \n",
       "4     2.8                8.8        101     5800        23          29  16430   \n",
       "\n",
       "   high_price  \n",
       "0       13950  \n",
       "1       17450  \n",
       "2       17710  \n",
       "3       23875  \n",
       "4       16430  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56632d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\512GB\\AppData\\Local\\Temp\\ipykernel_5584\\3645285313.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  auto['high_price'][cols]=price\n"
     ]
    }
   ],
   "source": [
    "for cols in  auto.index:\n",
    "    price = auto['high_price'][cols]\n",
    "    if price > 15000:\n",
    "        price=1\n",
    "    else:\n",
    "        price=0\n",
    "    auto['high_price'][cols]=price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98c396d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symboling</th>\n",
       "      <th>normalized_losses</th>\n",
       "      <th>make</th>\n",
       "      <th>fuel_type</th>\n",
       "      <th>aspiration</th>\n",
       "      <th>num_of_doors</th>\n",
       "      <th>body_style</th>\n",
       "      <th>drive_wheels</th>\n",
       "      <th>engine_location</th>\n",
       "      <th>wheel_base</th>\n",
       "      <th>...</th>\n",
       "      <th>fuel_system</th>\n",
       "      <th>bore</th>\n",
       "      <th>stroke</th>\n",
       "      <th>compression_ratio</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>peak_rpm</th>\n",
       "      <th>city_mpg</th>\n",
       "      <th>highway_mpg</th>\n",
       "      <th>price</th>\n",
       "      <th>high_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>164</td>\n",
       "      <td>audi</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>fwd</td>\n",
       "      <td>front</td>\n",
       "      <td>99.8</td>\n",
       "      <td>...</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>102</td>\n",
       "      <td>5500</td>\n",
       "      <td>24</td>\n",
       "      <td>30</td>\n",
       "      <td>13950</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>164</td>\n",
       "      <td>audi</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>4wd</td>\n",
       "      <td>front</td>\n",
       "      <td>99.4</td>\n",
       "      <td>...</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>115</td>\n",
       "      <td>5500</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>17450</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>audi</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>fwd</td>\n",
       "      <td>front</td>\n",
       "      <td>105.8</td>\n",
       "      <td>...</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.4</td>\n",
       "      <td>8.5</td>\n",
       "      <td>110</td>\n",
       "      <td>5500</td>\n",
       "      <td>19</td>\n",
       "      <td>25</td>\n",
       "      <td>17710</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>audi</td>\n",
       "      <td>gas</td>\n",
       "      <td>turbo</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>fwd</td>\n",
       "      <td>front</td>\n",
       "      <td>105.8</td>\n",
       "      <td>...</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.13</td>\n",
       "      <td>3.4</td>\n",
       "      <td>8.3</td>\n",
       "      <td>140</td>\n",
       "      <td>5500</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>23875</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>192</td>\n",
       "      <td>bmw</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>two</td>\n",
       "      <td>sedan</td>\n",
       "      <td>rwd</td>\n",
       "      <td>front</td>\n",
       "      <td>101.2</td>\n",
       "      <td>...</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.8</td>\n",
       "      <td>8.8</td>\n",
       "      <td>101</td>\n",
       "      <td>5800</td>\n",
       "      <td>23</td>\n",
       "      <td>29</td>\n",
       "      <td>16430</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   symboling  normalized_losses  make fuel_type aspiration num_of_doors  \\\n",
       "0          2                164  audi       gas        std         four   \n",
       "1          2                164  audi       gas        std         four   \n",
       "2          1                158  audi       gas        std         four   \n",
       "3          1                158  audi       gas      turbo         four   \n",
       "4          2                192   bmw       gas        std          two   \n",
       "\n",
       "  body_style drive_wheels engine_location  wheel_base  ...  fuel_system  bore  \\\n",
       "0      sedan          fwd           front        99.8  ...         mpfi  3.19   \n",
       "1      sedan          4wd           front        99.4  ...         mpfi  3.19   \n",
       "2      sedan          fwd           front       105.8  ...         mpfi  3.19   \n",
       "3      sedan          fwd           front       105.8  ...         mpfi  3.13   \n",
       "4      sedan          rwd           front       101.2  ...         mpfi  3.50   \n",
       "\n",
       "   stroke  compression_ratio horsepower peak_rpm  city_mpg highway_mpg  price  \\\n",
       "0     3.4               10.0        102     5500        24          30  13950   \n",
       "1     3.4                8.0        115     5500        18          22  17450   \n",
       "2     3.4                8.5        110     5500        19          25  17710   \n",
       "3     3.4                8.3        140     5500        17          20  23875   \n",
       "4     2.8                8.8        101     5800        23          29  16430   \n",
       "\n",
       "   high_price  \n",
       "0           0  \n",
       "1           1  \n",
       "2           1  \n",
       "3           1  \n",
       "4           1  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c0856a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((127, 25), (127,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= auto.drop(columns=['price','high_price'], axis=1)\n",
    "y=auto['high_price']\n",
    "\n",
    "x_train,x_test,y_train,y_test= train_test_split(x,y, test_size=0.20  ,random_state=731)\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5050369c",
   "metadata": {},
   "source": [
    "### Binary Classification\n",
    "On the previous screen, we learned about logistic regression and its use as a classification model. We also took the continuous price column and converted it into a binary variable named high_price, which was 1 if price was above $15,000 and 0 otherwise. On this screen, we'll delve a little bit more into why we did this.\n",
    "When we try to classify a categorical variable with only two classes, we are performing **binary classification** If we are trying to classify a variable with multiple classes, we would then be dealing with **multi-class classification**. In general, binary classification is easier than multi-class, so we'll focus on this particular task for this course.\n",
    "\n",
    "<img src='bm.svg' width=500 height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d0b266",
   "metadata": {},
   "source": [
    "By defining $15,000 as a cutoff for price, we're defining the classes for our classification problem. In this situation, we're converting a continuous variable into a categorical one, but this won't always be the case. We've chosen $15,000 because it's slightly above the average price in the dataset, but there's no special meaning to it. Our task for this lesson will be to create a classification model that can determine which attributes make a \"high price\" car. A high price car is what we refer to as a case, whereas a low-price car is a non-case. Likewise, a case is when the outcome equals 1, and a non-case is when the outcome is 0.\n",
    "<img src='hp.svg' width=500 height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c907bf07",
   "metadata": {},
   "source": [
    "Before we can continue, we need to make sure that both the training and test sets contain instances of high price cars and cars that are not high price. If we tried to train a logistic regression on data with only high price cars, there's no information on low price cars, so it couldn't possibly know which predictor values predict one. This is the problem of **complete separation.**\n",
    "\n",
    "Let's confirm that both datasets have both high and low price cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "498d1efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    94\n",
       "1    33\n",
       "Name: high_price, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "387a85ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    25\n",
       "1     7\n",
       "Name: high_price, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93cb9fe",
   "metadata": {},
   "source": [
    "### The Logistic Regression Model\n",
    "Now that we've settled the issue of separation, we know that both the training and test sets have examples of both cases and non-cases. A classification model needs to have an example of both so that it can be properly trained. The logistic regression also contains a linear model, where the outcome is a linear combination of the predictors. For simplicity, we'll only include one predictor.\n",
    "<img src='lcg.png' width=400 height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc19035a",
   "metadata": {},
   "source": [
    "In the first equation, we have something that resembles a linear regression. There is an intermediate variable Z that represents the linear combination of the predictors. In the second equation, we see that the outcome Y is the result of a function of this intermediate variable and not just the variable by itself. We'll explain this function more on the next screen, but for now, let's take some time to understand why we need this function, which is integral to logistic regression and classification.\n",
    "In linear regression, we assume that the outcome is continuous and able to take values from âˆ’âˆž and âˆž\n",
    "We also allow the predictors themselves to be either continuous or categorical (via dummy variables). Then, we determine the best values for the regression coefficients that minimize the error. We don't need any function here because both sides can range between âˆ’âˆž and âˆž.\n",
    "In the case of logistic regression, the outcome can only take two values: 0 and 1. The right side of the logistic regression is still allowed to range between the infinities, so we need a function like h(Z) to help us go from this infinite range to the constrained range between 0 and 1.\n",
    "<img src='logr.svg' width=500 height=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e364bdb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symboling</th>\n",
       "      <th>normalized_losses</th>\n",
       "      <th>make</th>\n",
       "      <th>fuel_type</th>\n",
       "      <th>aspiration</th>\n",
       "      <th>num_of_doors</th>\n",
       "      <th>body_style</th>\n",
       "      <th>drive_wheels</th>\n",
       "      <th>engine_location</th>\n",
       "      <th>wheel_base</th>\n",
       "      <th>...</th>\n",
       "      <th>fuel_system</th>\n",
       "      <th>bore</th>\n",
       "      <th>stroke</th>\n",
       "      <th>compression_ratio</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>peak_rpm</th>\n",
       "      <th>city_mpg</th>\n",
       "      <th>highway_mpg</th>\n",
       "      <th>price</th>\n",
       "      <th>high_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>164</td>\n",
       "      <td>audi</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>fwd</td>\n",
       "      <td>front</td>\n",
       "      <td>99.8</td>\n",
       "      <td>...</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>102</td>\n",
       "      <td>5500</td>\n",
       "      <td>24</td>\n",
       "      <td>30</td>\n",
       "      <td>13950</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>164</td>\n",
       "      <td>audi</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>4wd</td>\n",
       "      <td>front</td>\n",
       "      <td>99.4</td>\n",
       "      <td>...</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>115</td>\n",
       "      <td>5500</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>17450</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>audi</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>fwd</td>\n",
       "      <td>front</td>\n",
       "      <td>105.8</td>\n",
       "      <td>...</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.4</td>\n",
       "      <td>8.5</td>\n",
       "      <td>110</td>\n",
       "      <td>5500</td>\n",
       "      <td>19</td>\n",
       "      <td>25</td>\n",
       "      <td>17710</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>audi</td>\n",
       "      <td>gas</td>\n",
       "      <td>turbo</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>fwd</td>\n",
       "      <td>front</td>\n",
       "      <td>105.8</td>\n",
       "      <td>...</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.13</td>\n",
       "      <td>3.4</td>\n",
       "      <td>8.3</td>\n",
       "      <td>140</td>\n",
       "      <td>5500</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>23875</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>192</td>\n",
       "      <td>bmw</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>two</td>\n",
       "      <td>sedan</td>\n",
       "      <td>rwd</td>\n",
       "      <td>front</td>\n",
       "      <td>101.2</td>\n",
       "      <td>...</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.8</td>\n",
       "      <td>8.8</td>\n",
       "      <td>101</td>\n",
       "      <td>5800</td>\n",
       "      <td>23</td>\n",
       "      <td>29</td>\n",
       "      <td>16430</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   symboling  normalized_losses  make fuel_type aspiration num_of_doors  \\\n",
       "0          2                164  audi       gas        std         four   \n",
       "1          2                164  audi       gas        std         four   \n",
       "2          1                158  audi       gas        std         four   \n",
       "3          1                158  audi       gas      turbo         four   \n",
       "4          2                192   bmw       gas        std          two   \n",
       "\n",
       "  body_style drive_wheels engine_location  wheel_base  ...  fuel_system  bore  \\\n",
       "0      sedan          fwd           front        99.8  ...         mpfi  3.19   \n",
       "1      sedan          4wd           front        99.4  ...         mpfi  3.19   \n",
       "2      sedan          fwd           front       105.8  ...         mpfi  3.19   \n",
       "3      sedan          fwd           front       105.8  ...         mpfi  3.13   \n",
       "4      sedan          rwd           front       101.2  ...         mpfi  3.50   \n",
       "\n",
       "   stroke  compression_ratio horsepower peak_rpm  city_mpg highway_mpg  price  \\\n",
       "0     3.4               10.0        102     5500        24          30  13950   \n",
       "1     3.4                8.0        115     5500        18          22  17450   \n",
       "2     3.4                8.5        110     5500        19          25  17710   \n",
       "3     3.4                8.3        140     5500        17          20  23875   \n",
       "4     2.8                8.8        101     5800        23          29  16430   \n",
       "\n",
       "   high_price  \n",
       "0           0  \n",
       "1           1  \n",
       "2           1  \n",
       "3           1  \n",
       "4           1  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9524f261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.scatterplot(data=auto, x='horsepower', y='high_price');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2901d1",
   "metadata": {},
   "source": [
    "On the previous screen, we saw that higher horsepower is more commonly tied to our definition of high price cars. Here was the result.\n",
    "We also saw that the logistic regression takes the following form:\n",
    "\n",
    "Based on the picture above, the function h(Z) translates small values of Z\n",
    " to be non-cases, while high values will be cases. There needs to be a transition between the two extremes. For our example, this transition might occur at some intermediate value of horsepower that best separates low price and high price cars. The function that we will examine is the sigmoid function, also known as the logistic function. The sigmoid function looks like the following:\n",
    " <img src='sigmod.svg' width=500 height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd86fb3",
   "metadata": {},
   "source": [
    "We consider the predictors to be fixed, so we control the values of the regression coefficients. As with linear regression, we want to select the coefficients that minimize some cost function. However, unlike linear regression, calculating the logistic regression coefficients is more difficult.\n",
    "\n",
    "Let's see how the sigmoid function might apply to our previous plot on horsepower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebcd3670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAuzklEQVR4nO3df1zV9d3/8ecBEToqJ+QQaaKgULNcC+HSBHVZjWatZtdWZrtlNWsXy3TG1pZrXytubrS2tRWJ2VXp3Ipxuc1ym2tyuRLFnNNglTpDQdFE4aCCwi5J+Hz/6OJcHsHDAQ7nc87nPO63G7fbzvvzPofXZ2bn2fvXx2YYhiEAAACLiDC7AAAAAH8i3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsZZHYBgdbR0aEjR45o2LBhstlsZpcDAAB8YBiGTp06pZEjRyoiwvvYTNiFmyNHjigpKcnsMgAAQB8cOnRIo0aN8ton7MLNsGHDJH36f05sbKzJ1QAAAF80NzcrKSnJ/T3uTdiFm86pqNjYWMINAAAhxpclJSwoBgAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlhJ2j18AAAADp7rhtA4eb1Vy/BClOIeYUgPhBgAA9NvJ1jYtLK5UWVWDu216WoIK56TLYY8KaC1MSwEAgH5bWFyp8n0uj7byfS4tKK4IeC2EGwAA0C/VDadVVtWgdsPwaG83DJVVNajG1RLQegg3AACgXw4eb/V6/UAj4QYAAISQMcPtXq8nxwd2YTHhBgAA9MvYhKGanpagSJvNoz3SZtP0tISA75oi3AAAgH4rnJOu7FSnR1t2qlOFc9IDXgtbwQEAQL857FFaPW+SalwtOtDYwjk3AADAGlKc5oWaTkxLAQAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASxlkdgEAAISa6obTOni8VcnxQ5TiHGJ2OTgP4QYAAB+dbG3TwuJKlVU1uNumpyWocE66HPYoEyvDuZiWAgDARwuLK1W+z+XRVr7PpQXFFSZVhO4QbgAA8EF1w2mVVTWo3TA82tsNQ2VVDapxtZhUGc5HuAEAwAcHj7d6vX6gkXATLAg3AAD4YMxwu9fryfEsLA4WhBsAAHwwNmGopqclKNJm82iPtNk0PS2BXVNBhHADAICPCuekKzvV6dGWnepU4Zx0kypCd9gKDgCAjxz2KK2eN0k1rhYdaGzhnJsgRbgBAKCXUpyEmmDGtBQAALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAU08NNUVGRUlJSFBMTo4yMDG3evNlr/9dee02f+9znZLfbNWLECN1///1qbGwMULUAACDYmRpuSkpKtGjRIj3++OOqqKjQtGnTNHPmTNXW1nbbf8uWLZo7d67mzZunXbt2ac2aNfr73/+uBx54IMCVAwCAYGVquHn22Wc1b948PfDAAxo/frx+8YtfKCkpScuXL++2/7Zt25ScnKyFCxcqJSVFU6dO1X/8x39ox44dF/wdZ86cUXNzs8cPAACwLtPCTVtbm3bu3KmcnByP9pycHG3durXb92RlZenw4cNav369DMPQsWPH9Nvf/la33HLLBX9PQUGBHA6H+ycpKcmv9wEAAIKLaeHG5XKpvb1diYmJHu2JiYk6evRot+/JysrSa6+9ptmzZ2vw4MG69NJLdfHFF6uwsPCCv2fx4sVqampy/xw6dMiv9wEAAIKL6QuKbTabx2vDMLq0ddq9e7cWLlyoJUuWaOfOnXrrrbdUU1Oj3NzcC35+dHS0YmNjPX4AAIB1DTLrFzudTkVGRnYZpamvr+8ymtOpoKBA2dnZevTRRyVJV199tYYMGaJp06Zp6dKlGjFixIDXDQAAgptpIzeDBw9WRkaGSktLPdpLS0uVlZXV7XtaW1sVEeFZcmRkpKRPR3wAAABMnZbKy8vTyy+/rFdffVV79uzRI488otraWvc00+LFizV37lx3/1tvvVW///3vtXz5clVXV6u8vFwLFy7UpEmTNHLkSLNuAwAABBHTpqUkafbs2WpsbFR+fr7q6uo0YcIErV+/XmPGjJEk1dXVeZx5c9999+nUqVN64YUX9O1vf1sXX3yxrr/+ev34xz826xYAAECQsRlhNp/T3Nwsh8OhpqYmFhcDABAievP9bfpuKQAAAH8i3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsx9ZwbAADOtWlvvSoPn9TE0XGalpZgdjkIUYQbAIDpDja2aNaycp1o/cTdFmeP0rr5U5UUbzexMoQipqUAAKY7P9hI0onWT3Tbsi0mVYRQRrgBAJjqhY1VXYJNpxOtn2hzVUOAK0KoY1oKAGCK7qaiuvNe7QnW36BXGLkBAJjCl2AjSRNHxwWgGlgJ4QYAEHCb9tb7FGzi7FGM2qDXCDcAgID7RWlVj306d0sBvcWaGwBAwGz4sE7f+PV7Pfb77k1X6KEZqQGoCFbEyA0AIGB8CTZx9iiCDfqFcAMACIj8dbt67MNUFPyBaSkAQEBsrXZ5vZ4UF6PN37shQNXAyhi5AQAERNZYp9frN101IkCVwOoINwCAgFhy21Ver//gS1cGqBJYHeEGABAwr8zN7FU70Bc2wzAMs4sIpObmZjkcDjU1NSk2NtbscgAgLC39425t2degqakJjNjAJ735/mZBMQAg4Ag0GEhMSwEAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEthtxQAoN+qG07r4PFWJccPUYpziNnlIMwRbgAAfXaytU0LiytVVtXgbpuelqDCOely2KNMrAzhjGkpAECfLSyuVPk+zwdilu9zaUFxhUkVAYQbAEAfbdpbr7KqBrWfd9B9u2GorKpBNa4WkypDuGNaCgDQK91NRXXnQGML629gCkZuAAC90t1UVHeS4wk2MAcjNwAAn1U3nO5xxCbSZlN2qpNRG5iGkRsAgM8OHm/tsU92qlOFc9IDUA3QPUZuAAA+GzPc7vX6r+ZN0rS0hABVA3SPkRsAgM/GJgzV9LQERdpsHu2RNpumpyUQbBAUCDcAgF4pnJOu7FSnRxtTUQgmTEsBAHrFYY/S6nmTVONq0YHGFh65gKBDuAEA9EmKk1CD4MS0FAAAsBTCDQAAsBSmpQAAXZRsr9W7NY3KHufUHZlJZpcD9IrNMM574pnFNTc3y+FwqKmpSbGxsWaXAwBB5YPDJ3V70Vad7fi/r4ZBETatm5+tKy9zmFgZwl1vvr+ZlgIAuJ0fbCTpbIeh25aVm1QR0HuEGwCApE+nos4PNp3Odhhas+NQgCsC+oZwAwCQJL1b0+j1evn+np8EDgQDwg0AQJI0JSXe6/XscU6v14FgQbgBAEiSZk8arUERtm6vDYqwsWsKIYNwAwBwWzc/u0vA6dwtBYQKzrkBALhdeZlD+350s9bsOKTy/S7OuUFIItwAALq4IzOJUIOQxbQUAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFNPDTVFRkVJSUhQTE6OMjAxt3rzZa/8zZ87o8ccf15gxYxQdHa1x48bp1VdfDVC1AAAg2Jl6iF9JSYkWLVqkoqIiZWdna8WKFZo5c6Z2796t0aNHd/ueO++8U8eOHdMrr7yi1NRU1dfX6+zZswGuHAAABCubYRiGWb988uTJmjhxopYvX+5uGz9+vGbNmqWCgoIu/d966y3dddddqq6u1vDhw/v0O5ubm+VwONTU1KTY2Ng+1w4AAAKnN9/fpk1LtbW1aefOncrJyfFoz8nJ0datW7t9z7p165SZmalnnnlGl112mS6//HJ95zvf0b/+9a8L/p4zZ86oubnZ4wcAAFiXadNSLpdL7e3tSkxM9GhPTEzU0aNHu31PdXW1tmzZopiYGK1du1Yul0sPPfSQjh8/fsF1NwUFBXrqqaf8Xj8AAAhOpi8ottlsHq8Nw+jS1qmjo0M2m02vvfaaJk2apJtvvlnPPvusVq1adcHRm8WLF6upqcn9c+jQIb/fAwAACB6mjdw4nU5FRkZ2GaWpr6/vMprTacSIEbrsssvkcDjcbePHj5dhGDp8+LDS0tK6vCc6OlrR0dH+LR4AAAQt00ZuBg8erIyMDJWWlnq0l5aWKisrq9v3ZGdn68iRIzp9+rS77aOPPlJERIRGjRo1oPUCAIDQYOq0VF5enl5++WW9+uqr2rNnjx555BHV1tYqNzdX0qdTSnPnznX3v/vuuxUfH6/7779fu3fvVllZmR599FF9/etf10UXXWTWbQAAgCBi6jk3s2fPVmNjo/Lz81VXV6cJEyZo/fr1GjNmjCSprq5OtbW17v5Dhw5VaWmpFixYoMzMTMXHx+vOO+/U0qVLzboFAAAQZEw958YMnHMDAEDoCYlzbgAAAAYC4QYAAFiKqWtuAAD+Ud1wWgePtyo5fohSnEPMLgcwFeEGAELYydY2LSyuVFlVg7ttelqCCueky2GPMrEywDxMSwFACFtYXKnyfS6PtvJ9Li0orjCpIsB8hBsACFHVDadVVtWg9vM2vbYbhsqqGlTjajGpMsBchBsACFEHj7d6vX6gkXCD8ES4AYAQNWa43ev15HgWFiM8EW4AIESNTRiq6WkJirTZPNojbTZNT0tg1xTCFuEGAEJY4Zx0Zac6PdqyU50qnJNuUkWA+dgKDgAhzGGP0up5k1TjatGBxhbOuQFEuAGAkHKhw/pSnIQaoBPhBgBCAIf1Ab5jzQ0AhAAO6wN8R7gBgCDHYX1A7xBuACDI/a2m0et1DusDPLHmBgCCVHfrbLrDYX2AJ8INAASp7tbZnCvSZlN2qpNdUsB5CDcAEIQ619l4w2F9QPcINwAQhHp6KGbBv39WcyaNDlA1QGhhQTEABKGeHop57dj4AFUChB7CDQAEIR6KCfQd4QYAghQPxQT6hjU3ABCkeCgm0DeEGwAIEjwUE/APwg0AmIyHYgL+xZobADAZD8UE/ItwAwAm4qGYgP8RbgDARD0d1sdDMYHeI9wAgIl6OqyPh2ICvUe4AQATcVgf4H99Cjf5+fn661//2qW9paVF+fn5/S4KAMIJh/UB/mUzjPNWsfkgIiJCUVFRKigoUF5enrv92LFjGjlypNrb2/1apD81NzfL4XCoqalJsbGxZpcDAG4c1gdcWG++v/t8zs3q1av18MMP6/3339dLL72kwYMH9/WjAADisD7AX/q85mbGjBnatm2btm/fruuuu07Hjh3zZ10AAAB90qdwY/vfhW/jxo3Ttm3bFBsbq8zMTO3YscOvxQEAAPRWn8LNuct0YmNjtX79et1+++2aNWuWv+oCAADokz6tuVm5cqUcDof7dUREhJ5//nlNnDhRZWVlfisOAACgt/oUbu69915t3LhRGzduVH19vTo6OvxdFwAAQJ/0Kdw89dRTys/PV2ZmpkaMGOFegyPJ438DAAAEWp/CzYsvvqhVq1bpnnvu8Xc9AAAA/dKnBcVtbW3Kysrydy0AAAD91qdw88ADD+j111/3dy0AAAD95vO01LmPWejo6NBLL72k//7v/9bVV1+tqKgoj77PPvus/yoEAADoBZ/DTUVFhcfra665RpL04YcferSzoBgAAJjJ53Dz9ttvD2QdAAAAftHnZ0sBAAAEI8INAACwFMINAACwFMINAACwFMINAACwlD49fgEA8H+qG07r4PFWJccPUYpziNnlAGGPcAMAfXSytU0LiytVVtXgbpuelqDCOely2KO8vBPAQGJaCgD6aGFxpcr3uTzayve5tKC44gLvABAIhBsA6IPqhtMqq2pQu2F4tLcbhsqqGlTjajGpMgCEGwDog4PHW71eP9BIuAHMQrgBgD4YM9zu9XpyPAuLAbMQbgCgD8YmDNX0tARFnvew4EibTdPTEtg1BZiIcAMAfVQ4J13ZqU6PtuxUpwrnpJtUEQCJreAA0GcOe5RWz5ukGleLDjS2cM4NECRMH7kpKipSSkqKYmJilJGRoc2bN/v0vvLycg0aNEjXXHPNwBYIAD1IcQ7RjCsuIdgAQcLUcFNSUqJFixbp8ccfV0VFhaZNm6aZM2eqtrbW6/uampo0d+5c3XDDDQGqFAAAhAqbYZx3SEMATZ48WRMnTtTy5cvdbePHj9esWbNUUFBwwffdddddSktLU2RkpN544w1VVlb6/Dubm5vlcDjU1NSk2NjY/pQPAAACpDff36aN3LS1tWnnzp3KycnxaM/JydHWrVsv+L6VK1dq//79euKJJ3z6PWfOnFFzc7PHDwAAsC7Two3L5VJ7e7sSExM92hMTE3X06NFu31NVVaXHHntMr732mgYN8m0tdEFBgRwOh/snKSmp37UDAIDgZfqCYtt5Z0QYhtGlTZLa29t1991366mnntLll1/u8+cvXrxYTU1N7p9Dhw71u2YAABC8TNsK7nQ6FRkZ2WWUpr6+vstojiSdOnVKO3bsUEVFhR5++GFJUkdHhwzD0KBBg7RhwwZdf/31Xd4XHR2t6OjogbkJAAAQdEwbuRk8eLAyMjJUWlrq0V5aWqqsrKwu/WNjY/XBBx+osrLS/ZObm6srrrhClZWVmjx5cqBKBwAAQczUQ/zy8vJ0zz33KDMzU1OmTNFLL72k2tpa5ebmSvp0Sunjjz/W6tWrFRERoQkTJni8/5JLLlFMTEyXdgAAEL5MDTezZ89WY2Oj8vPzVVdXpwkTJmj9+vUaM2aMJKmurq7HM28AAADOZeo5N2bgnBsAAEJPSJxzAwAAMBAINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIGmV0AAPTHCxurVL7fpWlpCXpoRqrZ5QAIAoQbACFp674G3f3ydvfrd6uP65m/7FXJg9dq8rh4EysDYDampQCEpHODzblm/+e2AFcCINgQbgCEnBc2Vnm9XvT2vgBVAiAYEW4AhJzy/S6v1zdXNQSoEgDBiHADIORkj3N6vT4tLSFAlQAIRoQbACHn4RvSvF5n1xQQ3gg3AELGpr31em7jR9pc1aCSB6/tts+F2gGED7aCAwh6BxtbNGtZuU60fuJui7NHafOjM/SH949oc1UD59wAcLMZhmGYXUQgNTc3y+FwqKmpSbGxsWaXA8AH6fkbPIJNpzh7lCqW5JhQEYBA6833N9NSAILapr313QYbSTrR+gk7owB0QbgBENTe3lvv9fp7tScCVAmAUMGaGwBB6WRrmxYWV6qsh5GZiaPjAlQRgFDByA2AoLSwuFLl+7wf1hdnj+JMGwBdMHIDIOhUN5zuccQmzh6ldfOnBqgiAKGEcAMg6Bw83ur1+ndyLtfD13s/yA9A+GJaCkDQGTPc7vX6LVePDFAlAEIR4QZA0BmbMFTT0xIUabN5tEfabJqelqAU5xCTKgMQCgg3AIJS4Zx0Zad6PiAzO9WpwjnpJlUEIFSw5gZAUHLYo7R63iTVuFp0oLFFyfFDGLEB4BPCDYCgluIk1ADoHaalAACApRBuAACApRBuAACApbDmBkBA/fsLW7Tn2CldNSJWv30o2+xyAFgQIzcAAuK50r1KfuxPeu9wk/71SYd21J5U8mN/0rK/VpldGgCLIdwACIifb9zXbftPNnwU4EoAWB3hBsCAu/nnm7xe/2pReYAqARAOCDcABszJ1jbNfWW7dh877bXfrrrmAFUEIBwQbgAMmIXFlSrf5+qx31UjYgNQDYBwwW4pAAPiG7/8u8qqGnzqy64pAP5EuAHgV2t3HtIja973uf+jOZcPYDUAwhHhBoBf+RpsJowYpj9+a/oAVwMgHBFuAPjNt0sqe+wTabMpO9Wp1fMmDXxBAMISC4oB+M3fDjT22Cc71anCOekBqAZAuCLcAPCbycnxXq9/8apErZ43SQ57VIAqAhCOTA83RUVFSklJUUxMjDIyMrR58+YL9v3973+vL3zhC0pISFBsbKymTJmiv/zlLwGsFsD5Nu2t13MbP9Lmqgb9bPY1Xvu+eE9mYIoCENZMXXNTUlKiRYsWqaioSNnZ2VqxYoVmzpyp3bt3a/To0V36l5WV6Qtf+IJ+9KMf6eKLL9bKlSt166236m9/+5vS0xnmBgLpYGOLZi0r14nWT9xtcfYoLbllvPL/tKdL/+d7CD4A4C82wzAMs3755MmTNXHiRC1fvtzdNn78eM2aNUsFBQU+fcZVV12l2bNna8mSJd1eP3PmjM6cOeN+3dzcrKSkJDU1NSk2loPDgL5Kz9/gEWw6xdmjVLEkR4+u+YferXZpylinfnLH50yoEICVNDc3y+Fw+PT9bdrITVtbm3bu3KnHHnvMoz0nJ0dbt2716TM6Ojp06tQpDR8+/IJ9CgoK9NRTT/WrVgCeXthY1W2wkaQTrZ9oc1UDgQaAaUxbc+NyudTe3q7ExESP9sTERB09etSnz/jZz36mlpYW3XnnnRfss3jxYjU1Nbl/Dh061K+6gXB2sLFF6fkb9NNS70/yfq/2RIAqAoCuTD/nxmazebw2DKNLW3eKi4v15JNP6s0339Qll1xywX7R0dGKjo7ud51AuMtft0ur3j2gDh8msieOjhv4ggDgAkwLN06nU5GRkV1Gaerr67uM5pyvpKRE8+bN05o1a3TjjTcOZJlA2NvwYZ2+8ev3fO4fZ4/StLSEAawIALwzbVpq8ODBysjIUGlpqUd7aWmpsrKyLvi+4uJi3XfffXr99dd1yy23DHSZQNjrbbBZN3/qAFYDAD0zdVoqLy9P99xzjzIzMzVlyhS99NJLqq2tVW5urqRP18t8/PHHWr16taRPg83cuXP13HPP6dprr3WP+lx00UVyOBym3QdgVfnrdvnc97s3XaGHZqQOYDUA4BtTw83s2bPV2Nio/Px81dXVacKECVq/fr3GjBkjSaqrq1Ntba27/4oVK3T27FnNnz9f8+fPd7ffe++9WrVqVaDLByxva7XLp35x9iiCDYCgYeo5N2bozT55INzlr9ulV7ce8NqncyoqKd4emKIAhKXefH+b/vgFAMFryW1Xeb3+q3mTVLEkh2ADIKgQbgB49crc7p8H9crcTHZFAQhKTEsB8MnSP+7Wln0NmpqaoB986UqzywEQZkLi8QsAQguBBkCoINwAYaq64bQOHm9VcvwQpTiHmF0OAPgN4QYIMydb27SwuFJlVQ3utulpCSqcky6HPcrEygDAP1hQDISZhcWVKt/neX5N+T6XFhRXmFQRAPgX4QYII9UNp1VW1aD28/YRtBuGyqoaVONqMakyAPAfwg0QRg4eb/V6/UAj4QZA6CPcAGFkzHDvh+0lx7OwGEDoI9wAYWRswlBNT0tQpM3m0R5ps2l6WgK7pgBYAuEGCDOFc9KVner0aMtOdapwTrpJFQGAf7EVHAgzDnuUVs+bpBpXiw40tnDODQDLIdwAFvHCxiqV73dpWlqCHpqR2mP/FCehBoA1EW6AELd1X4Pufnm7+/W71cf1zF/2quTBazV5XLyJlQGAOVhzA4Swku21HsHmXLP/c1uAqwGA4MDIDRCCPjh8UrcXbdXZDsNrv6K39/k0RQUAVsLIDRBiNu2t120vlPcYbCRp8znPjwKAcMHIDRAiDja2aNaycp1o/cTn90xLSxjAigAgODFyA4SI3gYbSUxJAQhLhBsgBGzaW9/rYFPy4LUDVA0ABDempYAQUHn4pM99v3vTFYzYAAhrhBsgiFzoIL5rRl3c43sHRdi0bn62rrzMMYAVAkDwI9wAQaCng/g+f8UlirNHdTs1NTjSph/e/lndkZkUyJIBIGix5gYIAr4cxLdu/lTF2aM8rsfZo7Qx7zqCDQCcg5EbwATnTj91+HgQX1K8XRVLcrS5qkHv1Z7QxNFxbPUGgG4QboAA6m76qSebqxo81t9MS0sg1ACAF0xLAQGyaW/9BaefvCHIAEDvMHIDDLC+nCx8LrZ1A0DvMHIDDLD+BBsO4gOA3mPkBvCzcxcLXzUy1udg892brpD06Rqb88+5AQD4jnAD+ElfFgufqzPMEGoAoH8IN0A/VDec1sHjrUqOH9KnxcKdmH4CAP8h3AB9cLK1TQuLK1VW1dDnzxgUIeV9gedAAYC/EW6APlhYXKnyfS6f+0fapPZzzuqLs0dp3fypSoq3D0B1ABDeCDeAD/LX7dLWapempibo7smjez1i8+2cK/TZUQ5OFgaAALAZhuH97HeLaW5ulsPhUFNTk2JjY80uB0Fuw4d1+sav3+v35xx4+hY/VAMA4as339+M3ADnOHcb90MzUv0SbFgsDACBRbhB2CvZXqt1/zii8v2N7rZ3q4/rmb/s9fo+m6Rzhz0jbTZlpzp17djhnFUDACYi3CAs5a/bpY3/PKba4/9SX+dl7YMj1dLW7n6dnepU4Zx0OexRhBoAMBHhBmHFX2toJGnOpNH62rVjdKCxRcnxQ5TiHOKXzwUA9A/hBpZ27i6nH3zpSr8FG0n6wZeulCRCDQAEGcINLOn8EZp/Hj2tl7fU+O3zX5mb6bfPAgD4F+EGluTPEZoDT9+ipX/crS37GtwjQACA4EW4geXkr9vlt8/q3MZNoAGA0EG4geVsrfb9sQjni7BJV40YppmfHcmOJwAIUYQbBKXzFwL3RtZYp/559HSv3nNtynB9JWOU7shM6tX7AADBh8cvIOBKttfq3ZpGZY9zdgkTF9qq/crcTN1wZaLPvyP5sT9d8BpraAAg9PTm+5twg4D54PBJ3V60VWc7/u8fuUERNq2bn60rL3NI6jmU+Grj7mOat3pHl/behiQAQHDozfd3RIBqQpj49xe2aPz/+7O+WlTe5dr5wUaSznYYum3Zp317Wgi89I+7fa7jhisTdeDpW/TA1BR95tKhemBqig48fQvBBgDCAGtu0Cfjf7Be/zpryD7IpuT4Idp9zHONy47ak0p+7E96NOdyzb8+TSXba7sEm05nOwyt2XGox4XAW/Y19LpOppwAIPwwcoNemb18i5If+5P+dfbToNJ61ugSbM71kw0fSZLerWm8YB9JKt/vUtZYp9c+U1MTelktACAcEW7CzLdLKjX1xxv16Jp/9On9fzvY1Ov3fLWoXFNS4r32yR7n1JLbrvLah1EYAIAvCDcWVt1wWm/vrVeNq0Vrdx5S8mN/0u8qPtbhE/+jNTsPK/mxP2ldxcc+f974H6zvUx276po1e9JoDYqwdXt9UITNvWvqQo814HEHAABfseYmyFU3nNbB461Kjh+i2sYWVR4+qYmj4zQtLaHbPinOITrZ2qaFxZUqq+p5jcrCkkrdln6ZT7V0TkX11lUjPl3Vvm5+tm5bVt7tbqlOnQuB2aoNAOgrtoKbaNPe+m7DiqQeA0qcPUq/njdJP37rI48+09MSdLajQ3+rPq52H/9o78gYpZ/c8bke+3UuIu6t87dwr9lxSOX7Xd2ecwMAQHc458aLgQw354+gXMjBxhbNWlauE62fuNvi7FFaN3+qkuLtkqS5r2xX+T6X14AyKMImw5BHnwhJHb2se1RcjLZ87waf+no7h6Y7nbulAADoj958fzMt5QfdjbJMT0tQ4Zx0OexRXfqfH2wk6UTrJ7pt2RZVLMlRdcNpn6aUutta3dtgI0lTetildK6p4+K0Zf8Jr30ibVJ60sX67UPZXvsBADAQCDd+sLC4UuX7PM9oKd/n0oLiCq2eN8mjfdPe+i7BptOJ1k+0uarhgufBDBRfpqQ6/frBLEnSlT9Yr9b/PedmbMJQfVR/WpdfMlR//Nb0gSoTAACfEG766UKjLO2GobKqBtW4WjymqCoPn/T6ee/VntCtV4/sd10RNsmXjPT87Gv69Pm7l97cp/cBADDQTN8KXlRUpJSUFMXExCgjI0ObN2/22n/Tpk3KyMhQTEyMxo4dqxdffDFAlXbv4PFWr9cPNLZ4vL5m1MVe+08cHaexCUM1PS1Bkbbut053GhRh69In0mbTlLHxXQ68m56WoH8sydEdGaM0Ki5Gd2SM0oGnb/F5pxQAAKHC1JGbkpISLVq0SEVFRcrOztaKFSs0c+ZM7d69W6NHj+7Sv6amRjfffLMefPBB/frXv1Z5ebkeeughJSQk6Ctf+YoJdyCNGW73ej053nNh8eevuERx9qhup6bi7FHuXVOFc9K1oLjC626p1+ZN1tNv7fXok53qdK/1qXG16EBji8cC595MQQEAEIpM3S01efJkTZw4UcuXL3e3jR8/XrNmzVJBQUGX/t/73ve0bt067dmzx92Wm5urf/zjH3r33Xe7/R1nzpzRmTNn3K+bm5uVlJTk191S3e1sirTZlJ3q7LLmRpIONbbqtmVbvO6W6nRuQDl8olXv1Z7osnW8uxADAICVhMRW8La2Ntntdq1Zs0a33367u/1b3/qWKisrtWnTpi7vmT59utLT0/Xcc8+529auXas777xTra2tiorqujPpySef1FNPPdWl3Z/hpqn1ky6jLN52S3XaXNXQbVgBAACeQmIruMvlUnt7uxITEz3aExMTdfTo0W7fc/To0W77nz17Vi6XSyNGjOjynsWLFysvL8/9unPkxp8c9iitnjep1yMo09ISCDUAAPiZ6bulbOctiDUMo0tbT/27a+8UHR2t6OjoflbpmxQn00IAAJjNtN1STqdTkZGRXUZp6uvru4zOdLr00ku77T9o0CDFx3t/6jQAAAgPpoWbwYMHKyMjQ6WlpR7tpaWlysrK6vY9U6ZM6dJ/w4YNyszM7Ha9DQAACD+mnnOTl5enl19+Wa+++qr27NmjRx55RLW1tcrNzZX06XqZuXPnuvvn5ubq4MGDysvL0549e/Tqq6/qlVde0Xe+8x2zbgEAAAQZU9fczJ49W42NjcrPz1ddXZ0mTJig9evXa8yYMZKkuro61dbWuvunpKRo/fr1euSRR7Rs2TKNHDlSzz//vGln3AAAgODDU8EBAEDQ6833t+mPXwAAAPAnwg0AALAUwg0AALAUwg0AALAUwg0AALAU0x+/EGidm8Oam5tNrgQAAPiq83vbl03eYRduTp06JUl+f3gmAAAYeKdOnZLD4fDaJ+zOueno6NCRI0c0bNgwrw/otILOJ6AfOnTI8mf6cK/WFU73y71aVzjd70Ddq2EYOnXqlEaOHKmICO+rasJu5CYiIkKjRo0yu4yAio2Ntfxfpk7cq3WF0/1yr9YVTvc7EPfa04hNJxYUAwAASyHcAAAASyHcWFh0dLSeeOIJRUdHm13KgONerSuc7pd7ta5wut9guNewW1AMAACsjZEbAABgKYQbAABgKYQbAABgKYQbAABgKYSbMHPmzBldc801stlsqqysNLucAXHbbbdp9OjRiomJ0YgRI3TPPffoyJEjZpfldwcOHNC8efOUkpKiiy66SOPGjdMTTzyhtrY2s0sbMD/84Q+VlZUlu92uiy++2Oxy/KqoqEgpKSmKiYlRRkaGNm/ebHZJA6KsrEy33nqrRo4cKZvNpjfeeMPskgZMQUGB/u3f/k3Dhg3TJZdcolmzZmnv3r1mlzUgli9frquvvtp9cN+UKVP05z//2bR6CDdh5rvf/a5GjhxpdhkDasaMGfqv//ov7d27V7/73e+0f/9+ffWrXzW7LL/75z//qY6ODq1YsUK7du3Sz3/+c7344ov6/ve/b3ZpA6atrU133HGHvvnNb5pdil+VlJRo0aJFevzxx1VRUaFp06Zp5syZqq2tNbs0v2tpadHnPvc5vfDCC2aXMuA2bdqk+fPna9u2bSotLdXZs2eVk5OjlpYWs0vzu1GjRunpp5/Wjh07tGPHDl1//fX68pe/rF27dplTkIGwsX79euMzn/mMsWvXLkOSUVFRYXZJAfHmm28aNpvNaGtrM7uUAffMM88YKSkpZpcx4FauXGk4HA6zy/CbSZMmGbm5uR5tn/nMZ4zHHnvMpIoCQ5Kxdu1as8sImPr6ekOSsWnTJrNLCYi4uDjj5ZdfNuV3M3ITJo4dO6YHH3xQv/rVr2S3280uJ2COHz+u1157TVlZWYqKijK7nAHX1NSk4cOHm10GeqGtrU07d+5UTk6OR3tOTo62bt1qUlUYCE1NTZJk+b+j7e3t+s1vfqOWlhZNmTLFlBoIN2HAMAzdd999ys3NVWZmptnlBMT3vvc9DRkyRPHx8aqtrdWbb75pdkkDbv/+/SosLFRubq7ZpaAXXC6X2tvblZiY6NGemJioo0ePmlQV/M0wDOXl5Wnq1KmaMGGC2eUMiA8++EBDhw5VdHS0cnNztXbtWl155ZWm1EK4CWFPPvmkbDab158dO3aosLBQzc3NWrx4sdkl95mv99rp0UcfVUVFhTZs2KDIyEjNnTtXRogcxt3be5WkI0eO6Itf/KLuuOMOPfDAAyZV3jd9uV8rstlsHq8Nw+jShtD18MMP6/3331dxcbHZpQyYK664QpWVldq2bZu++c1v6t5779Xu3btNqYXHL4Qwl8sll8vltU9ycrLuuusu/eEPf/D4F2V7e7siIyP1ta99Tb/85S8HutR+8/VeY2JiurQfPnxYSUlJ2rp1q2lDpL3R23s9cuSIZsyYocmTJ2vVqlWKiAit/2bpy5/tqlWrtGjRIp08eXKAqxt4bW1tstvtWrNmjW6//XZ3+7e+9S1VVlZq06ZNJlY3sGw2m9auXatZs2aZXcqAWrBggd544w2VlZUpJSXF7HIC5sYbb9S4ceO0YsWKgP/uQQH/jfAbp9Mpp9PZY7/nn39eS5cudb8+cuSIbrrpJpWUlGjy5MkDWaLf+Hqv3enM72fOnPFnSQOmN/f68ccfa8aMGcrIyNDKlStDLthI/fuztYLBgwcrIyNDpaWlHuGmtLRUX/7yl02sDP1lGIYWLFigtWvX6p133gmrYCN9ev9m/XuXcBMGRo8e7fF66NChkqRx48Zp1KhRZpQ0YLZv367t27dr6tSpiouLU3V1tZYsWaJx48aFxKhNbxw5ckTXXXedRo8erZ/+9KdqaGhwX7v00ktNrGzg1NbW6vjx46qtrVV7e7v7rKbU1FT3P9ehKC8vT/fcc48yMzM1ZcoUvfTSS6qtrbXk+qnTp09r37597tc1NTWqrKzU8OHDu/y7KtTNnz9fr7/+ut58800NGzbMvYbK4XDooosuMrk6//r+97+vmTNnKikpSadOndJvfvMbvfPOO3rrrbfMKciUPVowVU1NjWW3gr///vvGjBkzjOHDhxvR0dFGcnKykZubaxw+fNjs0vxu5cqVhqRuf6zq3nvv7fZ+3377bbNL67dly5YZY8aMMQYPHmxMnDjRstuF33777W7/DO+9916zS/O7C/39XLlypdml+d3Xv/519z+/CQkJxg033GBs2LDBtHpYcwMAACwl9CboAQAAvCDcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAh5Bw4ckM1m6/Jz3XXXmV0aABMMMrsAAOivpKQk1dXVuV8fPXpUN954o6ZPn25iVQDMwlPBAVjK//zP/+i6665TQkKC3nzzTUVEMEANhBtGbgBYyrx583Tq1CmVlpYSbIAwRbgBYBlLly7VW2+9pe3bt2vYsGFmlwPAJExLAbCE3/3ud5ozZ47+/Oc/64YbbjC7HAAmItwACHkffvihJk+erLy8PM2fP9/dPnjwYA0fPtzEygCYgXADIOStWrVK999/f5f2z3/+83rnnXcCXxAAUxFuAACApbCVAAAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWMr/B2yDwwbkRKHtAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "beta0 = -7\n",
    "beta1 = 0.05\n",
    "\n",
    "\n",
    "auto[\"z\"] = beta0 + beta1 * auto[\"horsepower\"]\n",
    "auto[\"hz\"] = 1 / (1 + np.exp(-auto[\"z\"]))\n",
    "\n",
    "auto.plot.scatter(x=\"z\", y=\"hz\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bc79cb",
   "metadata": {},
   "source": [
    "##  Probability, Odds, and Log-Odds\n",
    "Like linear regression, logistic regression enjoys the benefit of clear interpretability. Before we can understand these interpretations, we need to understand exactly how the predictors influence the outcome itself.\n",
    "\n",
    "Let's look back at the two equations for logistic regression:\n",
    "\n",
    "<img src='2_logequations.png' width=500 height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e802e2be",
   "metadata": {},
   "source": [
    "<img src='logit_function.png' width=500 height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec96424",
   "metadata": {},
   "source": [
    "The above function is important in both statistics and machine learning, so it has a special name: the **logit function.** The logit function is the inverse of the sigmoid function, and it tells us how the predictors change the outcome. Because it \"links\" a function of the outcome to the linear model, we say that the logit function is the **link function** for logistic regression.\n",
    "\n",
    "Recall that the outcome Y is a binary variable in this case, like a coin flip. When we take the average of Y, we get the success probability that y = 1 In our running example, EY would be the probability that the car is a high price.\n",
    "\n",
    "While the probability is easy to interpret, it has the undesirable property of being limited to values between 0 and 1. This limitation makes it difficult to work with, so it's easier to work with a function of the probability. \n",
    "This brings us to the odds. Odds are defined as the ratio of the probability that y = 1\n",
    " over the probability that y = 0\n",
    ". In terms of the probability, it would look like this:\n",
    "\n",
    "EY/1-EY.\n",
    "\n",
    "Odds are nice because they can range from âˆ’âˆž to âˆž but still tell us about the relative likelihood of a binary outcome. This finally leads us to the **log-odds**, which is the logarithm of the odds. We'll discuss why log-odds are important in the next lesson, but you can get an idea by looking at the equation above. This structure is similar to what we've seen with linear regression, but the left side has changed. What could this mean?\n",
    "\n",
    "To summarize, we can discuss three distinct-but-related outcomes with binary classification: the probability, the odds and the log-odds. Changes in the predictor change the log-odds, which also influence the other two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e533af51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let us calculate the odds\n",
    "\n",
    "prob= np.mean(y_train)\n",
    "odds= prob/(1- prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b41d3b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.351063829787234"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61463d1a",
   "metadata": {},
   "source": [
    "## The Logistic Cost Function\n",
    "Knowing how to differentiate probability, odds, and log-odds will be useful when we know the values of the regression coefficients. But we've only learned about the structure of a logistic regression. We don't know how to derive the coefficients.\n",
    "\n",
    "Unlike linear regression, there is not a **closed form** to the logistic regression coefficients. Having a \"closed form\" means that there is a mathematical expression that we can derive and use to calculate the coefficients. Recall that the linear regression coefficients are calculated by minimizing the sum of squared errors, which results in the following\n",
    "\n",
    "<img src='linreg.png' width=500 height=500>\n",
    "                                           \n",
    "There is no equivalent for logistic regression. So how do we retrieve the coefficients? We must start at the **cost function**. As with linear regression, logistic regression also has a cost function that we must try to minimize, based on the observed data. Instead of dealing with the SSE, the classification aspect of logistic regression requires us to consider a different cost function. This function is the **log-loss**\n",
    "\n",
    "<img src='logloss.png' width=500 height=500>\n",
    "Why does the log-loss function take on two values? The answer lies in the fact that we are trying to predict a binary variable. We are using the sigmoid function to transform the linear combination of predictors into a number between 0 and 1. This explains why h is used in the log-loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765d0991",
   "metadata": {},
   "source": [
    "<img src='loglosscurve.svg' width=500 height=500>\n",
    "\n",
    "We have two components to the log-loss because are there only two outcomes: 0 and 1. We need to make sure that the cost is low or zero when the logistic regression is making good predictions. When the model is wrong, the log-loss will increase exponentially for both cases. Similar to linear regression, we are comparing the observed outcome with the prediction that the model makes in the cost function.\n",
    "\n",
    "Now that we have our cost function where does that leave us? Even though we can't create a closed form, we can still take derivatives on the log-loss cost function. This gives us access to gradient descent! We'll implement this on the next screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "659a48d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "auto[\"L\"] = 0\n",
    "\n",
    "auto.loc[auto[\"high_price\"] == 1, \"L\"] = -np.log(auto[\"hz\"])\n",
    "auto.loc[auto[\"high_price\"] == 0, \"L\"] = -np.log(1 - auto[\"hz\"])\n",
    "\n",
    "loss = sum(auto[\"L\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1dae90",
   "metadata": {},
   "source": [
    "## Using Gradient Descent\n",
    "We learned on the previous screen that logistic regression uses the log-loss as the cost function to minimize:\n",
    "\n",
    "Another way to write the log-loss is as follows:\n",
    "\n",
    "We encourage you to confirm that the two are equivalent. The equation above describes the loss for a single observation, and we saw on the previous screen that we can sum up all of the individual log-losses to get the total loss.\n",
    "\n",
    "We know that using gradient descent, we can choose a starting \"guess\" for the initial values of and and use iteration to create an improved next guess for these parameters. This guess is based on the derivative:\n",
    "\n",
    "Using this method, the values for the next parameters in the iteration are calculated as follows:\n",
    "where is a pre-specified learning rate. We can also see the partial derivatives for both parameters since we need to update two parameters. Using calculus, we can show that the above derivatives can be calculated as follows:\n",
    "\n",
    "Note that these two expressions for the derivative based on the average log-loss across the entire dataset. We can do this because the derivative will apply to every observation, so summing the values leads to summing the derivatives.\n",
    "\n",
    "Using all of this information, let's implement gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3c88fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "auto = pd.read_csv(\"automobiles.csv\")\n",
    "\n",
    "auto[\"high_price\"] = 0\n",
    "auto.loc[auto[\"price\"] > 15000, \"high_price\"] = 1\n",
    "\n",
    "X = auto.drop([\"price\", \"high_price\"], axis = 1)\n",
    "y = auto[\"high_price\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state = 731)\n",
    "\n",
    "\n",
    "def gradient_descent(x, y, init = np.array([0, 0]), iterations = 1000, learning_rate = 0.0001,\n",
    "                     stopping_threshold = 1e-6):\n",
    "    \n",
    "    # Set the previous cost and parameters\n",
    "    previous_cost = None\n",
    "    beta0 = init[0]\n",
    "    beta1 = init[1]\n",
    "\n",
    "    # Perform the gradient descent\n",
    "    for i in range(iterations):\n",
    "      \n",
    "        # Calculate the predicted outcome based on the predictor\n",
    "        hz = 1 / (1 + np.exp(-1 * (beta0 + beta1 * x)))\n",
    "\n",
    "        # Calculate the log-loss cost based on current parameters\n",
    "        costs = y * (-np.log(hz)) + (1 - y) * (-np.log(1 - hz))\n",
    "        current_cost = sum(costs)\n",
    "\n",
    "        # Check if we've met the conditions for breaking the loop\n",
    "        if previous_cost and abs(previous_cost - current_cost) <= stopping_threshold:\n",
    "          \n",
    "          break\n",
    "            \n",
    "        previous_cost = current_cost\n",
    "        \n",
    "        beta0_derivative = np.mean(hz - y)\n",
    "        beta1_derivative = np.mean(x * (hz - y))\n",
    "        \n",
    "        beta0 = beta0 - learning_rate * beta0_derivative\n",
    "        beta1 = beta1 - learning_rate * beta1_derivative\n",
    "        \n",
    "    return np.array([beta0, beta1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5204df56",
   "metadata": {},
   "source": [
    "## Deriving the Coefficients\n",
    "On the previous screen, we implemented gradient descent based on the log-loss. With this function, we can calculate the coefficients for logistic regression. When we start using the **scikit-learn** library to create our logistic regression models, we won't need to do this hand calculation, but we should always appreciate how the ML models are working.\n",
    "\n",
    "As a last note, the gradient descent we've implemented here isn't how scikit-learn derives the coefficients. We use gradient descent here to demonstrate that there are several ways for us to calculate model parameters â€” and to demonstrate that it is especially important in cases where there is no analytic solution.\n",
    "\n",
    "However, there are some precautions we must take because gradient descent is an iterative algorithm. In our implementation of the algorithm, we specified that the loop should terminate after 1000 iterations. If our initial guess for the parameters is far from where they actually minimize the cost, it might actually take more iterations to reach this point.\n",
    "\n",
    "This problem can be compounded by a small learning rate since it dictates how much the parameters can change with every iteration. If the step size is too small, the algorithm may need more iterations to truly reach the minimum. In practice, it's good to check how many iterations are necessary for the algorithm to terminate. If the maximum is reached, it's a good sign that this or the learning rate should be increased slightly. We could also change our initial guess, but we have no way of knowing if any guess is accurate or not. The number of iterations and learning rate at least give us a hint as to when the algorithm might be producing an incorrect value.\n",
    "\n",
    "Let's experiment with a few different values to demonstrate this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ceb20807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "auto = pd.read_csv(\"automobiles.csv\")\n",
    "\n",
    "auto[\"high_price\"] = 0\n",
    "auto.loc[auto[\"price\"] > 15000, \"high_price\"] = 1\n",
    "\n",
    "X = auto.drop([\"price\", \"high_price\"], axis = 1)\n",
    "y = auto[\"high_price\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state = 731)\n",
    "\n",
    "def gradient_descent(x, y, init = np.array([0, 0]), iterations = 1000, learning_rate = 0.0001,\n",
    "                     stopping_threshold = 1e-6):\n",
    "    \n",
    "    # Set the previous cost and parameters\n",
    "    previous_cost = None\n",
    "    beta0 = init[0]\n",
    "    beta1 = init[1]\n",
    "\n",
    "    # Perform the gradient descent\n",
    "    for i in range(iterations):\n",
    "      \n",
    "        # Calculate the predicted outcome based on the predictor\n",
    "        hz = 1 / (1 + np.exp(-1 * (beta0 + beta1 * x)))\n",
    "\n",
    "        # Calculate the log-loss cost based on current parameters\n",
    "        costs = y * (-np.log(hz)) + (1 - y) * (-np.log(1 - hz))\n",
    "        current_cost = sum(costs)\n",
    "\n",
    "        # Check if we've met the conditions for breaking the loop\n",
    "        if previous_cost and abs(previous_cost - current_cost) <= stopping_threshold:\n",
    "          \n",
    "          break\n",
    "            \n",
    "        previous_cost = current_cost\n",
    "        \n",
    "        beta0_derivative = np.mean(hz - y)\n",
    "        beta1_derivative = np.mean(x * (hz - y))\n",
    "        \n",
    "        beta0 = beta0 - learning_rate * beta0_derivative\n",
    "        beta1 = beta1 - learning_rate * beta1_derivative\n",
    "        \n",
    "    return np.array([beta0, beta1])\n",
    "\n",
    "\n",
    "first_gd = gradient_descent(auto[\"horsepower\"], auto[\"high_price\"], init = np.array([-7, 0.05]))\n",
    "second_gd = gradient_descent(auto[\"horsepower\"], auto[\"high_price\"], init = np.array([0, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aee8a668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.99997745,  0.05750936])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2db8d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01070029, -0.0060423 ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_gd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859dc6ed",
   "metadata": {},
   "source": [
    "## Interpreting the Regression Parameters\n",
    "##  Introduction\n",
    "Welcome to this lesson on **Logistic Regression Modeling in Python!** A logistic regression model predicts a categorical outcome. Categorical outcomes can either be binary or multi-class, but we'll be focusing on binary classification in this lesson. Binary classification is everywhere in our everyday lives. Predicting disease status (having a disease or not having it) using a model is a common application\n",
    "\n",
    "<img src='logclass.svg' width=500 height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc24b5b",
   "metadata": {},
   "source": [
    "Moving from a regression to a classification task changes several aspects of the model, such as the model structure, the cost function, and the interpretations of the coefficients. In this lesson, we'll explore how to interpret logistic regression coefficients, and we'll see why binary classification alters these interpretations. We'll also learn how to use the scikit-learn library to create logistic regression models instead of implementing gradient descent by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48259d5",
   "metadata": {},
   "source": [
    "<img src='bc.svg' width=500 height=500 >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0192597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f75f57f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symboling</th>\n",
       "      <th>normalized_losses</th>\n",
       "      <th>make</th>\n",
       "      <th>fuel_type</th>\n",
       "      <th>aspiration</th>\n",
       "      <th>num_of_doors</th>\n",
       "      <th>body_style</th>\n",
       "      <th>drive_wheels</th>\n",
       "      <th>engine_location</th>\n",
       "      <th>wheel_base</th>\n",
       "      <th>...</th>\n",
       "      <th>fuel_system</th>\n",
       "      <th>bore</th>\n",
       "      <th>stroke</th>\n",
       "      <th>compression_ratio</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>peak_rpm</th>\n",
       "      <th>city_mpg</th>\n",
       "      <th>highway_mpg</th>\n",
       "      <th>price</th>\n",
       "      <th>high_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>164</td>\n",
       "      <td>audi</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>fwd</td>\n",
       "      <td>front</td>\n",
       "      <td>99.8</td>\n",
       "      <td>...</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>102</td>\n",
       "      <td>5500</td>\n",
       "      <td>24</td>\n",
       "      <td>30</td>\n",
       "      <td>13950</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>164</td>\n",
       "      <td>audi</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>4wd</td>\n",
       "      <td>front</td>\n",
       "      <td>99.4</td>\n",
       "      <td>...</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>115</td>\n",
       "      <td>5500</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>17450</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>audi</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>fwd</td>\n",
       "      <td>front</td>\n",
       "      <td>105.8</td>\n",
       "      <td>...</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.4</td>\n",
       "      <td>8.5</td>\n",
       "      <td>110</td>\n",
       "      <td>5500</td>\n",
       "      <td>19</td>\n",
       "      <td>25</td>\n",
       "      <td>17710</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   symboling  normalized_losses  make fuel_type aspiration num_of_doors  \\\n",
       "0          2                164  audi       gas        std         four   \n",
       "1          2                164  audi       gas        std         four   \n",
       "2          1                158  audi       gas        std         four   \n",
       "\n",
       "  body_style drive_wheels engine_location  wheel_base  ...  fuel_system  bore  \\\n",
       "0      sedan          fwd           front        99.8  ...         mpfi  3.19   \n",
       "1      sedan          4wd           front        99.4  ...         mpfi  3.19   \n",
       "2      sedan          fwd           front       105.8  ...         mpfi  3.19   \n",
       "\n",
       "   stroke  compression_ratio horsepower peak_rpm  city_mpg highway_mpg  price  \\\n",
       "0     3.4               10.0        102     5500        24          30  13950   \n",
       "1     3.4                8.0        115     5500        18          22  17450   \n",
       "2     3.4                8.5        110     5500        19          25  17710   \n",
       "\n",
       "   high_price  \n",
       "0           0  \n",
       "1           1  \n",
       "2           1  \n",
       "\n",
       "[3 rows x 27 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c41ee8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= auto.drop(columns=['price','high_price'], axis=1)\n",
    "y= auto['high_price']\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y, test_size=0.20, random_state=732)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b9a79e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((127, 25), (127,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614f1d2c",
   "metadata": {},
   "source": [
    "##  The Logistic Regression Object\n",
    "As with many scikit-learn models, we create logistic regressions through a class. We'll use the LogisticRegression class that's available in the linear_model module. Curious students may notice that the documentation is much more detailed than the LinearRegression class. This difference is because logistic regression models encompass more tasks and considerations than their linear regression counterparts.\n",
    "To give a few examples, we may choose between binary and multi-class classification using the same LogisticRegression class. Gradient descent is one way to optimize the coefficients of a logistic regression model, and we may also specify different optimizers to calculate the coefficients. This is out of the scope of our lesson, but it's useful to know if you need to reference the documentation. (When necessary, we'll point out any important methods or attributes, so you don't have to wade through everything.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4ea352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f802832",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa3a635",
   "metadata": {},
   "source": [
    "## Fitting the Model\n",
    "On the previous screen, we learned how to create an instance of the LogisticRegression() object that will let us create a model without having to implement it. We'll create a model that tries to predict high_price using horsepower as the sole predictor.\n",
    "\n",
    "Now that we've learned how to instantiate a LogisticRegression() object, we can fit it to data â€” there is a fit() method. Given some predictors X and their associated outcomes y, we can fit the model as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b06cc4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_sub=x_train[['horsepower']]\n",
    "\n",
    "#fit the model\n",
    "model.fit(x_sub, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cb0a90",
   "metadata": {},
   "source": [
    "## Interpreting the Intercept\n",
    "Now that we've fitted a model, we can start to examine the coefficients and interpret them. Logistic regression is very interpretable, but it requires some extra thinking.\n",
    "\n",
    "The binary outcome can be related to the linear combination of predictors through the following relationship:\n",
    "\n",
    "Unlike linear regression, the linear combination does not directly act on the average of the outcome ; it acts on it via the **logit function**. This logit function changes how we should interpret the coefficients.\n",
    "We can read through this equation to get the correct interpretation. Recall that there are three related outcomes for logistic regression: **the success probability, the odds, and the log-odds**. The linear model acts on the log-odds, so we would interpret the intercept as: **the log-odds when** the predictor is 0. Keep in mind that this interpretation should always be adjusted to whatever predictor is being used in the model.\n",
    "\n",
    "Log-odds aren't interpretable by themselves, but it's a first step toward understanding the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b665137b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.3986778])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intercept = model.intercept_\n",
    "intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a097b2",
   "metadata": {},
   "source": [
    "## Interpreting the Intercept: Odds\n",
    "On the previous screen, we learned that the intercept of a logistic is the log-odds when the predictor is zero. While this is entirely correct, it's not very intuitive. We can take an extra step to simplify the interpretation. Intercept takes the following form:\n",
    "\n",
    "<img src='odds.png' width=500 height =500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ae87f1",
   "metadata": {},
   "source": [
    "Doing this gives us the odds on one side, and a function of the intercept on the other. Odds have better interpretability than log-odds because they give us a sense of what is more likely: an event happening or not happening. If the odds is greater than 1, it suggests that the probability of an event happening is greater than it not happening. Furthermore, if the odds are less than 1, it suggests the opposite conclusion.\n",
    "<img src='odd.svg' width=500 height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70efda19",
   "metadata": {},
   "source": [
    "For some, it might feel uncomfortable to talk in terms of the ratios of probabilities rather than the probabilities themselves. However, odds are used in familiar contexts like gambling, so they're not unheard of. Since we are usually expecting the binary event to happen several times, it's more helpful to use the odds to understand how many times we should expect to see a success versus a failure. The probability alone won't tell us that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52637aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "odds = np.exp(intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d096376c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00022516])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a4d4ef16",
   "metadata": {},
   "outputs": [],
   "source": [
    "more_likely = \"low\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a4411c",
   "metadata": {},
   "source": [
    "## Interpreting the Slope\n",
    "Now that we've learned how to interpret a logistic regression intercept, we can move onto interpreting the slope. We need to isolate the slope by subtracting two equations: one where X=0, and one where X=1, as shown below:\n",
    "\n",
    "Starting with . . .\n",
    "\n",
    "<img src='logslope.png' width=600 height=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda05440",
   "metadata": {},
   "source": [
    "The jump from the first to second line comes from the property of logarithms. For logistic regression, we can see that a unit increase in the predictor changes the outcome. Here, the slope represents the **log of the odds ratio**\n",
    "This odds ratio represents how the outcome changes with a change in the predictor. If the odds ratio is greater than one, then it implies that the value of the numerator (odds when X=1) is greater than the denominator (odds when X=0). This further implies that the probability of success, EY, is also increasing in a non-linear way.\n",
    "\n",
    "<img src='oddsratio.png' width=500 height=500>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4990ce37",
   "metadata": {},
   "source": [
    "When presenting results on logistic regressions, it's almost always better to present the coefficients in terms of odds ratios. These are more interpretable, and they only require an extra calculation to get. Keep this in mind for the guided project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "124fba4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "slope= model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c71645b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06921187]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d0adcaa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.07166324]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odds_ratio = np.exp(slope)\n",
    "odds_ratio"
   ]
  },
  {
   "cell_type": "raw",
   "id": "87ae263a",
   "metadata": {},
   "source": [
    "more_likey = \"True\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d606766e",
   "metadata": {},
   "source": [
    "##  Multiple Predictors\n",
    "For a multiple logistic regression, a coefficient would be interpreted as **the log odds ratio for a unit change in the predictor, holding the other predictors constant.** Another way to say \"holding the other predictors constant\" is controlling for the other predictors. This phrase is more commonly used in research literature, so it's good to be aware of it.\n",
    "\n",
    "Let's create a multiple logistic regression and try to interpret it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20d60f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_sub = x_train[['horsepower','highway_mpg']]\n",
    "\n",
    "model.fit(x_sub, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "feedf1d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.68375486])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intercept= model. intercept_\n",
    "intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cff6bb6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.38574076])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = np.exp(intercept)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec64a614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03986097, -0.23966618]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slope = model.coef_\n",
    "slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db69f700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.040666075641625"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "horsepower_or = np.exp(slope[0,0])\n",
    "horsepower_or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "df409b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7868904937228752"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highway_mpg_or = np.exp(slope[0,1])\n",
    "highway_mpg_or"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cb7c5c",
   "metadata": {},
   "source": [
    "##  Success Probabilities\n",
    "In general, when we deal with logistic regressions, we interpret in terms of odds and odds ratios. For that reason, we've largely sidestepped discussing how the coefficients affect the success probability.\n",
    "\n",
    "To conclude this lesson, let's use a fitted LogisticRegression model to return the predicted probability of an observation. To do so, we can look back to the logistic regression formula, in odds form:\n",
    "\n",
    "We can further rearrange the equation so that we can isolate on one side. Without going into the math, we get the following:\n",
    "\n",
    "LogisticRegression objects have a method that performs the above calculation: the predict_proba() method. This method takes in a dataset that has all of the predictors used in the model and outputs the estimated probability for each observation in the data.\n",
    "\n",
    "We'd like to note though that even though an observation might have a seemingly high predicted probability, it won't necessarily mean that the actual observation will also be a success. This is another reason it's more useful to discuss logistic regressions in terms of odds. They give us a sense of how many successes and failures we should expect to see in a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ba4cc948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.98876619e-01, 1.12338119e-03],\n",
       "       [3.55488672e-01, 6.44511328e-01],\n",
       "       [1.55867085e-02, 9.84413292e-01],\n",
       "       [8.80741051e-01, 1.19258949e-01],\n",
       "       [9.19711785e-01, 8.02882151e-02],\n",
       "       [9.99698730e-01, 3.01270339e-04],\n",
       "       [9.42523581e-01, 5.74764188e-02],\n",
       "       [7.31243507e-01, 2.68756493e-01],\n",
       "       [9.33220286e-01, 6.67797137e-02],\n",
       "       [9.95644770e-01, 4.35523035e-03]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = model.predict_proba(x_sub)\n",
    "probs[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf997b32",
   "metadata": {},
   "source": [
    "## Review\n",
    "In this lesson, we learned how to fit a LogisticRegression model and interpret the coefficients estimated by the model. Model coefficients affect the log-odds, which affects both the odds and the success probability. We can use this knowledge to understand how different predictors affect the outcome.\n",
    "\n",
    "In the next lesson, we'll learn how to evaluate the performance of a logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b121d9ee",
   "metadata": {},
   "source": [
    "# Evaluating Logistic Regression Models\n",
    "Welcome to the third lesson of **Logistic Regression Modeling in Python!** We can start building any binary classifier we want, but currently, we don't have any idea how to evaluate them. Here, we'll learn about various metrics that we can use to evaluate them.\n",
    "\n",
    "<img src='metrics.svg' width=500 height=500 >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7378edd5",
   "metadata": {},
   "source": [
    "With classification, there are more diverse arrays of metrics that we can look at. We'll discuss why we use each metric and what we can learn from them.\n",
    "\n",
    "Let's begin by setting up our libraries and data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9edac8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= auto.drop(columns=['price','high_price'], axis=1)\n",
    "y= auto['high_price']\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y, test_size=0.20, random_state=733)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3df0f54",
   "metadata": {},
   "source": [
    "##  Accuracy\n",
    "The first and most important classification metric that we will learn is accuracy. Accuracy gives us a straightforward picture of how well the logistic regression predicted the classes in the data. It's calculated as follows:\n",
    "\n",
    "**`Accuracy=# Correct Predictions/# Observations**`\n",
    "\n",
    "Based on the calculation above, we can see that accuracy is just the proportion of the dataset that is predicted correctly. \"Correct\" in a binary classification sense means that observed 1s were predicted to be 1 and likewise observed 0's were predicted to be 0. We'll see that other metrics focus more on one class.\n",
    "\n",
    "We can calculate accuracy by hand, but it's quicker to use the built-in methods in the LogisticRegression class. We call the score() method using the data and the associated classes.\n",
    "\n",
    "`from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "model.score(X, y)`\n",
    "\n",
    "Alternatively, we can also use a function from the metrics module: the accuracy_score function. This function takes two arguments: the true classes in the data and the predicted classes by the model.\n",
    "\n",
    "In order to retrieve the model predictions, we can use the predict() method. We can use these as the second argument in the accuracy_score() function.\n",
    "\n",
    "`from sklearn.metrics import accuracy_score\n",
    "accuracy_score(true_classes, predicted_classes)\n",
    "\n",
    "predicted_classes = model.predict(X)\n",
    "accuracy_score(true_classes, predicted_classes)`\n",
    "\n",
    "Either function will work, it's up to you. The score() method in the LogisticRegression class is convenient because it can be called for any fitted LogisticRegression() object. accuracy_score needs to be given the predictions as an argument, but it can be used for any classification model.\n",
    "\n",
    "Test accuracy will be the metric that we ultimately refer to when we compare models. Training accuracy will only be used to give us an optimistic sense of how well the model performs. Let's see how well a logistic model using only horsepower can predict high_price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1cc5a260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8661417322834646"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_sub= x_train[['horsepower']]\n",
    "\n",
    "model.fit(x_sub, y_train)\n",
    "predict=model.predict(x_sub)\n",
    "\n",
    "accuracy=model.score(x_sub, y_train)\n",
    "accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253e2e14",
   "metadata": {},
   "source": [
    "## Sensitivity\n",
    "While accuracy is the most important metric that we should take away from this course, it's still useful to know and understand other available metrics. On this and the next screen, we'll look at metrics that focus on how well a single class is classified.\n",
    "\n",
    "To understand this better, we'll consult a 2x2 table:\n",
    "<img src='confusionmatrix.svg' width=500 height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc0f106",
   "metadata": {},
   "source": [
    "Since there are two classes, 0 and 1, we can also see how well the model performs for each class. The word \"positive\" is used in a sense to refer to cases (class 1), while \"negative\" refers to non-cases (class 0). A model might have great overall accuracy, but it might mask weaknesses in predicting one of the classes. Perhaps it might perform very well at identifying non-cases but perform poorly for cases.\n",
    "\n",
    "If we were interested in focusing on how well the model performs with 1 classes, we look to a metric called **sensitivity**. Sensitivity is defined as follows:\n",
    "\n",
    "**`Sensitivity=# True Positives/# True Positives + # False Negatives**`\n",
    "<img src='sensitivity.svg' width=500 height=500 >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a899c84",
   "metadata": {},
   "source": [
    "Sensitivity may also be viewed as a **conditional probability**, as opposed to a marginal probability like accuracy. Given that we know an observation has a 1 class, what is the probability that the model predicted it correctly? It's important to keep this conditional aspect in mind, as it can lead to confusion later if forgotten.\n",
    "Sometimes we may be more concerned with how well 1 classes are captured, which would make sensitivity more useful than accuracy. You may see in other contexts that sensitivity is also referred to as **recall**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e69cce9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symboling</th>\n",
       "      <th>normalized_losses</th>\n",
       "      <th>make</th>\n",
       "      <th>fuel_type</th>\n",
       "      <th>aspiration</th>\n",
       "      <th>num_of_doors</th>\n",
       "      <th>body_style</th>\n",
       "      <th>drive_wheels</th>\n",
       "      <th>engine_location</th>\n",
       "      <th>wheel_base</th>\n",
       "      <th>...</th>\n",
       "      <th>fuel_system</th>\n",
       "      <th>bore</th>\n",
       "      <th>stroke</th>\n",
       "      <th>compression_ratio</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>peak_rpm</th>\n",
       "      <th>city_mpg</th>\n",
       "      <th>highway_mpg</th>\n",
       "      <th>price</th>\n",
       "      <th>high_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>164</td>\n",
       "      <td>audi</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>fwd</td>\n",
       "      <td>front</td>\n",
       "      <td>99.8</td>\n",
       "      <td>...</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>102</td>\n",
       "      <td>5500</td>\n",
       "      <td>24</td>\n",
       "      <td>30</td>\n",
       "      <td>13950</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>164</td>\n",
       "      <td>audi</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>4wd</td>\n",
       "      <td>front</td>\n",
       "      <td>99.4</td>\n",
       "      <td>...</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>115</td>\n",
       "      <td>5500</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>17450</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   symboling  normalized_losses  make fuel_type aspiration num_of_doors  \\\n",
       "0          2                164  audi       gas        std         four   \n",
       "1          2                164  audi       gas        std         four   \n",
       "\n",
       "  body_style drive_wheels engine_location  wheel_base  ...  fuel_system  bore  \\\n",
       "0      sedan          fwd           front        99.8  ...         mpfi  3.19   \n",
       "1      sedan          4wd           front        99.4  ...         mpfi  3.19   \n",
       "\n",
       "   stroke  compression_ratio horsepower peak_rpm  city_mpg highway_mpg  price  \\\n",
       "0     3.4               10.0        102     5500        24          30  13950   \n",
       "1     3.4                8.0        115     5500        18          22  17450   \n",
       "\n",
       "   high_price  \n",
       "0           0  \n",
       "1           1  \n",
       "\n",
       "[2 rows x 27 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "799a5adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp= sum(( y_train==1) & (predict==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f8c85742",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn= sum((y_train==1) & (predict==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "086873fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5806451612903226"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensitivity = tp/(tp + fn)\n",
    "sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2fb51b",
   "metadata": {},
   "source": [
    "## Specificity\n",
    "Now that we've learned about sensitivity, we can learn about its counterpart, specificity. Specificity has a similar interpretation as sensitivity, but it's concerned with the 0 class. As such, specificity is defined as follows:\n",
    "\n",
    "**`Specificity=# True Negatives/# True Negatives + # False Positives`**\n",
    "<img src='specificity.svg' width=500 height=500 >\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3722f06e",
   "metadata": {},
   "source": [
    "Specificity is also a conditional probability. Given that we know an observation has a 0 class, what is the probability that the model predicted it correctly? We might want to look at a model's specificity if we value more accurate predictions of the 0 class.\n",
    "\n",
    "In general, we're interested in a model's overall accuracy, but there can be situations where favoring one class over another might be useful. Sensitivity and specificity are crucial in understanding the following topics: **positive predictive value and negative predictive value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "30a4248a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8761904761904762"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn = sum((y_train== 0) & (predict==0))\n",
    "fn = sum((y_train ==1) & (predict==0))\n",
    "specificity= tn/(tn+fn)\n",
    "specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691066d4",
   "metadata": {},
   "source": [
    "##  Positive Predictive Value\n",
    "As we conclude this lesson, we'll look at two more metrics for our logistic regression. Metrics like sensitivity and specificity are built in with a subtle assumption. Both of these metrics assume that we know what the true class is for the observation. These metrics help us understand how well a model performs on data with known classes.\n",
    "\n",
    "But what about unknown classes? We may want to apply our model to new observations to try to predict their class. As an example, this is how we expect medical diagnostic tests to work. We may potentially be sick, but we don't actually know, so we are using the result of a diagnostic test to try to predict our health status.\n",
    "\n",
    "Using our training data, we can use another metric of how well our logistic regression will perform on unknown classes. This metric is **positive predictive value (PPV), also known as precision**. The PPV is calculated as follows:\n",
    "\n",
    "**`PPV or precision =# True Positives/# True Positives + # False Positives`**\n",
    "<img src='precision.svg' width=500 height=500 >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9962e1ae",
   "metadata": {},
   "source": [
    "Like sensitivity, PPV is also a conditional probability. But, the condition is different. We may interpret PPV like so: given that the model predicted an observation had a 1 class, what is the probability it actually had a 1 class.\n",
    "\n",
    "Sensitivity assumes that the true class was 1, while PPV assumes that the model prediction was 1. It's incredibly easy to mix this distinction up, so it takes some time to work out these definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7e2881e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8181818181818182"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp= sum((y_train ==1) & (predict ==1))\n",
    "fp = sum((y_train == 0) & (predict ==1))\n",
    "precision = tp/(tp+fp)\n",
    "precision#PPV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b3b2cf",
   "metadata": {},
   "source": [
    "## Negative Predictive Value\n",
    "\n",
    "To conclude this lesson, we'll look at the negative counterpart to positive predictive value: negative predictive value (NPV). This metric estimates how well the model correctly predicts a new observation to have a 0 class.\n",
    "\n",
    "**`NPV=# True Negatives/# True Negatives + # False Negatives`**\n",
    "<img src='npv.svg' width = 500 height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fda747",
   "metadata": {},
   "source": [
    "Both PPV and NPV have predictive in their name because they measure how well the model correctly predicts the classes. Sometimes, we would be interested in making sure that the negative predictions are truly negative, so we'd be interested in checking the NPV in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4ae11b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8761904761904762"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn = sum((y_train ==0) & (predict==0))\n",
    "fn = sum((y_train==1) & (predict==0))\n",
    "NPV = tn/(tn+fn)\n",
    "NPV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f7e9cb",
   "metadata": {},
   "source": [
    "# Applying Logistic Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa13844",
   "metadata": {},
   "source": [
    "Welcome to the final lesson of **Logistic Regression Modeling in Python!** By knowing how to evaluate logistic regression models, we can see how well our models work on the test dataset and begin to iterate and improve their predictive ability.\n",
    "\n",
    "Here, we'll also learn some other topics about logistic regression that will be useful as we progress through the Machine Learning path. We'll briefly introduce the idea of regularization and sparsity, and we'll also introduce how we can get started with multi-class classification.\n",
    "\n",
    "Let's set up our libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dd827112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['symboling', 'normalized_losses', 'make', 'fuel_type', 'aspiration',\n",
       "       'num_of_doors', 'body_style', 'drive_wheels', 'engine_location',\n",
       "       'wheel_base', 'length', 'width', 'height', 'curb_weight', 'engine_type',\n",
       "       'num_of_cylinders', 'engine_size', 'fuel_system', 'bore', 'stroke',\n",
       "       'compression_ratio', 'horsepower', 'peak_rpm', 'city_mpg',\n",
       "       'highway_mpg', 'price', 'high_price'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "211d36ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x= auto.drop(columns=['price','high_price'], axis=1)\n",
    "y=auto['high_price']\n",
    "\n",
    "x_train,x_test,y_train,y_test= train_test_split(x,y, test_size=0.20, random_state=734)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab29c08",
   "metadata": {},
   "source": [
    "## Test Accuracy\n",
    "When evaluating the accuracy of a logistic regression model, we initially look at the training accuracy, but we cannot truly evaluate the predictive ability of a model from this alone. We have to compare models based on their test accuracy. We want the model with the greatest test accuracy (the accuracy closest to 1).\n",
    "<img src='accuracy.gif' width=400 height=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "53d5b2c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "auto = pd.read_csv(\"automobiles.csv\")\n",
    "\n",
    "auto[\"high_price\"] = 0\n",
    "auto.loc[auto[\"price\"] > 15000, \"high_price\"] = 1\n",
    "\n",
    "X = auto.drop([\"price\", \"high_price\"], axis = 1)\n",
    "y = auto[\"high_price\"]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state = 734)\n",
    "\n",
    "x1 = x_train[[\"length\", \"horsepower\"]]\n",
    "x2 = x_train[[\"stroke\", \"compression_ratio\"]]\n",
    "\n",
    "model1 = LogisticRegression()\n",
    "model2 = LogisticRegression()\n",
    "\n",
    "model1.fit(x1, y_train)\n",
    "model2.fit(x2, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "07a5e321",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the models\n",
    "\n",
    "model1_accuracy= model1.score(x1, y_train),\n",
    "model2_accuracy= model2.score(x2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7dd83456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.889763779527559,) 0.7480314960629921\n"
     ]
    }
   ],
   "source": [
    "print(model1_accuracy, model2_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6a649",
   "metadata": {},
   "source": [
    "## Feature Selection via Split-Apply-Combine\n",
    "It's one thing to know how to put together a logistic regression; it's another to polish it into a refined model. As of now, we can build models based on horsepower â€” but there is no justification for doing so other than for educational purposes. We have to contend with the issue of **feature selection** since it won't be clear ahead of time which variables will produce the best classification model.\n",
    "\n",
    "One strategy in linear regression is to use visualizations to determine if there were high correlations between a predictor and the outcome. We also use visualizations to make this judgment for binary variables. We'll do something similar here, but we'll leverage a new way of summarizing the information in the auto dataset.\n",
    "We are interested in which predictors are meaningfully different when comparing between the 0 and 1 classes. For example, if the `range and distribution of horsepower` for low-price cars is significantly different from high-price cars, then horsepower can be a useful predictor to include in a model because it distinguishes `cases from non-cases`. We might go through each predictor and repeatedly calculate its means among the 0 and 1 classes. This can get repetitive and is error-prone. Another way to do this quickly is to take advantage of the `groupby() and agg()` methods that are available for `pandas DataFrames.`\n",
    "Consequently, the agg() method (short for \"aggregate\") is what we use to specify what columns to use and what operations to use to calculate for each strata. For example, if we want to calculate the mean horsepower and width for both the 0 and 1 classes, we write the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "558f724d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>horsepower</th>\n",
       "      <th>width</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>high_price</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83.907563</td>\n",
       "      <td>64.821849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>131.325000</td>\n",
       "      <td>67.945000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            horsepower      width\n",
       "high_price                       \n",
       "0            83.907563  64.821849\n",
       "1           131.325000  67.945000"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto.groupby('high_price').agg({'horsepower':'mean','width':'mean'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b74eb62",
   "metadata": {},
   "source": [
    "First, groupby() specifies on which column we are splitting up the data (the high_price column in this case). Next, we need to give the agg() method a dictionary. The keys of this dictionary are the predictors we want to summarize, and the values are the operations that we want to perform. In this example, we want to summarize the horsepower and width columns using the mean() function.\n",
    "\n",
    "This workflow is what we refer to as **split-apply-combine**, and it is a powerful and quick way to understand the data. There are several methods for writing this workflow for pandas â€” we've chosen this method because it allows us to summarize multiple columns at once, which is what we'll want for feature selection.\n",
    "<img src='split_apply_combine.gif' width=400 height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9808195b",
   "metadata": {},
   "source": [
    "**Instructions**\n",
    "\n",
    "Recall from the previous screen that we compared two models: one used horsepower and length, and the other used stroke and compression_ratio. Let's use split-apply-combine to determine why one model was superior to the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f5fc73c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>horsepower</th>\n",
       "      <th>length</th>\n",
       "      <th>stroke</th>\n",
       "      <th>compression_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>high_price</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83.907563</td>\n",
       "      <td>168.22605</td>\n",
       "      <td>3.241261</td>\n",
       "      <td>9.886723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>131.325000</td>\n",
       "      <td>184.87250</td>\n",
       "      <td>3.221750</td>\n",
       "      <td>10.977500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            horsepower     length    stroke  compression_ratio\n",
       "high_price                                                    \n",
       "0            83.907563  168.22605  3.241261           9.886723\n",
       "1           131.325000  184.87250  3.221750          10.977500"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary=auto.groupby('high_price').agg({\n",
    "    'horsepower':'mean',\n",
    "    'length':'mean',\n",
    "    'stroke':'mean',\n",
    "    'compression_ratio':'mean'\n",
    "})\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1d43d489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47.41743699999999"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "131.325000- 83.907563"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "97e43b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.646450000000016"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "184.87250-168.22605"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9967590f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.019511000000000056"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3.241261-3.221750"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9a00eaa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0907769999999992"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10.977500-9.886723"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3bfd729e",
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_diff= 'horsepower'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13700944",
   "metadata": {},
   "source": [
    "## Model Complexity\n",
    "We've learned a fair amount of tools to perform feature selection so far, which enables us to pick a good set of predictors for our models. In the toy examples we've covered, the number of predictors has been relatively small. What if we were to find in feature selection that many, many predictors are highly correlated with or easily predict the outcome? We may be tempted to add all of these predictors into the model, making it more complex.\n",
    "\n",
    "In some cases, increasing complexity through the addition of predictors can be useful if they all truly influence the outcome. In others, the model might just be learning how to fit the data rather than capturing any true relationship that links the predictors to the outcome. We'll explore this further in a different course, but we'll introduce this problem briefly here. This is a problem of overfitting, when the model starts to learn the data itself, rather than the fundamental relationship between the predictors and outcome.\n",
    "<img src='noise.svg' width=400 height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ffcb38",
   "metadata": {},
   "source": [
    "We've created a series of three nested models of increasing complexity. Let's see what happens with the training and test error.\n",
    "\n",
    "**Instructions**\n",
    "- Calculate the training accuracies for all three of the models in order of increasing complexity. Store these in an array called train_accuracies.\n",
    "- Calculate the test accuracies for all three of the models in order of increasing complexity. Store these in an array called test_accuracies.\n",
    "Use the information from these variables to answer the questions below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8e5efea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "auto = pd.read_csv(\"automobiles.csv\")\n",
    "\n",
    "auto[\"high_price\"] = 0\n",
    "auto.loc[auto[\"price\"] > 15000, \"high_price\"] = 1\n",
    "\n",
    "X = auto.drop([\"price\", \"high_price\"], axis = 1)\n",
    "y = auto[\"high_price\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state = 734)\n",
    "\n",
    "X1 = X_train[[\"horsepower\"]]\n",
    "X2 = X_train[[\"horsepower\", \"compression_ratio\"]]\n",
    "X3 = X_train[[\"horsepower\", \"compression_ratio\", \"city_mpg\"]]\n",
    "\n",
    "X1_test = X_test[[\"horsepower\"]]\n",
    "X2_test = X_test[[\"horsepower\", \"compression_ratio\"]]\n",
    "X3_test = X_test[[\"horsepower\", \"compression_ratio\", \"city_mpg\"]]\n",
    "\n",
    "model1 = LogisticRegression()\n",
    "model2 = LogisticRegression()\n",
    "model3 = LogisticRegression()\n",
    "\n",
    "model1.fit(X1, y_train)\n",
    "model2.fit(X2, y_train)\n",
    "model3.fit(X3, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b76900a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the model\n",
    "\n",
    "model1_accuracy_train= model1.score(X1, y_train)\n",
    "model2_acuuracy_train= model2.score(X2, y_train)\n",
    "model3_accuracy_train= model3.score(X3, y_train)\n",
    "\n",
    "model1_accuracy_test= model1.score(X1_test, y_test)\n",
    "model2_acuuracy_test= model2.score(X2_test, y_test)\n",
    "model3_accuracy_test= model3.score(X3_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "77068ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8976377952755905 0.905511811023622 0.9212598425196851\n"
     ]
    }
   ],
   "source": [
    "print(model1_accuracy_train,model2_acuuracy_train,model3_accuracy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e6e4b43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.78125 0.78125 0.875\n"
     ]
    }
   ],
   "source": [
    "print(model1_accuracy_test,model2_acuuracy_test,model3_accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b1b2c3",
   "metadata": {},
   "source": [
    "##  Multi-class Classification\n",
    "We've learned how to go from data exploration to a trained logistic model. We're equipped to start applying these models to new datasets, so we'll use the rest of the lesson to cover some extra topics on logistic regression.\n",
    "You may be surprised to learn that you already know most of what you need to know to perform **multi-class classification.** The LogisticRegression class is equipped to detect what type of classification task it's working with, based on what it detects with the outcome. The API that we've learned throughout this course still applies in both the binary and multi-class cases.\n",
    "\n",
    "But of course, with more categories comes slightly different metrics. Accuracy still applies to multi-class classification, but we lose sensitivity and specificity as metrics because they make sense only with binary outcomes. A better way to see how a multi-class classifier performs is through a **confusion matrix**, which is an extended version of the 2 x 2 tables we saw earlier. The diagonals of a confusion matrix count the number of correct predictions that the model made. The off-diagonals represent how often a class is predicted as another class. These counts are useful because we might find that the model has trouble with two specific classes, rather than just being a generally bad classifier\n",
    "<img src='confusion_matrix.svg' width=400 height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c5f7f7",
   "metadata": {},
   "source": [
    "In order to generate a confusion matrix, we can use the confusion_matrix() function in the metrics module. This function takes in two arguments: the true classes and then the predicted classes. We can use the predict() method to generate the model predictions, and put these in the function as shown below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1b56f2ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['symboling', 'normalized_losses', 'make', 'fuel_type', 'aspiration',\n",
       "       'num_of_doors', 'body_style', 'drive_wheels', 'engine_location',\n",
       "       'wheel_base', 'length', 'width', 'height', 'curb_weight', 'engine_type',\n",
       "       'num_of_cylinders', 'engine_size', 'fuel_system', 'bore', 'stroke',\n",
       "       'compression_ratio', 'horsepower', 'peak_rpm', 'city_mpg',\n",
       "       'highway_mpg', 'price', 'high_price'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ae89ea48",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto['price_category']='Low'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e5e4cbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\512GB\\AppData\\Local\\Temp\\ipykernel_5584\\3713218005.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  auto['price'][i]=price\n",
      "C:\\Users\\512GB\\AppData\\Local\\Temp\\ipykernel_5584\\3713218005.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  auto['price_category'][i]=price_category\n"
     ]
    }
   ],
   "source": [
    "for i in auto.index:\n",
    "    price= auto['price'][i]\n",
    "    price_category=auto['price_category'][i]\n",
    "    if price <= 10000 & price <15000:\n",
    "        price_category = 'Mid'\n",
    "    elif price >= 15000:\n",
    "        price_category = 'High'\n",
    "    auto['price'][i]=price\n",
    "    auto['price_category'][i]=price_category\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b74d26f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symboling</th>\n",
       "      <th>normalized_losses</th>\n",
       "      <th>make</th>\n",
       "      <th>fuel_type</th>\n",
       "      <th>aspiration</th>\n",
       "      <th>num_of_doors</th>\n",
       "      <th>body_style</th>\n",
       "      <th>drive_wheels</th>\n",
       "      <th>engine_location</th>\n",
       "      <th>wheel_base</th>\n",
       "      <th>...</th>\n",
       "      <th>bore</th>\n",
       "      <th>stroke</th>\n",
       "      <th>compression_ratio</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>peak_rpm</th>\n",
       "      <th>city_mpg</th>\n",
       "      <th>highway_mpg</th>\n",
       "      <th>price</th>\n",
       "      <th>high_price</th>\n",
       "      <th>price_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>164</td>\n",
       "      <td>audi</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>fwd</td>\n",
       "      <td>front</td>\n",
       "      <td>99.8</td>\n",
       "      <td>...</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>102</td>\n",
       "      <td>5500</td>\n",
       "      <td>24</td>\n",
       "      <td>30</td>\n",
       "      <td>13950</td>\n",
       "      <td>0</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>164</td>\n",
       "      <td>audi</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>4wd</td>\n",
       "      <td>front</td>\n",
       "      <td>99.4</td>\n",
       "      <td>...</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>115</td>\n",
       "      <td>5500</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>17450</td>\n",
       "      <td>1</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>audi</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>fwd</td>\n",
       "      <td>front</td>\n",
       "      <td>105.8</td>\n",
       "      <td>...</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.4</td>\n",
       "      <td>8.5</td>\n",
       "      <td>110</td>\n",
       "      <td>5500</td>\n",
       "      <td>19</td>\n",
       "      <td>25</td>\n",
       "      <td>17710</td>\n",
       "      <td>1</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>audi</td>\n",
       "      <td>gas</td>\n",
       "      <td>turbo</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>fwd</td>\n",
       "      <td>front</td>\n",
       "      <td>105.8</td>\n",
       "      <td>...</td>\n",
       "      <td>3.13</td>\n",
       "      <td>3.4</td>\n",
       "      <td>8.3</td>\n",
       "      <td>140</td>\n",
       "      <td>5500</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>23875</td>\n",
       "      <td>1</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>192</td>\n",
       "      <td>bmw</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>two</td>\n",
       "      <td>sedan</td>\n",
       "      <td>rwd</td>\n",
       "      <td>front</td>\n",
       "      <td>101.2</td>\n",
       "      <td>...</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.8</td>\n",
       "      <td>8.8</td>\n",
       "      <td>101</td>\n",
       "      <td>5800</td>\n",
       "      <td>23</td>\n",
       "      <td>29</td>\n",
       "      <td>16430</td>\n",
       "      <td>1</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   symboling  normalized_losses  make fuel_type aspiration num_of_doors  \\\n",
       "0          2                164  audi       gas        std         four   \n",
       "1          2                164  audi       gas        std         four   \n",
       "2          1                158  audi       gas        std         four   \n",
       "3          1                158  audi       gas      turbo         four   \n",
       "4          2                192   bmw       gas        std          two   \n",
       "\n",
       "  body_style drive_wheels engine_location  wheel_base  ...  bore  stroke  \\\n",
       "0      sedan          fwd           front        99.8  ...  3.19     3.4   \n",
       "1      sedan          4wd           front        99.4  ...  3.19     3.4   \n",
       "2      sedan          fwd           front       105.8  ...  3.19     3.4   \n",
       "3      sedan          fwd           front       105.8  ...  3.13     3.4   \n",
       "4      sedan          rwd           front       101.2  ...  3.50     2.8   \n",
       "\n",
       "   compression_ratio  horsepower peak_rpm city_mpg  highway_mpg  price  \\\n",
       "0               10.0         102     5500       24           30  13950   \n",
       "1                8.0         115     5500       18           22  17450   \n",
       "2                8.5         110     5500       19           25  17710   \n",
       "3                8.3         140     5500       17           20  23875   \n",
       "4                8.8         101     5800       23           29  16430   \n",
       "\n",
       "   high_price  price_category  \n",
       "0           0             Low  \n",
       "1           1            High  \n",
       "2           1            High  \n",
       "3           1            High  \n",
       "4           1            High  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "48880d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Low', 'High'], dtype=object)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto['price_category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "15b24e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x= auto.drop(columns=['price','price_category'], axis=1)\n",
    "y= auto['high_price']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "43b071a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1= x_train[['engine_size','horsepower']]\n",
    "x1_test= x_test[['engine_size','horsepower']]\n",
    "\n",
    "model= LogisticRegression()\n",
    "\n",
    "model.fit(x1, y_train)\n",
    "predictions= model.predict(x1_test)\n",
    "\n",
    "#evaluate the model\n",
    "\n",
    "test_accuracy= model.score(x1_test, y_test)\n",
    "\n",
    "confusion=confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3ed00270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.78125 [[22  2]\n",
      " [ 5  3]]\n"
     ]
    }
   ],
   "source": [
    "print(test_accuracy, confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24639e8",
   "metadata": {},
   "source": [
    "## Communicating Results\n",
    "We're almost finished with the course, so let's look back our process of learning classification through logistic regression. We've examined the auto dataset to understand how to best predict a high_price column that we created. Classification is to categorical outcomes as regression is to continuous outcomes, and both are different forms of supervised learning. How should we communicate our findings to others?\n",
    "\n",
    "We should be transparent about the data and the process we used to select a final model. Here's a non-exhaustive list of questions we should consider when summarizing our model â€” along with some extra points to consider.\n",
    "\n",
    "- How much data was used for training? What about the test set?\n",
    "- Did we have to make any transformations to the outcome or predictors? If so, what justification did we make to do so?\n",
    "- What predictors did we include in the model? \n",
    "- Were there any particular reasons they were included?\n",
    "- How do the model predictions look against the plotted data? \n",
    "- Does the predicted logistic curve along a single predictor seem to match the distribution of the outcomes?\n",
    "- How well does our model predict unseen data? Is the level of error acceptable for its application?\n",
    "- Does the model perform better with 0 classes? 1 classes?\n",
    "Even though we're discussing a different supervised learning task, the same backbone still remains. Some details may simply differ between models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9106cf",
   "metadata": {},
   "source": [
    "## Review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed748c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
